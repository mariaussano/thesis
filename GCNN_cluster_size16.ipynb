{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d5c0ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 12:20:11.012223: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import seaborn as sns                             \n",
    "import matplotlib.pyplot as plt   \n",
    "import scipy.sparse as sp\n",
    "import os\n",
    "\n",
    "from spektral.data import Dataset, Loader, Graph, SingleLoader, DisjointLoader\n",
    "from spektral.layers import GCNConv\n",
    "from keras.layers import Input,Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "import keras.metrics\n",
    "from tensorflow.keras import regularizers\n",
    "from keras import Model\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31afc0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(mypath):\n",
    "    c = pd.read_csv(mypath)\n",
    "    c=c.rename(columns={\"Unnamed: 0\": \"number\"})\n",
    "    c.drop(columns=['number'],inplace=True)\n",
    "    #print(c)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e7a10cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "c2=load('/Users/mariaussano/Desktop/scaled_cluster2.txt')\n",
    "c4=load(('/Users/mariaussano/Desktop/scaled_cluster4.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e9eedab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>TOT_mean</th>\n",
       "      <th>Avg_population</th>\n",
       "      <th>Number of employees administrative sector</th>\n",
       "      <th>Number of employees manufacturing sector</th>\n",
       "      <th>Number of establishments information sector</th>\n",
       "      <th>Family income 2010</th>\n",
       "      <th>GDP per capita 2016</th>\n",
       "      <th>Percent single-family houses</th>\n",
       "      <th>Percent high density houses</th>\n",
       "      <th>...</th>\n",
       "      <th>('d_from_', 'Rochester, Minnesota')</th>\n",
       "      <th>('d_from_', 'Vernal, Utah')</th>\n",
       "      <th>('d_from_', 'Bend, Oregon')</th>\n",
       "      <th>('d_from_', 'Flagstaff, Arizona')</th>\n",
       "      <th>('d_from_', 'Montrose, Colorado')</th>\n",
       "      <th>('d_from_', 'Clovis, California')</th>\n",
       "      <th>('d_from_', 'Santa Rosa, California')</th>\n",
       "      <th>('d_from_', 'Spearfish, South Dakota')</th>\n",
       "      <th>('d_from_', 'Marietta, Georgia')</th>\n",
       "      <th>('d_from_', 'Belgrade, Montana')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fort Worth, Texas</td>\n",
       "      <td>-0.165631</td>\n",
       "      <td>-0.148714</td>\n",
       "      <td>-0.008793</td>\n",
       "      <td>1.583277</td>\n",
       "      <td>-0.351125</td>\n",
       "      <td>0.487157</td>\n",
       "      <td>1.544886</td>\n",
       "      <td>1.068921</td>\n",
       "      <td>-0.381515</td>\n",
       "      <td>...</td>\n",
       "      <td>1.999056</td>\n",
       "      <td>2.160587</td>\n",
       "      <td>2.236987</td>\n",
       "      <td>2.568616</td>\n",
       "      <td>2.639444</td>\n",
       "      <td>2.717338</td>\n",
       "      <td>2.919790</td>\n",
       "      <td>3.117179</td>\n",
       "      <td>3.150085</td>\n",
       "      <td>3.983262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Louisville/Jefferson County metro government (...</td>\n",
       "      <td>-0.228196</td>\n",
       "      <td>0.160199</td>\n",
       "      <td>-0.629881</td>\n",
       "      <td>1.161723</td>\n",
       "      <td>-0.218416</td>\n",
       "      <td>0.371602</td>\n",
       "      <td>0.206409</td>\n",
       "      <td>0.760416</td>\n",
       "      <td>-0.885700</td>\n",
       "      <td>...</td>\n",
       "      <td>1.486713</td>\n",
       "      <td>1.648244</td>\n",
       "      <td>1.724644</td>\n",
       "      <td>2.056273</td>\n",
       "      <td>2.127101</td>\n",
       "      <td>2.204995</td>\n",
       "      <td>2.407447</td>\n",
       "      <td>2.604836</td>\n",
       "      <td>2.637742</td>\n",
       "      <td>3.470919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Phoenix, Arizona</td>\n",
       "      <td>1.286664</td>\n",
       "      <td>1.500587</td>\n",
       "      <td>1.650245</td>\n",
       "      <td>1.491922</td>\n",
       "      <td>1.321002</td>\n",
       "      <td>0.442537</td>\n",
       "      <td>-0.666485</td>\n",
       "      <td>0.471193</td>\n",
       "      <td>-0.219456</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997660</td>\n",
       "      <td>1.159190</td>\n",
       "      <td>1.235590</td>\n",
       "      <td>1.567220</td>\n",
       "      <td>1.638047</td>\n",
       "      <td>1.715941</td>\n",
       "      <td>1.918393</td>\n",
       "      <td>2.115782</td>\n",
       "      <td>2.148688</td>\n",
       "      <td>2.981865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Charlotte, North Carolina</td>\n",
       "      <td>-0.314859</td>\n",
       "      <td>0.088878</td>\n",
       "      <td>0.944596</td>\n",
       "      <td>0.045956</td>\n",
       "      <td>1.086551</td>\n",
       "      <td>1.444059</td>\n",
       "      <td>0.894501</td>\n",
       "      <td>-0.010846</td>\n",
       "      <td>0.176690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.851630</td>\n",
       "      <td>1.013160</td>\n",
       "      <td>1.089561</td>\n",
       "      <td>1.421190</td>\n",
       "      <td>1.492018</td>\n",
       "      <td>1.569912</td>\n",
       "      <td>1.772363</td>\n",
       "      <td>1.969753</td>\n",
       "      <td>2.002659</td>\n",
       "      <td>2.835836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Memphis, Tennessee</td>\n",
       "      <td>-0.023420</td>\n",
       "      <td>-0.103242</td>\n",
       "      <td>0.477276</td>\n",
       "      <td>-0.144607</td>\n",
       "      <td>-0.169756</td>\n",
       "      <td>-1.515459</td>\n",
       "      <td>-0.238709</td>\n",
       "      <td>0.413348</td>\n",
       "      <td>-0.849687</td>\n",
       "      <td>...</td>\n",
       "      <td>0.668485</td>\n",
       "      <td>0.830016</td>\n",
       "      <td>0.906416</td>\n",
       "      <td>1.238045</td>\n",
       "      <td>1.308873</td>\n",
       "      <td>1.386767</td>\n",
       "      <td>1.589218</td>\n",
       "      <td>1.786608</td>\n",
       "      <td>1.819514</td>\n",
       "      <td>2.652691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Austin, Texas</td>\n",
       "      <td>-0.065523</td>\n",
       "      <td>0.178101</td>\n",
       "      <td>0.581356</td>\n",
       "      <td>0.022312</td>\n",
       "      <td>2.236691</td>\n",
       "      <td>1.547744</td>\n",
       "      <td>1.307759</td>\n",
       "      <td>-0.907438</td>\n",
       "      <td>1.725259</td>\n",
       "      <td>...</td>\n",
       "      <td>0.314885</td>\n",
       "      <td>0.476416</td>\n",
       "      <td>0.552816</td>\n",
       "      <td>0.884445</td>\n",
       "      <td>0.955273</td>\n",
       "      <td>1.033167</td>\n",
       "      <td>1.235619</td>\n",
       "      <td>1.433008</td>\n",
       "      <td>1.465914</td>\n",
       "      <td>2.299091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tucson, Arizona</td>\n",
       "      <td>-0.180550</td>\n",
       "      <td>-0.374074</td>\n",
       "      <td>-0.483790</td>\n",
       "      <td>-1.153146</td>\n",
       "      <td>-0.594423</td>\n",
       "      <td>-0.833285</td>\n",
       "      <td>-1.599467</td>\n",
       "      <td>-0.280788</td>\n",
       "      <td>0.140677</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.161530</td>\n",
       "      <td>0.237931</td>\n",
       "      <td>0.569560</td>\n",
       "      <td>0.640388</td>\n",
       "      <td>0.718281</td>\n",
       "      <td>0.920733</td>\n",
       "      <td>1.118122</td>\n",
       "      <td>1.151028</td>\n",
       "      <td>1.984205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Kansas City, Missouri</td>\n",
       "      <td>-0.654611</td>\n",
       "      <td>-0.817708</td>\n",
       "      <td>-0.785886</td>\n",
       "      <td>-0.294412</td>\n",
       "      <td>-0.311312</td>\n",
       "      <td>0.380469</td>\n",
       "      <td>0.535886</td>\n",
       "      <td>0.644727</td>\n",
       "      <td>0.050644</td>\n",
       "      <td>...</td>\n",
       "      <td>0.161530</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076401</td>\n",
       "      <td>0.408030</td>\n",
       "      <td>0.478858</td>\n",
       "      <td>0.556751</td>\n",
       "      <td>0.759203</td>\n",
       "      <td>0.956592</td>\n",
       "      <td>0.989498</td>\n",
       "      <td>1.822675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Dallas, Texas</td>\n",
       "      <td>0.587553</td>\n",
       "      <td>0.931239</td>\n",
       "      <td>2.640475</td>\n",
       "      <td>1.837665</td>\n",
       "      <td>1.896072</td>\n",
       "      <td>-1.145912</td>\n",
       "      <td>1.544886</td>\n",
       "      <td>-1.196661</td>\n",
       "      <td>1.923332</td>\n",
       "      <td>...</td>\n",
       "      <td>0.237931</td>\n",
       "      <td>0.076401</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.331629</td>\n",
       "      <td>0.402457</td>\n",
       "      <td>0.480351</td>\n",
       "      <td>0.682802</td>\n",
       "      <td>0.880192</td>\n",
       "      <td>0.913098</td>\n",
       "      <td>1.746275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Wichita, Kansas</td>\n",
       "      <td>-0.759692</td>\n",
       "      <td>-0.841498</td>\n",
       "      <td>-0.932980</td>\n",
       "      <td>0.430307</td>\n",
       "      <td>-0.722708</td>\n",
       "      <td>0.684801</td>\n",
       "      <td>-0.176806</td>\n",
       "      <td>1.174969</td>\n",
       "      <td>-0.831681</td>\n",
       "      <td>...</td>\n",
       "      <td>0.569560</td>\n",
       "      <td>0.408030</td>\n",
       "      <td>0.331629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070828</td>\n",
       "      <td>0.148721</td>\n",
       "      <td>0.351173</td>\n",
       "      <td>0.548562</td>\n",
       "      <td>0.581468</td>\n",
       "      <td>1.414645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Cincinnati, Ohio</td>\n",
       "      <td>0.081140</td>\n",
       "      <td>0.503860</td>\n",
       "      <td>-0.945170</td>\n",
       "      <td>-0.637839</td>\n",
       "      <td>-0.753674</td>\n",
       "      <td>-1.060819</td>\n",
       "      <td>0.545666</td>\n",
       "      <td>-1.697981</td>\n",
       "      <td>0.572835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.640388</td>\n",
       "      <td>0.478858</td>\n",
       "      <td>0.402457</td>\n",
       "      <td>0.070828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077893</td>\n",
       "      <td>0.280345</td>\n",
       "      <td>0.477734</td>\n",
       "      <td>0.510641</td>\n",
       "      <td>1.343817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Greensboro, North Carolina</td>\n",
       "      <td>-0.928743</td>\n",
       "      <td>-1.252529</td>\n",
       "      <td>-0.340593</td>\n",
       "      <td>-0.309293</td>\n",
       "      <td>-0.749250</td>\n",
       "      <td>0.234309</td>\n",
       "      <td>-0.199994</td>\n",
       "      <td>-0.203661</td>\n",
       "      <td>-0.795667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718281</td>\n",
       "      <td>0.556751</td>\n",
       "      <td>0.480351</td>\n",
       "      <td>0.148721</td>\n",
       "      <td>0.077893</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.202452</td>\n",
       "      <td>0.399841</td>\n",
       "      <td>0.432747</td>\n",
       "      <td>1.265924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Orlando, Florida</td>\n",
       "      <td>-0.657724</td>\n",
       "      <td>-0.906113</td>\n",
       "      <td>-0.089573</td>\n",
       "      <td>-0.790703</td>\n",
       "      <td>-0.479409</td>\n",
       "      <td>-0.801393</td>\n",
       "      <td>-0.237499</td>\n",
       "      <td>-2.286068</td>\n",
       "      <td>1.905325</td>\n",
       "      <td>...</td>\n",
       "      <td>0.920733</td>\n",
       "      <td>0.759203</td>\n",
       "      <td>0.682802</td>\n",
       "      <td>0.351173</td>\n",
       "      <td>0.280345</td>\n",
       "      <td>0.202452</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.197389</td>\n",
       "      <td>0.230295</td>\n",
       "      <td>1.063472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>El Paso, Texas</td>\n",
       "      <td>-0.315049</td>\n",
       "      <td>-0.150825</td>\n",
       "      <td>-0.281339</td>\n",
       "      <td>-0.743166</td>\n",
       "      <td>-0.727132</td>\n",
       "      <td>-1.575810</td>\n",
       "      <td>-1.847785</td>\n",
       "      <td>1.049639</td>\n",
       "      <td>-1.011747</td>\n",
       "      <td>...</td>\n",
       "      <td>1.118122</td>\n",
       "      <td>0.956592</td>\n",
       "      <td>0.880192</td>\n",
       "      <td>0.548562</td>\n",
       "      <td>0.477734</td>\n",
       "      <td>0.399841</td>\n",
       "      <td>0.197389</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032906</td>\n",
       "      <td>0.866083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Winston-Salem, North Carolina</td>\n",
       "      <td>-0.909371</td>\n",
       "      <td>-1.364115</td>\n",
       "      <td>-0.993816</td>\n",
       "      <td>-1.014749</td>\n",
       "      <td>-1.147375</td>\n",
       "      <td>0.027083</td>\n",
       "      <td>-1.111602</td>\n",
       "      <td>0.538678</td>\n",
       "      <td>-0.993740</td>\n",
       "      <td>...</td>\n",
       "      <td>1.151028</td>\n",
       "      <td>0.989498</td>\n",
       "      <td>0.913098</td>\n",
       "      <td>0.581468</td>\n",
       "      <td>0.510641</td>\n",
       "      <td>0.432747</td>\n",
       "      <td>0.230295</td>\n",
       "      <td>0.032906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.833177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Las Vegas, Nevada</td>\n",
       "      <td>3.248013</td>\n",
       "      <td>2.595954</td>\n",
       "      <td>-0.802127</td>\n",
       "      <td>-1.485246</td>\n",
       "      <td>-0.315736</td>\n",
       "      <td>1.312916</td>\n",
       "      <td>-0.501645</td>\n",
       "      <td>0.461552</td>\n",
       "      <td>-0.525568</td>\n",
       "      <td>...</td>\n",
       "      <td>1.984205</td>\n",
       "      <td>1.822675</td>\n",
       "      <td>1.746275</td>\n",
       "      <td>1.414645</td>\n",
       "      <td>1.343817</td>\n",
       "      <td>1.265924</td>\n",
       "      <td>1.063472</td>\n",
       "      <td>0.866083</td>\n",
       "      <td>0.833177</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 city  TOT_mean  \\\n",
       "0                                   Fort Worth, Texas -0.165631   \n",
       "1   Louisville/Jefferson County metro government (... -0.228196   \n",
       "2                                    Phoenix, Arizona  1.286664   \n",
       "3                           Charlotte, North Carolina -0.314859   \n",
       "4                                  Memphis, Tennessee -0.023420   \n",
       "5                                       Austin, Texas -0.065523   \n",
       "6                                     Tucson, Arizona -0.180550   \n",
       "7                               Kansas City, Missouri -0.654611   \n",
       "8                                       Dallas, Texas  0.587553   \n",
       "9                                     Wichita, Kansas -0.759692   \n",
       "10                                   Cincinnati, Ohio  0.081140   \n",
       "11                         Greensboro, North Carolina -0.928743   \n",
       "12                                   Orlando, Florida -0.657724   \n",
       "13                                     El Paso, Texas -0.315049   \n",
       "14                      Winston-Salem, North Carolina -0.909371   \n",
       "15                                  Las Vegas, Nevada  3.248013   \n",
       "\n",
       "    Avg_population  Number of employees administrative sector  \\\n",
       "0        -0.148714                                  -0.008793   \n",
       "1         0.160199                                  -0.629881   \n",
       "2         1.500587                                   1.650245   \n",
       "3         0.088878                                   0.944596   \n",
       "4        -0.103242                                   0.477276   \n",
       "5         0.178101                                   0.581356   \n",
       "6        -0.374074                                  -0.483790   \n",
       "7        -0.817708                                  -0.785886   \n",
       "8         0.931239                                   2.640475   \n",
       "9        -0.841498                                  -0.932980   \n",
       "10        0.503860                                  -0.945170   \n",
       "11       -1.252529                                  -0.340593   \n",
       "12       -0.906113                                  -0.089573   \n",
       "13       -0.150825                                  -0.281339   \n",
       "14       -1.364115                                  -0.993816   \n",
       "15        2.595954                                  -0.802127   \n",
       "\n",
       "    Number of employees manufacturing sector  \\\n",
       "0                                   1.583277   \n",
       "1                                   1.161723   \n",
       "2                                   1.491922   \n",
       "3                                   0.045956   \n",
       "4                                  -0.144607   \n",
       "5                                   0.022312   \n",
       "6                                  -1.153146   \n",
       "7                                  -0.294412   \n",
       "8                                   1.837665   \n",
       "9                                   0.430307   \n",
       "10                                 -0.637839   \n",
       "11                                 -0.309293   \n",
       "12                                 -0.790703   \n",
       "13                                 -0.743166   \n",
       "14                                 -1.014749   \n",
       "15                                 -1.485246   \n",
       "\n",
       "    Number of establishments information sector  Family income 2010  \\\n",
       "0                                     -0.351125            0.487157   \n",
       "1                                     -0.218416            0.371602   \n",
       "2                                      1.321002            0.442537   \n",
       "3                                      1.086551            1.444059   \n",
       "4                                     -0.169756           -1.515459   \n",
       "5                                      2.236691            1.547744   \n",
       "6                                     -0.594423           -0.833285   \n",
       "7                                     -0.311312            0.380469   \n",
       "8                                      1.896072           -1.145912   \n",
       "9                                     -0.722708            0.684801   \n",
       "10                                    -0.753674           -1.060819   \n",
       "11                                    -0.749250            0.234309   \n",
       "12                                    -0.479409           -0.801393   \n",
       "13                                    -0.727132           -1.575810   \n",
       "14                                    -1.147375            0.027083   \n",
       "15                                    -0.315736            1.312916   \n",
       "\n",
       "    GDP per capita 2016  Percent single-family houses  \\\n",
       "0              1.544886                      1.068921   \n",
       "1              0.206409                      0.760416   \n",
       "2             -0.666485                      0.471193   \n",
       "3              0.894501                     -0.010846   \n",
       "4             -0.238709                      0.413348   \n",
       "5              1.307759                     -0.907438   \n",
       "6             -1.599467                     -0.280788   \n",
       "7              0.535886                      0.644727   \n",
       "8              1.544886                     -1.196661   \n",
       "9             -0.176806                      1.174969   \n",
       "10             0.545666                     -1.697981   \n",
       "11            -0.199994                     -0.203661   \n",
       "12            -0.237499                     -2.286068   \n",
       "13            -1.847785                      1.049639   \n",
       "14            -1.111602                      0.538678   \n",
       "15            -0.501645                      0.461552   \n",
       "\n",
       "    Percent high density houses  ...  ('d_from_', 'Rochester, Minnesota')  \\\n",
       "0                     -0.381515  ...                             1.999056   \n",
       "1                     -0.885700  ...                             1.486713   \n",
       "2                     -0.219456  ...                             0.997660   \n",
       "3                      0.176690  ...                             0.851630   \n",
       "4                     -0.849687  ...                             0.668485   \n",
       "5                      1.725259  ...                             0.314885   \n",
       "6                      0.140677  ...                             0.000000   \n",
       "7                      0.050644  ...                             0.161530   \n",
       "8                      1.923332  ...                             0.237931   \n",
       "9                     -0.831681  ...                             0.569560   \n",
       "10                     0.572835  ...                             0.640388   \n",
       "11                    -0.795667  ...                             0.718281   \n",
       "12                     1.905325  ...                             0.920733   \n",
       "13                    -1.011747  ...                             1.118122   \n",
       "14                    -0.993740  ...                             1.151028   \n",
       "15                    -0.525568  ...                             1.984205   \n",
       "\n",
       "    ('d_from_', 'Vernal, Utah')  ('d_from_', 'Bend, Oregon')  \\\n",
       "0                      2.160587                     2.236987   \n",
       "1                      1.648244                     1.724644   \n",
       "2                      1.159190                     1.235590   \n",
       "3                      1.013160                     1.089561   \n",
       "4                      0.830016                     0.906416   \n",
       "5                      0.476416                     0.552816   \n",
       "6                      0.161530                     0.237931   \n",
       "7                      0.000000                     0.076401   \n",
       "8                      0.076401                     0.000000   \n",
       "9                      0.408030                     0.331629   \n",
       "10                     0.478858                     0.402457   \n",
       "11                     0.556751                     0.480351   \n",
       "12                     0.759203                     0.682802   \n",
       "13                     0.956592                     0.880192   \n",
       "14                     0.989498                     0.913098   \n",
       "15                     1.822675                     1.746275   \n",
       "\n",
       "    ('d_from_', 'Flagstaff, Arizona')  ('d_from_', 'Montrose, Colorado')  \\\n",
       "0                            2.568616                           2.639444   \n",
       "1                            2.056273                           2.127101   \n",
       "2                            1.567220                           1.638047   \n",
       "3                            1.421190                           1.492018   \n",
       "4                            1.238045                           1.308873   \n",
       "5                            0.884445                           0.955273   \n",
       "6                            0.569560                           0.640388   \n",
       "7                            0.408030                           0.478858   \n",
       "8                            0.331629                           0.402457   \n",
       "9                            0.000000                           0.070828   \n",
       "10                           0.070828                           0.000000   \n",
       "11                           0.148721                           0.077893   \n",
       "12                           0.351173                           0.280345   \n",
       "13                           0.548562                           0.477734   \n",
       "14                           0.581468                           0.510641   \n",
       "15                           1.414645                           1.343817   \n",
       "\n",
       "    ('d_from_', 'Clovis, California')  ('d_from_', 'Santa Rosa, California')  \\\n",
       "0                            2.717338                               2.919790   \n",
       "1                            2.204995                               2.407447   \n",
       "2                            1.715941                               1.918393   \n",
       "3                            1.569912                               1.772363   \n",
       "4                            1.386767                               1.589218   \n",
       "5                            1.033167                               1.235619   \n",
       "6                            0.718281                               0.920733   \n",
       "7                            0.556751                               0.759203   \n",
       "8                            0.480351                               0.682802   \n",
       "9                            0.148721                               0.351173   \n",
       "10                           0.077893                               0.280345   \n",
       "11                           0.000000                               0.202452   \n",
       "12                           0.202452                               0.000000   \n",
       "13                           0.399841                               0.197389   \n",
       "14                           0.432747                               0.230295   \n",
       "15                           1.265924                               1.063472   \n",
       "\n",
       "    ('d_from_', 'Spearfish, South Dakota')  ('d_from_', 'Marietta, Georgia')  \\\n",
       "0                                 3.117179                          3.150085   \n",
       "1                                 2.604836                          2.637742   \n",
       "2                                 2.115782                          2.148688   \n",
       "3                                 1.969753                          2.002659   \n",
       "4                                 1.786608                          1.819514   \n",
       "5                                 1.433008                          1.465914   \n",
       "6                                 1.118122                          1.151028   \n",
       "7                                 0.956592                          0.989498   \n",
       "8                                 0.880192                          0.913098   \n",
       "9                                 0.548562                          0.581468   \n",
       "10                                0.477734                          0.510641   \n",
       "11                                0.399841                          0.432747   \n",
       "12                                0.197389                          0.230295   \n",
       "13                                0.000000                          0.032906   \n",
       "14                                0.032906                          0.000000   \n",
       "15                                0.866083                          0.833177   \n",
       "\n",
       "    ('d_from_', 'Belgrade, Montana')  \n",
       "0                           3.983262  \n",
       "1                           3.470919  \n",
       "2                           2.981865  \n",
       "3                           2.835836  \n",
       "4                           2.652691  \n",
       "5                           2.299091  \n",
       "6                           1.984205  \n",
       "7                           1.822675  \n",
       "8                           1.746275  \n",
       "9                           1.414645  \n",
       "10                          1.343817  \n",
       "11                          1.265924  \n",
       "12                          1.063472  \n",
       "13                          0.866083  \n",
       "14                          0.833177  \n",
       "15                          0.000000  \n",
       "\n",
       "[16 rows x 36 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03f45040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>TOT_mean</th>\n",
       "      <th>Avg_population</th>\n",
       "      <th>Number of employees administrative sector</th>\n",
       "      <th>Number of employees manufacturing sector</th>\n",
       "      <th>Number of establishments information sector</th>\n",
       "      <th>Family income 2010</th>\n",
       "      <th>GDP per capita 2016</th>\n",
       "      <th>Percent single-family houses</th>\n",
       "      <th>Percent high density houses</th>\n",
       "      <th>...</th>\n",
       "      <th>('d_from_', 'Portland, Maine')</th>\n",
       "      <th>('d_from_', 'Watertown, New York')</th>\n",
       "      <th>('d_from_', 'Des Moines, Iowa')</th>\n",
       "      <th>('d_from_', 'Tacoma, Washington')</th>\n",
       "      <th>('d_from_', 'La Crosse, Wisconsin')</th>\n",
       "      <th>('d_from_', 'Philadelphia, Pennsylvania')</th>\n",
       "      <th>('d_from_', 'Bangor, Maine')</th>\n",
       "      <th>('d_from_', 'Omaha, Nebraska')</th>\n",
       "      <th>('d_from_', 'San Francisco, California')</th>\n",
       "      <th>('d_from_', 'Grand Forks, North Dakota')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Boston, Massachusetts</td>\n",
       "      <td>0.264430</td>\n",
       "      <td>0.795511</td>\n",
       "      <td>1.496522</td>\n",
       "      <td>-0.348658</td>\n",
       "      <td>0.973572</td>\n",
       "      <td>0.164685</td>\n",
       "      <td>1.216763</td>\n",
       "      <td>-1.768195</td>\n",
       "      <td>1.406956</td>\n",
       "      <td>...</td>\n",
       "      <td>2.044361</td>\n",
       "      <td>2.100952</td>\n",
       "      <td>2.137851</td>\n",
       "      <td>2.295671</td>\n",
       "      <td>2.442569</td>\n",
       "      <td>2.831406</td>\n",
       "      <td>2.866480</td>\n",
       "      <td>4.128395</td>\n",
       "      <td>4.138900</td>\n",
       "      <td>4.171846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rochester, New York</td>\n",
       "      <td>-0.113194</td>\n",
       "      <td>0.222612</td>\n",
       "      <td>-0.554986</td>\n",
       "      <td>1.165513</td>\n",
       "      <td>-0.432376</td>\n",
       "      <td>-1.891881</td>\n",
       "      <td>-0.745825</td>\n",
       "      <td>0.255713</td>\n",
       "      <td>-0.604923</td>\n",
       "      <td>...</td>\n",
       "      <td>1.196119</td>\n",
       "      <td>1.252709</td>\n",
       "      <td>1.289608</td>\n",
       "      <td>1.447429</td>\n",
       "      <td>1.594327</td>\n",
       "      <td>1.983163</td>\n",
       "      <td>2.018238</td>\n",
       "      <td>3.280153</td>\n",
       "      <td>3.290658</td>\n",
       "      <td>3.323604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Milwaukee, Wisconsin</td>\n",
       "      <td>0.222025</td>\n",
       "      <td>0.495016</td>\n",
       "      <td>1.018258</td>\n",
       "      <td>1.605725</td>\n",
       "      <td>-0.153048</td>\n",
       "      <td>-1.387606</td>\n",
       "      <td>-0.186342</td>\n",
       "      <td>-0.055657</td>\n",
       "      <td>-0.296797</td>\n",
       "      <td>...</td>\n",
       "      <td>1.145285</td>\n",
       "      <td>1.201875</td>\n",
       "      <td>1.238775</td>\n",
       "      <td>1.396595</td>\n",
       "      <td>1.543493</td>\n",
       "      <td>1.932330</td>\n",
       "      <td>1.967404</td>\n",
       "      <td>3.229319</td>\n",
       "      <td>3.239824</td>\n",
       "      <td>3.272770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Manchester, New Hampshire</td>\n",
       "      <td>-0.875688</td>\n",
       "      <td>-0.659578</td>\n",
       "      <td>0.362959</td>\n",
       "      <td>-0.569877</td>\n",
       "      <td>-0.373406</td>\n",
       "      <td>0.668868</td>\n",
       "      <td>0.030462</td>\n",
       "      <td>-0.236252</td>\n",
       "      <td>0.011328</td>\n",
       "      <td>...</td>\n",
       "      <td>1.096371</td>\n",
       "      <td>1.152961</td>\n",
       "      <td>1.189860</td>\n",
       "      <td>1.347681</td>\n",
       "      <td>1.494579</td>\n",
       "      <td>1.883415</td>\n",
       "      <td>1.918489</td>\n",
       "      <td>3.180405</td>\n",
       "      <td>3.190910</td>\n",
       "      <td>3.223856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Salt Lake City, Utah</td>\n",
       "      <td>0.479794</td>\n",
       "      <td>-0.376712</td>\n",
       "      <td>0.592136</td>\n",
       "      <td>1.795676</td>\n",
       "      <td>0.132487</td>\n",
       "      <td>-0.022110</td>\n",
       "      <td>0.069734</td>\n",
       "      <td>0.442535</td>\n",
       "      <td>0.881330</td>\n",
       "      <td>...</td>\n",
       "      <td>0.927005</td>\n",
       "      <td>0.983595</td>\n",
       "      <td>1.020494</td>\n",
       "      <td>1.178315</td>\n",
       "      <td>1.325213</td>\n",
       "      <td>1.714050</td>\n",
       "      <td>1.749124</td>\n",
       "      <td>3.011039</td>\n",
       "      <td>3.021544</td>\n",
       "      <td>3.054490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Long Beach, California</td>\n",
       "      <td>0.061147</td>\n",
       "      <td>-0.148216</td>\n",
       "      <td>-0.254608</td>\n",
       "      <td>-0.217410</td>\n",
       "      <td>-0.382717</td>\n",
       "      <td>0.085838</td>\n",
       "      <td>0.427507</td>\n",
       "      <td>-0.018293</td>\n",
       "      <td>0.138203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.507637</td>\n",
       "      <td>0.564227</td>\n",
       "      <td>0.601126</td>\n",
       "      <td>0.758947</td>\n",
       "      <td>0.905845</td>\n",
       "      <td>1.294681</td>\n",
       "      <td>1.329756</td>\n",
       "      <td>2.591671</td>\n",
       "      <td>2.602176</td>\n",
       "      <td>2.635122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Portland, Maine</td>\n",
       "      <td>-0.745319</td>\n",
       "      <td>-0.658986</td>\n",
       "      <td>-0.870781</td>\n",
       "      <td>-0.909737</td>\n",
       "      <td>-0.336163</td>\n",
       "      <td>0.586565</td>\n",
       "      <td>-0.620095</td>\n",
       "      <td>-0.248707</td>\n",
       "      <td>-0.405548</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056590</td>\n",
       "      <td>0.093489</td>\n",
       "      <td>0.251310</td>\n",
       "      <td>0.398208</td>\n",
       "      <td>0.787044</td>\n",
       "      <td>0.822119</td>\n",
       "      <td>2.084034</td>\n",
       "      <td>2.094539</td>\n",
       "      <td>2.127485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Watertown, New York</td>\n",
       "      <td>-1.120492</td>\n",
       "      <td>-0.874575</td>\n",
       "      <td>-1.113748</td>\n",
       "      <td>-1.108339</td>\n",
       "      <td>-0.727221</td>\n",
       "      <td>-0.915890</td>\n",
       "      <td>-1.012012</td>\n",
       "      <td>0.087573</td>\n",
       "      <td>-1.039924</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056590</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036899</td>\n",
       "      <td>0.194720</td>\n",
       "      <td>0.341618</td>\n",
       "      <td>0.730454</td>\n",
       "      <td>0.765528</td>\n",
       "      <td>2.027444</td>\n",
       "      <td>2.037949</td>\n",
       "      <td>2.070895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Des Moines, Iowa</td>\n",
       "      <td>-0.154956</td>\n",
       "      <td>-0.163754</td>\n",
       "      <td>-0.586519</td>\n",
       "      <td>-0.454200</td>\n",
       "      <td>-0.401339</td>\n",
       "      <td>-0.204540</td>\n",
       "      <td>1.088175</td>\n",
       "      <td>1.507423</td>\n",
       "      <td>-0.188047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.093489</td>\n",
       "      <td>0.036899</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.157821</td>\n",
       "      <td>0.304718</td>\n",
       "      <td>0.693555</td>\n",
       "      <td>0.728629</td>\n",
       "      <td>1.990545</td>\n",
       "      <td>2.001050</td>\n",
       "      <td>2.033996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Tacoma, Washington</td>\n",
       "      <td>-0.023405</td>\n",
       "      <td>-0.397697</td>\n",
       "      <td>-0.847848</td>\n",
       "      <td>-0.425034</td>\n",
       "      <td>-0.615490</td>\n",
       "      <td>0.189239</td>\n",
       "      <td>1.314870</td>\n",
       "      <td>1.339283</td>\n",
       "      <td>-0.568673</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251310</td>\n",
       "      <td>0.194720</td>\n",
       "      <td>0.157821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.146898</td>\n",
       "      <td>0.535734</td>\n",
       "      <td>0.570809</td>\n",
       "      <td>1.832724</td>\n",
       "      <td>1.843229</td>\n",
       "      <td>1.876175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>La Crosse, Wisconsin</td>\n",
       "      <td>-0.967951</td>\n",
       "      <td>-0.833198</td>\n",
       "      <td>-1.005668</td>\n",
       "      <td>-0.632782</td>\n",
       "      <td>-0.686874</td>\n",
       "      <td>-0.082587</td>\n",
       "      <td>-0.999336</td>\n",
       "      <td>0.579538</td>\n",
       "      <td>-0.151797</td>\n",
       "      <td>...</td>\n",
       "      <td>0.398208</td>\n",
       "      <td>0.341618</td>\n",
       "      <td>0.304718</td>\n",
       "      <td>0.146898</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.388837</td>\n",
       "      <td>0.423911</td>\n",
       "      <td>1.685826</td>\n",
       "      <td>1.696331</td>\n",
       "      <td>1.729277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Philadelphia, Pennsylvania</td>\n",
       "      <td>2.260045</td>\n",
       "      <td>1.593251</td>\n",
       "      <td>0.835103</td>\n",
       "      <td>1.578413</td>\n",
       "      <td>0.877360</td>\n",
       "      <td>-1.015835</td>\n",
       "      <td>0.189969</td>\n",
       "      <td>-2.035974</td>\n",
       "      <td>-0.804298</td>\n",
       "      <td>...</td>\n",
       "      <td>0.787044</td>\n",
       "      <td>0.730454</td>\n",
       "      <td>0.693555</td>\n",
       "      <td>0.535734</td>\n",
       "      <td>0.388837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035074</td>\n",
       "      <td>1.296990</td>\n",
       "      <td>1.307495</td>\n",
       "      <td>1.340440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Bangor, Maine</td>\n",
       "      <td>-1.109890</td>\n",
       "      <td>-0.864931</td>\n",
       "      <td>-1.068812</td>\n",
       "      <td>-1.113654</td>\n",
       "      <td>-0.671356</td>\n",
       "      <td>0.015085</td>\n",
       "      <td>-1.896517</td>\n",
       "      <td>0.112483</td>\n",
       "      <td>-1.420549</td>\n",
       "      <td>...</td>\n",
       "      <td>0.822119</td>\n",
       "      <td>0.765528</td>\n",
       "      <td>0.728629</td>\n",
       "      <td>0.570809</td>\n",
       "      <td>0.423911</td>\n",
       "      <td>0.035074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.261915</td>\n",
       "      <td>1.272420</td>\n",
       "      <td>1.305366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Omaha, Nebraska</td>\n",
       "      <td>0.832670</td>\n",
       "      <td>-0.193768</td>\n",
       "      <td>1.512173</td>\n",
       "      <td>0.876570</td>\n",
       "      <td>0.188352</td>\n",
       "      <td>0.394132</td>\n",
       "      <td>0.005990</td>\n",
       "      <td>1.494968</td>\n",
       "      <td>-0.876798</td>\n",
       "      <td>...</td>\n",
       "      <td>2.084034</td>\n",
       "      <td>2.027444</td>\n",
       "      <td>1.990545</td>\n",
       "      <td>1.832724</td>\n",
       "      <td>1.685826</td>\n",
       "      <td>1.296990</td>\n",
       "      <td>1.261915</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010505</td>\n",
       "      <td>0.043451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>San Francisco, California</td>\n",
       "      <td>2.007159</td>\n",
       "      <td>2.895176</td>\n",
       "      <td>1.575394</td>\n",
       "      <td>-0.281798</td>\n",
       "      <td>3.332337</td>\n",
       "      <td>2.636310</td>\n",
       "      <td>2.001109</td>\n",
       "      <td>-1.288685</td>\n",
       "      <td>1.841956</td>\n",
       "      <td>...</td>\n",
       "      <td>2.094539</td>\n",
       "      <td>2.037949</td>\n",
       "      <td>2.001050</td>\n",
       "      <td>1.843229</td>\n",
       "      <td>1.696331</td>\n",
       "      <td>1.307495</td>\n",
       "      <td>1.272420</td>\n",
       "      <td>0.010505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Grand Forks, North Dakota</td>\n",
       "      <td>-1.016375</td>\n",
       "      <td>-0.830152</td>\n",
       "      <td>-1.089575</td>\n",
       "      <td>-0.960407</td>\n",
       "      <td>-0.724118</td>\n",
       "      <td>0.779726</td>\n",
       "      <td>-0.884450</td>\n",
       "      <td>-0.167751</td>\n",
       "      <td>2.077582</td>\n",
       "      <td>...</td>\n",
       "      <td>2.127485</td>\n",
       "      <td>2.070895</td>\n",
       "      <td>2.033996</td>\n",
       "      <td>1.876175</td>\n",
       "      <td>1.729277</td>\n",
       "      <td>1.340440</td>\n",
       "      <td>1.305366</td>\n",
       "      <td>0.043451</td>\n",
       "      <td>0.032946</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          city  TOT_mean  Avg_population  \\\n",
       "0        Boston, Massachusetts  0.264430        0.795511   \n",
       "1          Rochester, New York -0.113194        0.222612   \n",
       "2         Milwaukee, Wisconsin  0.222025        0.495016   \n",
       "3    Manchester, New Hampshire -0.875688       -0.659578   \n",
       "4         Salt Lake City, Utah  0.479794       -0.376712   \n",
       "5       Long Beach, California  0.061147       -0.148216   \n",
       "6              Portland, Maine -0.745319       -0.658986   \n",
       "7          Watertown, New York -1.120492       -0.874575   \n",
       "8             Des Moines, Iowa -0.154956       -0.163754   \n",
       "9           Tacoma, Washington -0.023405       -0.397697   \n",
       "10        La Crosse, Wisconsin -0.967951       -0.833198   \n",
       "11  Philadelphia, Pennsylvania  2.260045        1.593251   \n",
       "12               Bangor, Maine -1.109890       -0.864931   \n",
       "13             Omaha, Nebraska  0.832670       -0.193768   \n",
       "14   San Francisco, California  2.007159        2.895176   \n",
       "15   Grand Forks, North Dakota -1.016375       -0.830152   \n",
       "\n",
       "    Number of employees administrative sector  \\\n",
       "0                                    1.496522   \n",
       "1                                   -0.554986   \n",
       "2                                    1.018258   \n",
       "3                                    0.362959   \n",
       "4                                    0.592136   \n",
       "5                                   -0.254608   \n",
       "6                                   -0.870781   \n",
       "7                                   -1.113748   \n",
       "8                                   -0.586519   \n",
       "9                                   -0.847848   \n",
       "10                                  -1.005668   \n",
       "11                                   0.835103   \n",
       "12                                  -1.068812   \n",
       "13                                   1.512173   \n",
       "14                                   1.575394   \n",
       "15                                  -1.089575   \n",
       "\n",
       "    Number of employees manufacturing sector  \\\n",
       "0                                  -0.348658   \n",
       "1                                   1.165513   \n",
       "2                                   1.605725   \n",
       "3                                  -0.569877   \n",
       "4                                   1.795676   \n",
       "5                                  -0.217410   \n",
       "6                                  -0.909737   \n",
       "7                                  -1.108339   \n",
       "8                                  -0.454200   \n",
       "9                                  -0.425034   \n",
       "10                                 -0.632782   \n",
       "11                                  1.578413   \n",
       "12                                 -1.113654   \n",
       "13                                  0.876570   \n",
       "14                                 -0.281798   \n",
       "15                                 -0.960407   \n",
       "\n",
       "    Number of establishments information sector  Family income 2010  \\\n",
       "0                                      0.973572            0.164685   \n",
       "1                                     -0.432376           -1.891881   \n",
       "2                                     -0.153048           -1.387606   \n",
       "3                                     -0.373406            0.668868   \n",
       "4                                      0.132487           -0.022110   \n",
       "5                                     -0.382717            0.085838   \n",
       "6                                     -0.336163            0.586565   \n",
       "7                                     -0.727221           -0.915890   \n",
       "8                                     -0.401339           -0.204540   \n",
       "9                                     -0.615490            0.189239   \n",
       "10                                    -0.686874           -0.082587   \n",
       "11                                     0.877360           -1.015835   \n",
       "12                                    -0.671356            0.015085   \n",
       "13                                     0.188352            0.394132   \n",
       "14                                     3.332337            2.636310   \n",
       "15                                    -0.724118            0.779726   \n",
       "\n",
       "    GDP per capita 2016  Percent single-family houses  \\\n",
       "0              1.216763                     -1.768195   \n",
       "1             -0.745825                      0.255713   \n",
       "2             -0.186342                     -0.055657   \n",
       "3              0.030462                     -0.236252   \n",
       "4              0.069734                      0.442535   \n",
       "5              0.427507                     -0.018293   \n",
       "6             -0.620095                     -0.248707   \n",
       "7             -1.012012                      0.087573   \n",
       "8              1.088175                      1.507423   \n",
       "9              1.314870                      1.339283   \n",
       "10            -0.999336                      0.579538   \n",
       "11             0.189969                     -2.035974   \n",
       "12            -1.896517                      0.112483   \n",
       "13             0.005990                      1.494968   \n",
       "14             2.001109                     -1.288685   \n",
       "15            -0.884450                     -0.167751   \n",
       "\n",
       "    Percent high density houses  ...  ('d_from_', 'Portland, Maine')  \\\n",
       "0                      1.406956  ...                        2.044361   \n",
       "1                     -0.604923  ...                        1.196119   \n",
       "2                     -0.296797  ...                        1.145285   \n",
       "3                      0.011328  ...                        1.096371   \n",
       "4                      0.881330  ...                        0.927005   \n",
       "5                      0.138203  ...                        0.507637   \n",
       "6                     -0.405548  ...                        0.000000   \n",
       "7                     -1.039924  ...                        0.056590   \n",
       "8                     -0.188047  ...                        0.093489   \n",
       "9                     -0.568673  ...                        0.251310   \n",
       "10                    -0.151797  ...                        0.398208   \n",
       "11                    -0.804298  ...                        0.787044   \n",
       "12                    -1.420549  ...                        0.822119   \n",
       "13                    -0.876798  ...                        2.084034   \n",
       "14                     1.841956  ...                        2.094539   \n",
       "15                     2.077582  ...                        2.127485   \n",
       "\n",
       "    ('d_from_', 'Watertown, New York')  ('d_from_', 'Des Moines, Iowa')  \\\n",
       "0                             2.100952                         2.137851   \n",
       "1                             1.252709                         1.289608   \n",
       "2                             1.201875                         1.238775   \n",
       "3                             1.152961                         1.189860   \n",
       "4                             0.983595                         1.020494   \n",
       "5                             0.564227                         0.601126   \n",
       "6                             0.056590                         0.093489   \n",
       "7                             0.000000                         0.036899   \n",
       "8                             0.036899                         0.000000   \n",
       "9                             0.194720                         0.157821   \n",
       "10                            0.341618                         0.304718   \n",
       "11                            0.730454                         0.693555   \n",
       "12                            0.765528                         0.728629   \n",
       "13                            2.027444                         1.990545   \n",
       "14                            2.037949                         2.001050   \n",
       "15                            2.070895                         2.033996   \n",
       "\n",
       "    ('d_from_', 'Tacoma, Washington')  ('d_from_', 'La Crosse, Wisconsin')  \\\n",
       "0                            2.295671                             2.442569   \n",
       "1                            1.447429                             1.594327   \n",
       "2                            1.396595                             1.543493   \n",
       "3                            1.347681                             1.494579   \n",
       "4                            1.178315                             1.325213   \n",
       "5                            0.758947                             0.905845   \n",
       "6                            0.251310                             0.398208   \n",
       "7                            0.194720                             0.341618   \n",
       "8                            0.157821                             0.304718   \n",
       "9                            0.000000                             0.146898   \n",
       "10                           0.146898                             0.000000   \n",
       "11                           0.535734                             0.388837   \n",
       "12                           0.570809                             0.423911   \n",
       "13                           1.832724                             1.685826   \n",
       "14                           1.843229                             1.696331   \n",
       "15                           1.876175                             1.729277   \n",
       "\n",
       "    ('d_from_', 'Philadelphia, Pennsylvania')  ('d_from_', 'Bangor, Maine')  \\\n",
       "0                                    2.831406                      2.866480   \n",
       "1                                    1.983163                      2.018238   \n",
       "2                                    1.932330                      1.967404   \n",
       "3                                    1.883415                      1.918489   \n",
       "4                                    1.714050                      1.749124   \n",
       "5                                    1.294681                      1.329756   \n",
       "6                                    0.787044                      0.822119   \n",
       "7                                    0.730454                      0.765528   \n",
       "8                                    0.693555                      0.728629   \n",
       "9                                    0.535734                      0.570809   \n",
       "10                                   0.388837                      0.423911   \n",
       "11                                   0.000000                      0.035074   \n",
       "12                                   0.035074                      0.000000   \n",
       "13                                   1.296990                      1.261915   \n",
       "14                                   1.307495                      1.272420   \n",
       "15                                   1.340440                      1.305366   \n",
       "\n",
       "    ('d_from_', 'Omaha, Nebraska')  ('d_from_', 'San Francisco, California')  \\\n",
       "0                         4.128395                                  4.138900   \n",
       "1                         3.280153                                  3.290658   \n",
       "2                         3.229319                                  3.239824   \n",
       "3                         3.180405                                  3.190910   \n",
       "4                         3.011039                                  3.021544   \n",
       "5                         2.591671                                  2.602176   \n",
       "6                         2.084034                                  2.094539   \n",
       "7                         2.027444                                  2.037949   \n",
       "8                         1.990545                                  2.001050   \n",
       "9                         1.832724                                  1.843229   \n",
       "10                        1.685826                                  1.696331   \n",
       "11                        1.296990                                  1.307495   \n",
       "12                        1.261915                                  1.272420   \n",
       "13                        0.000000                                  0.010505   \n",
       "14                        0.010505                                  0.000000   \n",
       "15                        0.043451                                  0.032946   \n",
       "\n",
       "    ('d_from_', 'Grand Forks, North Dakota')  \n",
       "0                                   4.171846  \n",
       "1                                   3.323604  \n",
       "2                                   3.272770  \n",
       "3                                   3.223856  \n",
       "4                                   3.054490  \n",
       "5                                   2.635122  \n",
       "6                                   2.127485  \n",
       "7                                   2.070895  \n",
       "8                                   2.033996  \n",
       "9                                   1.876175  \n",
       "10                                  1.729277  \n",
       "11                                  1.340440  \n",
       "12                                  1.305366  \n",
       "13                                  0.043451  \n",
       "14                                  0.032946  \n",
       "15                                  0.000000  \n",
       "\n",
       "[16 rows x 36 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "669696bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(c):\n",
    "    labels = []\n",
    "    nodes = []\n",
    "    X = []\n",
    "    for i,data in enumerate(c.index): \n",
    "        labels.append(c.loc[i,'TOT_mean'])\n",
    "        X.append(c.iloc[i,2:])\n",
    "        nodes.append(c.loc[i,'city'])\n",
    "\n",
    "    X = np.array(X,dtype=float)  \n",
    "    N = X.shape[0] #the number of nodes \n",
    "    F = X.shape[1] #the size of node features #1\n",
    "    print('X shape: ', X.shape)        \n",
    "\n",
    "    print('\\nNumber of nodes (N): ', N)\n",
    "    print('\\nNumber of features (F) of each node: ', F)\n",
    "    #print('\\nsorted targets: ', sorted(set(labels)))\n",
    "    print('labels:', labels)\n",
    "\n",
    "    num_classes = len(set(labels))\n",
    "    print('\\nNumber of classes: ', num_classes)\n",
    "    \n",
    "    V=[]\n",
    "    a = np.zeros((N, N))\n",
    "    for i in range(N):\n",
    "        for j in range(1,N+1):\n",
    "            #print('self.cluster.loc[',i,',',j,']',self.cluster.iloc[i,j])\n",
    "            if(c.iloc[i,j]<=2):\n",
    "                a[i,j-1]=1\n",
    "                V.append(c.iloc[i,j])\n",
    "            #print ('a[',i,',',j-2,']:', a[i,j-2] )     \n",
    "        #print('a:', a)           \n",
    "    a = np.maximum(a, a.T).astype(int) #.T do the transpose\n",
    "    a = sp.csr_matrix(a)                   \n",
    "    print('Shape of A: ', a.shape)\n",
    "    print('\\nAdjacency Matrix (a):\\n', a)\n",
    "    # Preprocessing operations\n",
    "    a = GCNConv.preprocess(a).astype('f4')\n",
    "    print('X:',X)\n",
    "    \n",
    "    #r,v= a.nonzero()\n",
    "    #e = sp.csr_matrix((V,(r,v))) \n",
    "    #ee=e.toarray()\n",
    "    \n",
    "    return labels,nodes,X,N,F,a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac4063e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (16, 34)\n",
      "\n",
      "Number of nodes (N):  16\n",
      "\n",
      "Number of features (F) of each node:  34\n",
      "labels: [-0.1656311673202647, -0.2281957811227475, 1.2866635452399902, -0.3148588629083396, -0.023419867114868, -0.0655231480934081, -0.1805503599413035, -0.6546105245838832, 0.5875526102590108, -0.7596920778723519, 0.0811401931011167, -0.9287432655190312, -0.6577243253464531, -0.3150488778099439, -0.9093710254455416, 3.248012934478018]\n",
      "\n",
      "Number of classes:  16\n",
      "Shape of A:  (16, 16)\n",
      "\n",
      "Adjacency Matrix (a):\n",
      "   (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 14)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 9)\t1\n",
      "  :\t:\n",
      "  (14, 6)\t1\n",
      "  (14, 7)\t1\n",
      "  (14, 8)\t1\n",
      "  (14, 9)\t1\n",
      "  (14, 10)\t1\n",
      "  (14, 11)\t1\n",
      "  (14, 12)\t1\n",
      "  (14, 13)\t1\n",
      "  (14, 14)\t1\n",
      "  (14, 15)\t1\n",
      "  (15, 1)\t1\n",
      "  (15, 2)\t1\n",
      "  (15, 3)\t1\n",
      "  (15, 4)\t1\n",
      "  (15, 5)\t1\n",
      "  (15, 6)\t1\n",
      "  (15, 7)\t1\n",
      "  (15, 8)\t1\n",
      "  (15, 9)\t1\n",
      "  (15, 10)\t1\n",
      "  (15, 11)\t1\n",
      "  (15, 12)\t1\n",
      "  (15, 13)\t1\n",
      "  (15, 14)\t1\n",
      "  (15, 15)\t1\n",
      "X: [[-0.14871364 -0.00879312  1.58327703 -0.3511245   0.48715704  1.544886\n",
      "   1.06892066 -0.38151516  0.80766647 -1.16196945 -0.18853105  0.34064624\n",
      "   0.37588492  1.54752343  2.65449444  1.36995034  1.80814018  2.19848637\n",
      "   0.          0.51234304  1.00139694  1.14742617  1.33057107  1.68417098\n",
      "   1.99905647  2.16058662  2.23698725  2.56861647  2.63944433  2.7173378\n",
      "   2.91978957  3.11717879  3.15008484  3.98326182]\n",
      " [ 0.16019898 -0.62988118  1.16172295 -0.21841603  0.37160226  0.20640866\n",
      "   0.76041594 -0.8857004  -0.60981034 -0.42860188  0.92868997  0.62750623\n",
      "  -1.00417566  0.68056139  0.18806586 -0.27349963  0.17483321  1.60430108\n",
      "   0.51234304  0.          0.4890539   0.63508313  0.81822804  1.17182795\n",
      "   1.48671343  1.64824359  1.72464421  2.05627344  2.12710129  2.20499477\n",
      "   2.40744653  2.60483576  2.63774181  3.47091879]\n",
      " [ 1.50058667  1.65024496  1.4919224   1.32100226  0.44253688 -0.66648504\n",
      "   0.47119276 -0.21945563  0.24067575  0.85800789 -0.79011159  0.05378625\n",
      "   1.18939431 -0.94887395  1.49234061  2.61497165  0.79935228  1.01546201\n",
      "   1.00139694  0.4890539   0.          0.14602923  0.32917414  0.68277405\n",
      "   0.99765953  1.15918968  1.23559031  1.56721954  1.63804739  1.71594086\n",
      "   1.91839263  2.11578186  2.14868791  2.98186489]\n",
      " [ 0.08887816  0.94459634  0.04595634  1.08655063  1.44405924  0.89450055\n",
      "  -0.01084587  0.17668991  1.29590849 -0.99471018 -0.70417151  0.91436621\n",
      "   0.6954779   1.51935437  0.43229473 -0.5073877  -0.26998307 -0.89556357\n",
      "   1.14742617  0.63508313  0.14602923  0.          0.18314491  0.53674482\n",
      "   0.8516303   1.01316045  1.08956108  1.42119031  1.49201816  1.56991163\n",
      "   1.7723634   1.96975263  2.00265868  2.83583565]\n",
      " [-0.1032424   0.4772758  -0.14460694 -0.16975626 -1.51545864 -0.23870865\n",
      "   0.41334812 -0.84968716 -1.21617542  1.46271449  0.29273339  0.34064624\n",
      "   0.95696306  0.29957828 -0.19269354  0.04106326  0.33941615 -0.7478349\n",
      "   1.33057107  0.81822804  0.32917414  0.18314491  0.          0.35359991\n",
      "   0.6684854   0.83001555  0.90641618  1.2380454   1.30887325  1.38676673\n",
      "   1.58921849  1.78660772  1.81951377  2.65269075]\n",
      " [ 0.17810111  0.58135627  0.02231161  2.23669073  1.5477439   1.30775896\n",
      "  -0.90743772  1.72525884  0.72104289 -0.19701212 -0.72995354 -1.66737368\n",
      "  -0.80079831  0.64077487 -0.68600424  0.05948277  0.25439108  1.05603039\n",
      "   1.68417098  1.17182795  0.68277405  0.53674482  0.35359991  0.\n",
      "   0.31488549  0.47641564  0.55281626  0.88444549  0.95527334  1.03316682\n",
      "   1.23561858  1.43300781  1.46591386  2.29909084]\n",
      " [-0.37407391 -0.48379047 -1.15314553 -0.59442337 -0.83328505 -1.59946707\n",
      "  -0.2807875   0.14067668 -0.49956214  1.51417888 -0.51510334 -1.09365371\n",
      "  -0.24877408 -0.8297537   0.53985631 -0.159834   -0.95603643 -0.21696773\n",
      "   1.99905647  1.48671343  0.99765953  0.8516303   0.6684854   0.31488549\n",
      "   0.          0.16153015  0.23793078  0.56956001  0.64038786  0.71828133\n",
      "   0.9207331   1.11812232  1.15102838  1.98420535]\n",
      " [-0.8177079  -0.78588616 -0.294412   -0.31131196  0.38046909  0.53588621\n",
      "   0.64472667  0.05064361 -0.8381816  -0.42860188  1.21229222  1.2012262\n",
      "   0.08534585 -0.34579742 -0.16396248 -0.56101366 -0.35594958 -0.80809939\n",
      "   2.16058662  1.64824359  1.15918968  1.01316045  0.83001555  0.47641564\n",
      "   0.16153015  0.          0.07640063  0.40802985  0.47885771  0.55675118\n",
      "   0.75920295  0.95659217  0.98949822  1.8226752 ]\n",
      " [ 0.93123902  2.64047537  1.83766454  1.89607232 -1.14591219  1.544886\n",
      "  -1.1966609   1.92333161 -0.65705956  1.398384   -0.2486891  -1.66737368\n",
      "   1.30560994  0.23537744  0.29168442  0.14988415  1.86627164 -0.269606\n",
      "   2.23698725  1.72464421  1.23559031  1.08956108  0.90641618  0.55281626\n",
      "   0.23793078  0.07640063  0.          0.33162923  0.40245708  0.48035055\n",
      "   0.68280232  0.88019154  0.9130976   1.74627457]\n",
      " [-0.84149812 -0.93297987  0.43030718 -0.72270823  0.68480146 -0.17680559\n",
      "   1.17496916 -0.83168055 -0.57831085  0.39482837  0.40445549  1.48808619\n",
      "  -1.16397215 -1.24115073  0.03554919 -0.82154244 -0.82155708  0.34834797\n",
      "   2.56861647  2.05627344  1.56721954  1.42119031  1.2380454   0.88444549\n",
      "   0.56956001  0.40802985  0.33162923  0.          0.07082785  0.14872133\n",
      "   0.35117309  0.54856232  0.58146837  1.41464535]\n",
      " [ 0.50386013 -0.94517017 -0.63783927 -0.75367354 -1.06081926  0.54566569\n",
      "  -1.69798107  0.57283545 -1.98003792 -1.4450236   3.0685979  -1.3805137\n",
      "  -0.03086978  1.12274268  0.46860851 -0.21580282 -0.2082106  -0.05906552\n",
      "   2.63944433  2.12710129  1.63804739  1.49201816  1.30887325  0.95527334\n",
      "   0.64038786  0.47885771  0.40245708  0.07082785  0.          0.07789347\n",
      "   0.28034524  0.47773447  0.51064052  1.3438175 ]\n",
      " [-1.25252902 -0.34059302 -0.3092933  -0.74924992  0.23430947 -0.19999403\n",
      "  -0.20366132 -0.79566732  0.31154959  0.34336398 -0.42916326  0.34064624\n",
      "  -0.51025924 -0.46084957 -1.13549576 -1.00018257 -0.91807233 -1.3354412\n",
      "   2.7173378   2.20499477  1.71594086  1.56991163  1.38676673  1.03316682\n",
      "   0.71828133  0.55675118  0.48035055  0.14872133  0.07789347  0.\n",
      "   0.20245177  0.39984099  0.43274704  1.26592402]\n",
      " [-0.90611299 -0.08957315 -0.79070327 -0.47940936 -0.80139308 -0.23749882\n",
      "  -2.2860682   1.90532499  0.80766647 -0.51866456 -0.66979548 -1.09365371\n",
      "  -1.73052333 -0.11148828 -0.99560505 -0.87977991 -1.4358677   0.32340942\n",
      "   2.91978957  2.40744653  1.91839263  1.7723634   1.58921849  1.23561858\n",
      "   0.9207331   0.75920295  0.68280232  0.35117309  0.28034524  0.20245177\n",
      "   0.          0.19738923  0.23029528  1.06347225]\n",
      " [-0.15082511 -0.28133891 -0.74316579 -0.72713185 -1.57581027 -1.84778506\n",
      "   1.04963912 -1.0117467   0.24067575  0.2661674  -0.45494529  0.62750623\n",
      "   0.75358571 -1.63974519 -0.88857278 -0.15666426 -0.37553847 -1.30254936\n",
      "   3.11717879  2.60483576  2.11578186  1.96975263  1.78660772  1.43300781\n",
      "   1.11812232  0.95659217  0.88019154  0.54856232  0.47773447  0.39984099\n",
      "   0.19738923  0.          0.03290605  0.86608303]\n",
      " [-1.3641149  -0.99381564 -1.01474947 -1.14737535  0.02708315 -1.11160236\n",
      "   0.53867817 -0.99374009 -0.22394165  0.66501643 -0.11118498  0.62750623\n",
      "  -1.42545731  0.93745022 -1.3521295  -1.24520186 -1.27645705 -0.76212189\n",
      "   3.15008484  2.63774181  2.14868791  2.00265868  1.81951377  1.46591386\n",
      "   1.15102838  0.98949822  0.9130976   0.58146837  0.51064052  0.43274704\n",
      "   0.23029528  0.03290605  0.          0.83317698]\n",
      " [ 2.59595392 -0.80212703 -1.48524649 -0.31573558  1.31291601 -0.50164545\n",
      "   0.46155199 -0.52556809  2.17789406 -1.72807775 -1.06511984  0.34064624\n",
      "   1.55256815 -1.40570384 -0.68843071  1.58555671  1.37526778 -0.14878769\n",
      "   3.98326182  3.47091879  2.98186489  2.83583565  2.65269075  2.29909084\n",
      "   1.98420535  1.8226752   1.74627457  1.41464535  1.3438175   1.26592402\n",
      "   1.06347225  0.86608303  0.83317698  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "targets2,nodes2,x2,n2,f2,a2=build_graph(c2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dd1520c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (16, 34)\n",
      "\n",
      "Number of nodes (N):  16\n",
      "\n",
      "Number of features (F) of each node:  34\n",
      "labels: [0.2644297840685283, -0.1131941650064338, 0.2220246864052988, -0.8756880962435769, 0.4797944079872109, 0.0611467202141334, -0.745318909607009, -1.120491711783869, -0.1549562668993513, -0.0234054809798646, -0.967950728929834, 2.260045325646394, -1.109889920872829, 0.8326701739128662, 2.0071594618698563, -1.016375279781519]\n",
      "\n",
      "Number of classes:  16\n",
      "Shape of A:  (16, 16)\n",
      "\n",
      "Adjacency Matrix (a):\n",
      "   (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 15)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 8)\t1\n",
      "  :\t:\n",
      "  (14, 7)\t1\n",
      "  (14, 8)\t1\n",
      "  (14, 9)\t1\n",
      "  (14, 10)\t1\n",
      "  (14, 11)\t1\n",
      "  (14, 12)\t1\n",
      "  (14, 13)\t1\n",
      "  (14, 14)\t1\n",
      "  (14, 15)\t1\n",
      "  (15, 0)\t1\n",
      "  (15, 1)\t1\n",
      "  (15, 2)\t1\n",
      "  (15, 3)\t1\n",
      "  (15, 4)\t1\n",
      "  (15, 5)\t1\n",
      "  (15, 6)\t1\n",
      "  (15, 7)\t1\n",
      "  (15, 8)\t1\n",
      "  (15, 9)\t1\n",
      "  (15, 10)\t1\n",
      "  (15, 11)\t1\n",
      "  (15, 12)\t1\n",
      "  (15, 13)\t1\n",
      "  (15, 14)\t1\n",
      "  (15, 15)\t1\n",
      "X: [[ 0.79551149  1.49652223 -0.34865832  0.97357246  0.16468483  1.21676254\n",
      "  -1.76819546  1.40695564 -0.08297516 -1.4549349   1.04739908 -1.47736088\n",
      "   1.67388065 -0.97540936 -0.76260471  0.03074196  0.55964272 -0.62138983\n",
      "   0.          0.84824237  0.89907609  0.94799047  1.11735619  1.53672429\n",
      "   2.04436122  2.10095154  2.13785061  2.29567131  2.4425691   2.83140571\n",
      "   2.86647993  4.12839523  4.13890039  4.17184619]\n",
      " [ 0.22261238 -0.55498559  1.16551319 -0.43237557 -1.89188054 -0.74582505\n",
      "   0.25571306 -0.60492295 -1.41057764 -1.34760363  1.54417382  0.52961994\n",
      "   0.59401532 -0.56654901 -0.46956513 -0.56193904 -0.62634911 -0.84511627\n",
      "   0.84824237  0.          0.05083372  0.0997481   0.26911382  0.68848192\n",
      "   1.19611885  1.25270917  1.28960824  1.44742894  1.59432673  1.98316334\n",
      "   2.01823756  3.28015286  3.29065802  3.32360382]\n",
      " [ 0.49501572  1.01825803  1.60572532 -0.15304815 -1.38760642 -0.18634178\n",
      "  -0.05565748 -0.2967974  -1.03612565  0.2981424   0.37985803 -0.13937367\n",
      "   0.93728377  0.03061335  0.87164731  1.35461535  0.44634655  0.18022941\n",
      "   0.89907609  0.05083372  0.          0.04891438  0.2182801   0.6376482\n",
      "   1.14528513  1.20187545  1.23877452  1.39659522  1.54349301  1.93232962\n",
      "   1.96740384  3.22931914  3.2398243   3.2727701 ]\n",
      " [-0.65957835  0.36295937 -0.56987665 -0.37340645  0.66886801  0.03046165\n",
      "  -0.2362524   0.01132815  0.03616866  0.38162227 -0.32649355 -0.13937367\n",
      "  -0.30706435 -0.27337621 -0.42922854 -0.23039537 -0.56884678 -0.40852647\n",
      "   0.94799047  0.0997481   0.04891438  0.          0.16936573  0.58873382\n",
      "   1.09637076  1.15296107  1.18986015  1.34768085  1.49457863  1.88341525\n",
      "   1.91848946  3.18040476  3.19090992  3.22385573]\n",
      " [-0.37671193  0.59213571  1.79567648  0.13248655 -0.02211031  0.06973392\n",
      "   0.44253538  0.8813297   0.12127138  0.31006809 -0.27215882 -0.36237154\n",
      "   0.32226113  0.43984001  1.04680774  0.76845687  0.7680447   0.52755618\n",
      "   1.11735619  0.26911382  0.2182801   0.16936573  0.          0.4193681\n",
      "   0.92700503  0.98359535  1.02049442  1.17831512  1.32521291  1.71404952\n",
      "   1.74912373  3.01103903  3.0215442   3.05449   ]\n",
      " [-0.14821591 -0.25460809 -0.21741035 -0.38271736  0.085838    0.4275072\n",
      "  -0.01829302  0.13820337 -0.95102293  1.93196273 -0.78445777 -1.70035875\n",
      "   1.79545489 -0.89215053 -0.68617802  0.70064112  0.2888642  -0.57144287\n",
      "   1.53672429  0.68848192  0.6376482   0.58873382  0.4193681   0.\n",
      "   0.50763693  0.56422725  0.60112633  0.75894702  0.90584481  1.29468142\n",
      "   1.32975564  2.59167094  2.6021761   2.63512191]\n",
      " [-0.65898607 -0.87078097 -0.90973722 -0.33616279  0.58656538 -0.62009518\n",
      "  -0.24870722 -0.4055476  -0.2531806  -1.01368415  0.83006014 -0.13937367\n",
      "  -1.45844394  2.17295541  0.34623837 -0.59140871 -0.53606727  0.98597497\n",
      "   2.04436122  1.19611885  1.14528513  1.09637076  0.92700503  0.50763693\n",
      "   0.          0.05659032  0.09348939  0.25131009  0.39820788  0.78704449\n",
      "   0.8221187   2.08403401  2.09453917  2.12748497]\n",
      " [-0.87457463 -1.11374817 -1.10833938 -0.72722119 -0.91588958 -1.01201189\n",
      "   0.08757296 -1.03992373 -1.08718729 -0.93020428  1.14830645  0.75261781\n",
      "  -0.25700437 -1.14604257 -1.01875862 -1.32185155 -1.13157567 -1.32199733\n",
      "   2.10095154  1.25270917  1.20187545  1.15296107  0.98359535  0.56422725\n",
      "   0.05659032  0.          0.03689907  0.19471977  0.34161756  0.73045417\n",
      "   0.76552839  2.02744369  2.03794885  2.07089466]\n",
      " [-0.16375428 -0.58651864 -0.45420048 -0.40133919 -0.20454022  1.08817518\n",
      "   1.50742263 -0.18804721  0.51274391  0.56050771 -0.52830829  0.30662207\n",
      "  -0.87202701  1.06380723  0.59275241 -0.66226217 -0.62817485  0.07565855\n",
      "   2.13785061  1.28960824  1.23877452  1.18986015  1.02049442  0.60112633\n",
      "   0.09348939  0.03689907  0.          0.1578207   0.30471849  0.6935551\n",
      "   0.72862931  1.99054461  2.00104978  2.03399558]\n",
      " [-0.397697   -0.84784784 -0.42503426 -0.61549022  0.18923921  1.31486994\n",
      "   1.33928254 -0.56867289  1.17654515  0.32199379 -0.67578829  0.52961994\n",
      "  -0.14973298  0.79916308  0.83173434  0.7980186   0.09592236  0.56563502\n",
      "   2.29567131  1.44742894  1.39659522  1.34768085  1.17831512  0.75894702\n",
      "   0.25131009  0.19471977  0.1578207   0.          0.14689779  0.5357344\n",
      "   0.57080861  1.83272392  1.84322908  1.87617488]\n",
      " [-0.83319842 -1.00566805 -0.63278176 -0.68687389 -0.08258682 -0.99933633\n",
      "   0.57953842 -0.15179715  0.61486717  0.47702784 -0.66026408  0.52961994\n",
      "  -0.31421578 -1.05112664 -0.87119528 -1.02960077 -0.9634069  -1.07410807\n",
      "   2.4425691   1.59432673  1.54349301  1.49457863  1.32521291  0.90584481\n",
      "   0.39820788  0.34161756  0.30471849  0.14689779  0.          0.38883661\n",
      "   0.42391083  1.68582613  1.69633129  1.72927709]\n",
      " [ 1.59325149  0.83510291  1.57841289  0.87735968 -1.01583498  0.1899686\n",
      "  -2.03597413 -0.80429831 -1.03612565 -0.51280492  0.86110856  1.42161142\n",
      "   1.09461514  1.06662993  0.14642551  2.2975528   2.94120732  0.96397018\n",
      "   2.83140571  1.98316334  1.93232962  1.88341525  1.71404952  1.29468142\n",
      "   0.78704449  0.73045417  0.6935551   0.5357344   0.38883661  0.\n",
      "   0.03507421  1.29698952  1.30749468  1.34044048]\n",
      " [-0.86493125 -1.06881163 -1.11365356 -0.6713557   0.01508503 -1.89651713\n",
      "   0.11248261 -1.42054941  0.22339465 -0.03577709 -0.1013925   0.0836242\n",
      "  -1.02220695 -0.73364846 -0.86850893 -1.12553172 -0.89654139 -0.888893\n",
      "   2.86647993  2.01823756  1.96740384  1.91848946  1.74912373  1.32975564\n",
      "   0.8221187   0.76552839  0.72862931  0.57080861  0.42391083  0.03507421\n",
      "   0.          1.2619153   1.27242046  1.30536627]\n",
      " [-0.19376753  1.51217254  0.87656992  0.18835203  0.39413183  0.00598975\n",
      "   1.49496781 -0.87679844  0.59784663  1.37145503 -1.40542619  1.86760716\n",
      "   0.24359545  1.45152967  2.79083853  0.78079179  0.60531964  0.9453885\n",
      "   4.12839523  3.28015286  3.22931914  3.18040476  3.01103903  2.59167094\n",
      "   2.08403401  2.02744369  1.99054461  1.83272392  1.68582613  1.29698952\n",
      "   1.2619153   0.          0.01050516  0.04345097]\n",
      " [ 2.89517622  1.5753936  -0.28179848  3.33233736  2.6363101   2.00110885\n",
      "  -1.28868483  1.84195641 -0.11701624 -1.4549349   1.0706854  -1.92335662\n",
      "  -1.67298672 -0.62804505 -0.81968967 -0.05774596  0.63362693  2.5010019\n",
      "   4.13890039  3.29065802  3.2398243   3.19090992  3.0215442   2.6021761\n",
      "   2.09453917  2.03794885  2.00104978  1.84322908  1.69633129  1.30749468\n",
      "   1.27242046  0.01050516  0.          0.0329458 ]\n",
      " [-0.83015194 -1.08957541 -0.96040734 -0.72411755  0.77972647 -0.88445029\n",
      "  -0.16775088  2.07758183  2.69137362  1.09716402 -2.12730198 -0.13937367\n",
      "  -0.60742425 -0.75819085 -0.70071531 -1.15008321 -0.98801247 -1.01394086\n",
      "   4.17184619  3.32360382  3.2727701   3.22385573  3.05449     2.63512191\n",
      "   2.12748497  2.07089466  2.03399558  1.87617488  1.72927709  1.34044048\n",
      "   1.30536627  0.04345097  0.0329458   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "targets4,nodes4,x4,n4,f4,a4=build_graph(c4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c54a3b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.16563117  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.         -0.22819578  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.          0.          1.28666355  0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.         -0.31485886  0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.         -0.02341987  0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.         -0.06552315\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "  -0.18055036  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.         -0.65461052  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.58755261  0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.         -0.75969208  0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.08114019  0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.         -0.92874327\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "  -0.65772433  0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.         -0.31504888  0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.         -0.90937103  0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          3.24801293]]\n"
     ]
    }
   ],
   "source": [
    "yy2=np.zeros(shape=(16,16))\n",
    "for i,data in enumerate(targets2):\n",
    "    #print((i,data))\n",
    "    yy2[i,i]=data\n",
    "    #print(l[i,i])\n",
    "print(yy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b402dec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.26442978 -0.11319417  0.22202469 -0.8756881   0.47979441  0.06114672\n",
      " -0.74531891 -1.12049171 -0.15495627 -0.02340548 -0.96795073  2.26004533\n",
      " -1.10988992  0.83267017  2.00715946 -1.01637528]\n"
     ]
    }
   ],
   "source": [
    "yy4=np.zeros(shape=(16,))\n",
    "for i,data in enumerate(targets4):\n",
    "    #print((i,data))\n",
    "    yy4[i]=data\n",
    "    #print(l[i,i])\n",
    "print(yy4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "160dc812",
   "metadata": {},
   "outputs": [],
   "source": [
    "#targets1,nodes1,x1,n1,f1,a1,e1=build_graph(c4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6fb444b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from sklearn import preprocessing\\nfrom keras.utils import to_categorical\\ndef encode_label(labels):\\n    label_encoder = preprocessing.LabelEncoder()\\n    #print('label_encoder',label_encoder)\\n    labels = label_encoder.fit_transform(labels)\\n    #print('labels',label)\\n    labels = to_categorical(labels)\\n    print('labels',labels)\\n    print('labels.shape',labels.shape)\\n    return labels, label_encoder.classes_\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from sklearn import preprocessing\n",
    "from keras.utils import to_categorical\n",
    "def encode_label(labels):\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    #print('label_encoder',label_encoder)\n",
    "    labels = label_encoder.fit_transform(labels)\n",
    "    #print('labels',label)\n",
    "    labels = to_categorical(labels)\n",
    "    print('labels',labels)\n",
    "    print('labels.shape',labels.shape)\n",
    "    return labels, label_encoder.classes_'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1c4ace6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"encoder = preprocessing.LabelEncoder()\\nlabels = encoder.fit_transform(targets)\\n#print(labels)\\n#print(encoder.inverse_transform(labels))\\nprint('targets',targets)\\nlabels_encoded=to_categorical(labels)\\nprint('labels',labels_encoded)\\n#print('labels.shape',labels_encoded.shape)\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''encoder = preprocessing.LabelEncoder()\n",
    "labels = encoder.fit_transform(targets)\n",
    "#print(labels)\n",
    "#print(encoder.inverse_transform(labels))\n",
    "print('targets',targets)\n",
    "labels_encoded=to_categorical(labels)\n",
    "print('labels',labels_encoded)\n",
    "#print('labels.shape',labels_encoded.shape)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58a625a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels_encoded1, classes1 = encode_label(targets1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1c8eafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the graph with spektral.data\n",
    "G2 = Graph(x=x2,a=a2,y=yy2)\n",
    "G4 = Graph(x=x4,a=a4,y=yy4)\n",
    "#G4 = Graph(x=x1,a=a1,e=e1,y=labels_encoded1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7eaaeaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph2 info:  Graph(n_nodes=16, n_node_features=34, n_edge_features=None, n_labels=16)\n",
      "Graph4 info:  Graph(n_nodes=16, n_node_features=34, n_edge_features=None, n_labels=16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rj/j2rdzd094m342tyh8mxvz6vc0000gn/T/ipykernel_3124/789246816.py:1: DeprecationWarning: info is deprecated and will be removed in version 3.0.\n",
      "\n",
      "  print('Graph2 info: ', nx.info(G2))\n",
      "/var/folders/rj/j2rdzd094m342tyh8mxvz6vc0000gn/T/ipykernel_3124/789246816.py:2: DeprecationWarning: info is deprecated and will be removed in version 3.0.\n",
      "\n",
      "  print('Graph4 info: ', nx.info(G4))\n"
     ]
    }
   ],
   "source": [
    "print('Graph2 info: ', nx.info(G2))\n",
    "print('Graph4 info: ', nx.info(G4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66691de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(keras.Model):\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data\n",
    "        x, y = data\n",
    "        # Compute predictions\n",
    "        y_pred = self(x, training=False)\n",
    "        # Updates the metrics tracking the loss\n",
    "        self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "        # Update the metrics.\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        # Return a dict mapping metric names to current value.\n",
    "        # Note that it will include the loss (tracked in self.metrics).\n",
    "        return {m.name: m.result() for m in self.metrics} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b99939a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 12:20:22.757153: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"GCNN3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 34)]         0           []                               \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 34)           0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 16)]         0           []                               \n",
      "                                                                                                  \n",
      " gcn_conv (GCNConv)             (None, 16)           544         ['dropout[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 16)           0           ['gcn_conv[0][0]']               \n",
      "                                                                                                  \n",
      " gcn_conv_1 (GCNConv)           (None, 16)           256         ['dropout_1[0][0]',              \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 800\n",
      "Trainable params: 800\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "channels = 16           # Number of channels in the first layer\n",
    "dropout = 0.5           # Dropout rate for the features\n",
    "l2_reg = 5e-4           # L2 regularization rate\n",
    "learning_rate = 1e-2    # Learning rate\n",
    "epochs = 100           # Number of training epochs\n",
    "es_patience = 10       # Patience for early stopping\n",
    "\n",
    "# Model definition\n",
    "X_in = Input(shape=(f2, ))\n",
    "fltr_in = Input((n2, ), sparse=True)\n",
    "\n",
    "dropout_1 = Dropout(dropout)(X_in)\n",
    "graph_conv_1 = GCNConv(channels,\n",
    "                         activation='relu',\n",
    "                         kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                         use_bias=False)([dropout_1, fltr_in])\n",
    "dropout_2 = Dropout(dropout)(graph_conv_1)\n",
    "graph_conv_2 = GCNConv(16,\n",
    "                         activation='linear',kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                         use_bias=False)([dropout_2, fltr_in])\n",
    "#dropout_3 = Dropout(dropout)(graph_conv_2)\n",
    "#dense = Dense(16,activation='linear',use_bias=False)(dropout_3)\n",
    "\n",
    "# Build model\n",
    "model = Model(inputs=[X_in, fltr_in], outputs=graph_conv_2, name='GCNN3')\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='mse',\n",
    "              metrics=['mae',tf.keras.metrics.RootMeanSquaredError()],\n",
    "              weighted_metrics=[])\n",
    "model.summary()\n",
    "\n",
    "tbCallBack_GCN = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir='./Tensorboard_GCN_cora')\n",
    "callback_GCN = [tbCallBack_GCN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a22f588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_weights(\"untrained_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0c5fd4",
   "metadata": {},
   "source": [
    "def limit_data(labels,limit=1,val_num=2,test_num=2):\n",
    "    '''\n",
    "    Get the index of train, validation, and test data\n",
    "    '''\n",
    "    label_counter = dict((l, 0) for l in labels)\n",
    "    print('label_counter:',label_counter) \n",
    "    train_idx = []\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        label = labels[i]\n",
    "        if label_counter[label]<limit:\n",
    "            #add the example to the training data\n",
    "            train_idx.append(i)\n",
    "            label_counter[label]+=1\n",
    "        \n",
    "        #exit the loop once we found 20 examples for each class\n",
    "        if all(count == limit for count in label_counter.values()):\n",
    "            break\n",
    "    \n",
    "    #get the indices that do not go to traning data\n",
    "    rest_idx = [x for x in range(len(labels)) if x not in train_idx]\n",
    "    val_idx = rest_idx[:val_num]\n",
    "    test_idx = rest_idx[val_num:(val_num+test_num)]\n",
    "    return train_idx, val_idx,test_idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db5326a",
   "metadata": {},
   "source": [
    "train_idx,val_idx,test_idx = limit_data(targets)\n",
    "\n",
    "print('train_idx')\n",
    "print(train_idx)\n",
    "\n",
    "print('val_idx')\n",
    "print(val_idx)\n",
    "\n",
    "print('test_idx')\n",
    "print(test_idx)\n",
    "\n",
    "#set the mask\n",
    "train_mask = np.zeros((n,),dtype=bool)\n",
    "train_mask[train_idx] = True\n",
    "\n",
    "print('train_mask')\n",
    "print(train_mask)\n",
    "\n",
    "val_mask = np.zeros((n,),dtype=bool)\n",
    "val_mask[val_idx] = True\n",
    "print('val_mask')\n",
    "print(val_mask)\n",
    "\n",
    "\n",
    "test_mask = np.zeros((n,),dtype=bool)\n",
    "test_mask[test_idx] = True\n",
    "print('test_mask')\n",
    "print(test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4731cf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to create folders and callbacks for training\n",
    "# parameters-> monitor, mode\n",
    "from datetime import datetime\n",
    "\n",
    "def create_folders_and_callbacks(model_name,now):\n",
    "    exps_dir = os.path.join('Model_Training_24/02')\n",
    "    if not os.path.exists(exps_dir):\n",
    "        os.makedirs(exps_dir)\n",
    "\n",
    "\n",
    "    exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\n",
    "    if not os.path.exists(exp_dir):\n",
    "        os.makedirs(exp_dir)\n",
    "      \n",
    "    callbacks = []\n",
    "\n",
    "    # Model checkpoint\n",
    "    # ----------------\n",
    "    ckpt_dir = os.path.join(exp_dir, 'ckpts')\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        os.makedirs(ckpt_dir)\n",
    "\n",
    "    ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp.ckpt'), verbose=1,\n",
    "                                                     save_weights_only=True, # True to save only weights\n",
    "                                                     save_best_only=True\n",
    "                                                    ) # True to save only the best epoch \n",
    "    callbacks.append(ckpt_callback)\n",
    "\n",
    "    # Visualize Learning on Tensorboard\n",
    "    # ---------------------------------\n",
    "    tb_dir = os.path.join(exp_dir, 'tb_logs')\n",
    "    if not os.path.exists(tb_dir):\n",
    "        os.makedirs(tb_dir)\n",
    "      \n",
    "    # By default shows losses and metrics for both training and validation\n",
    "    tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir, \n",
    "                                               profile_batch=0,\n",
    "                                               histogram_freq=1)  # if > 0 (epochs) shows weights histograms\n",
    "    callbacks.append(tb_callback)\n",
    "\n",
    "    # Early Stopping\n",
    "    # --------------\n",
    "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=es_patience, restore_best_weights=True)\n",
    "    reducelr_callback=tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', patience=7, factor=0.5, min_lr=1e-5)\n",
    "    callbacks.append(es_callback)\n",
    "    callbacks.append(reducelr_callback)\n",
    "   \n",
    "\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b678d293",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "handmade_callbacks = create_folders_and_callbacks(model_name='model1',now=now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ecbf41d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx_tr: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
      "idx_va: [0]\n",
      "train_mask: [False  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True]\n",
      "val_mask: [ True False False False False False False False False False False False\n",
      " False False False False]\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0796 - root_mean_squared_error: 0.2424\n",
      "Epoch 1: val_loss improved from 0.01288 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0714 - mae: 0.0796 - root_mean_squared_error: 0.2424 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0707 - mae: 0.0720 - root_mean_squared_error: 0.2411\n",
      "Epoch 2: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0707 - mae: 0.0720 - root_mean_squared_error: 0.2411 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0723 - mae: 0.0778 - root_mean_squared_error: 0.2442\n",
      "Epoch 3: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0723 - mae: 0.0778 - root_mean_squared_error: 0.2442 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0675 - root_mean_squared_error: 0.2431\n",
      "Epoch 4: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0717 - mae: 0.0675 - root_mean_squared_error: 0.2431 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0738 - root_mean_squared_error: 0.2417\n",
      "Epoch 5: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.0710 - mae: 0.0738 - root_mean_squared_error: 0.2417 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0672 - root_mean_squared_error: 0.2434\n",
      "Epoch 6: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.0718 - mae: 0.0672 - root_mean_squared_error: 0.2434 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0682 - root_mean_squared_error: 0.2431\n",
      "Epoch 7: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 0.0717 - mae: 0.0682 - root_mean_squared_error: 0.2431 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0619 - root_mean_squared_error: 0.2439\n",
      "Epoch 8: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 0.0721 - mae: 0.0619 - root_mean_squared_error: 0.2439 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0758 - root_mean_squared_error: 0.2422\n",
      "Epoch 9: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.0713 - mae: 0.0758 - root_mean_squared_error: 0.2422 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0772 - root_mean_squared_error: 0.2427\n",
      "Epoch 10: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 352ms/step - loss: 0.0715 - mae: 0.0772 - root_mean_squared_error: 0.2427 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0769 - root_mean_squared_error: 0.2426\n",
      "Epoch 11: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 0.0715 - mae: 0.0769 - root_mean_squared_error: 0.2426 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0787 - root_mean_squared_error: 0.2426\n",
      "Epoch 12: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 0.0715 - mae: 0.0787 - root_mean_squared_error: 0.2426 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0727 - mae: 0.0598 - root_mean_squared_error: 0.2451\n",
      "Epoch 13: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 240ms/step - loss: 0.0727 - mae: 0.0598 - root_mean_squared_error: 0.2451 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0748 - root_mean_squared_error: 0.2426\n",
      "Epoch 14: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0714 - mae: 0.0748 - root_mean_squared_error: 0.2426 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0769 - root_mean_squared_error: 0.2437\n",
      "Epoch 15: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0720 - mae: 0.0769 - root_mean_squared_error: 0.2437 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0978 - root_mean_squared_error: 0.2435\n",
      "Epoch 16: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.0719 - mae: 0.0978 - root_mean_squared_error: 0.2435 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0802 - root_mean_squared_error: 0.2417\n",
      "Epoch 17: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0710 - mae: 0.0802 - root_mean_squared_error: 0.2417 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0656 - root_mean_squared_error: 0.2436\n",
      "Epoch 18: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.0719 - mae: 0.0656 - root_mean_squared_error: 0.2436 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0743 - mae: 0.0994 - root_mean_squared_error: 0.2484\n",
      "Epoch 19: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0743 - mae: 0.0994 - root_mean_squared_error: 0.2484 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0738 - root_mean_squared_error: 0.2433\n",
      "Epoch 20: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0718 - mae: 0.0738 - root_mean_squared_error: 0.2433 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0668 - root_mean_squared_error: 0.2432\n",
      "Epoch 21: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0717 - mae: 0.0668 - root_mean_squared_error: 0.2432 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0693 - root_mean_squared_error: 0.2429\n",
      "Epoch 22: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0716 - mae: 0.0693 - root_mean_squared_error: 0.2429 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0732 - mae: 0.0570 - root_mean_squared_error: 0.2462\n",
      "Epoch 23: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0732 - mae: 0.0570 - root_mean_squared_error: 0.2462 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0776 - root_mean_squared_error: 0.2428\n",
      "Epoch 24: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.0715 - mae: 0.0776 - root_mean_squared_error: 0.2428 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0651 - root_mean_squared_error: 0.2422\n",
      "Epoch 25: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0713 - mae: 0.0651 - root_mean_squared_error: 0.2422 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0735 - mae: 0.0969 - root_mean_squared_error: 0.2467\n",
      "Epoch 26: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0735 - mae: 0.0969 - root_mean_squared_error: 0.2467 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0766 - root_mean_squared_error: 0.2417\n",
      "Epoch 27: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0710 - mae: 0.0766 - root_mean_squared_error: 0.2417 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0706 - root_mean_squared_error: 0.2438\n",
      "Epoch 28: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0720 - mae: 0.0706 - root_mean_squared_error: 0.2438 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0713 - root_mean_squared_error: 0.2437\n",
      "Epoch 29: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0720 - mae: 0.0713 - root_mean_squared_error: 0.2437 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0781 - root_mean_squared_error: 0.2423\n",
      "Epoch 30: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0713 - mae: 0.0781 - root_mean_squared_error: 0.2423 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0839 - root_mean_squared_error: 0.2430\n",
      "Epoch 31: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0717 - mae: 0.0839 - root_mean_squared_error: 0.2430 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0640 - root_mean_squared_error: 0.2434\n",
      "Epoch 32: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0719 - mae: 0.0640 - root_mean_squared_error: 0.2434 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0919 - root_mean_squared_error: 0.2427\n",
      "Epoch 33: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0715 - mae: 0.0919 - root_mean_squared_error: 0.2427 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0730 - mae: 0.0963 - root_mean_squared_error: 0.2459\n",
      "Epoch 34: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0730 - mae: 0.0963 - root_mean_squared_error: 0.2459 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0610 - root_mean_squared_error: 0.2440\n",
      "Epoch 35: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0721 - mae: 0.0610 - root_mean_squared_error: 0.2440 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0663 - root_mean_squared_error: 0.2432\n",
      "Epoch 36: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0717 - mae: 0.0663 - root_mean_squared_error: 0.2432 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0716 - root_mean_squared_error: 0.2427\n",
      "Epoch 37: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0715 - mae: 0.0716 - root_mean_squared_error: 0.2427 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0775 - root_mean_squared_error: 0.2415\n",
      "Epoch 38: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0709 - mae: 0.0775 - root_mean_squared_error: 0.2415 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0723 - mae: 0.0699 - root_mean_squared_error: 0.2442\n",
      "Epoch 39: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0723 - mae: 0.0699 - root_mean_squared_error: 0.2442 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0764 - root_mean_squared_error: 0.2425\n",
      "Epoch 40: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0714 - mae: 0.0764 - root_mean_squared_error: 0.2425 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0700 - root_mean_squared_error: 0.2433\n",
      "Epoch 41: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.0718 - mae: 0.0700 - root_mean_squared_error: 0.2433 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0855 - root_mean_squared_error: 0.2429\n",
      "Epoch 42: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.0716 - mae: 0.0855 - root_mean_squared_error: 0.2429 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0817 - root_mean_squared_error: 0.2423\n",
      "Epoch 43: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0713 - mae: 0.0817 - root_mean_squared_error: 0.2423 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0702 - root_mean_squared_error: 0.2417\n",
      "Epoch 44: val_loss improved from 0.01286 to 0.01286, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0710 - mae: 0.0702 - root_mean_squared_error: 0.2417 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0639 - root_mean_squared_error: 0.2424\n",
      "Epoch 45: val_loss improved from 0.01286 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0714 - mae: 0.0639 - root_mean_squared_error: 0.2424 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0727 - mae: 0.0558 - root_mean_squared_error: 0.2451\n",
      "Epoch 46: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0727 - mae: 0.0558 - root_mean_squared_error: 0.2451 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0658 - root_mean_squared_error: 0.2434\n",
      "Epoch 47: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0718 - mae: 0.0658 - root_mean_squared_error: 0.2434 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0755 - root_mean_squared_error: 0.2415\n",
      "Epoch 48: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0709 - mae: 0.0755 - root_mean_squared_error: 0.2415 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0717 - root_mean_squared_error: 0.2419\n",
      "Epoch 49: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0711 - mae: 0.0717 - root_mean_squared_error: 0.2419 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0741 - root_mean_squared_error: 0.2438\n",
      "Epoch 50: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.0720 - mae: 0.0741 - root_mean_squared_error: 0.2438 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0697 - root_mean_squared_error: 0.2438\n",
      "Epoch 51: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0720 - mae: 0.0697 - root_mean_squared_error: 0.2438 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0693 - root_mean_squared_error: 0.2421\n",
      "Epoch 52: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.0712 - mae: 0.0693 - root_mean_squared_error: 0.2421 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0725 - root_mean_squared_error: 0.2428\n",
      "Epoch 53: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0716 - mae: 0.0725 - root_mean_squared_error: 0.2428 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0723 - mae: 0.0603 - root_mean_squared_error: 0.2443\n",
      "Epoch 54: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0723 - mae: 0.0603 - root_mean_squared_error: 0.2443 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0752 - root_mean_squared_error: 0.2427\n",
      "Epoch 55: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.0715 - mae: 0.0752 - root_mean_squared_error: 0.2427 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0723 - mae: 0.0709 - root_mean_squared_error: 0.2443\n",
      "Epoch 56: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0723 - mae: 0.0709 - root_mean_squared_error: 0.2443 - val_loss: 0.0129 - val_mae: 0.0717 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0683 - root_mean_squared_error: 0.2420\n",
      "Epoch 57: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0711 - mae: 0.0683 - root_mean_squared_error: 0.2420 - val_loss: 0.0129 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0742 - mae: 0.0972 - root_mean_squared_error: 0.2481\n",
      "Epoch 58: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0742 - mae: 0.0972 - root_mean_squared_error: 0.2481 - val_loss: 0.0129 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0758 - root_mean_squared_error: 0.2424\n",
      "Epoch 59: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0714 - mae: 0.0758 - root_mean_squared_error: 0.2424 - val_loss: 0.0129 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0804 - root_mean_squared_error: 0.2417\n",
      "Epoch 60: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0710 - mae: 0.0804 - root_mean_squared_error: 0.2417 - val_loss: 0.0129 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0803 - root_mean_squared_error: 0.2416\n",
      "Epoch 61: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0710 - mae: 0.0803 - root_mean_squared_error: 0.2416 - val_loss: 0.0129 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0697 - root_mean_squared_error: 0.2418\n",
      "Epoch 62: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0711 - mae: 0.0697 - root_mean_squared_error: 0.2418 - val_loss: 0.0129 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0722 - mae: 0.0564 - root_mean_squared_error: 0.2440\n",
      "Epoch 63: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0722 - mae: 0.0564 - root_mean_squared_error: 0.2440 - val_loss: 0.0129 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0936 - root_mean_squared_error: 0.2440\n",
      "Epoch 64: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0721 - mae: 0.0936 - root_mean_squared_error: 0.2440 - val_loss: 0.0129 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0741 - root_mean_squared_error: 0.2430\n",
      "Epoch 65: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.0716 - mae: 0.0741 - root_mean_squared_error: 0.2430 - val_loss: 0.0129 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0676 - root_mean_squared_error: 0.2428\n",
      "Epoch 66: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0716 - mae: 0.0676 - root_mean_squared_error: 0.2428 - val_loss: 0.0129 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0783 - mae: 0.0957 - root_mean_squared_error: 0.2563\n",
      "Epoch 67: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0783 - mae: 0.0957 - root_mean_squared_error: 0.2563 - val_loss: 0.0129 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0782 - root_mean_squared_error: 0.2424\n",
      "Epoch 68: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0714 - mae: 0.0782 - root_mean_squared_error: 0.2424 - val_loss: 0.0129 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0714 - root_mean_squared_error: 0.2423\n",
      "Epoch 69: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0713 - mae: 0.0714 - root_mean_squared_error: 0.2423 - val_loss: 0.0129 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0724 - mae: 0.0649 - root_mean_squared_error: 0.2446\n",
      "Epoch 70: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0724 - mae: 0.0649 - root_mean_squared_error: 0.2446 - val_loss: 0.0129 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0817 - root_mean_squared_error: 0.2434\n",
      "Epoch 71: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0719 - mae: 0.0817 - root_mean_squared_error: 0.2434 - val_loss: 0.0129 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0699 - root_mean_squared_error: 0.2417\n",
      "Epoch 72: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0710 - mae: 0.0699 - root_mean_squared_error: 0.2417 - val_loss: 0.0129 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0741 - root_mean_squared_error: 0.2414\n",
      "Epoch 73: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0709 - mae: 0.0741 - root_mean_squared_error: 0.2414 - val_loss: 0.0129 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0723 - mae: 0.0648 - root_mean_squared_error: 0.2444\n",
      "Epoch 74: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0723 - mae: 0.0648 - root_mean_squared_error: 0.2444 - val_loss: 0.0129 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0924 - root_mean_squared_error: 0.2437\n",
      "Epoch 75: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0720 - mae: 0.0924 - root_mean_squared_error: 0.2437 - val_loss: 0.0129 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0701 - root_mean_squared_error: 0.2432\n",
      "Epoch 76: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0717 - mae: 0.0701 - root_mean_squared_error: 0.2432 - val_loss: 0.0129 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0735 - root_mean_squared_error: 0.2426\n",
      "Epoch 77: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0715 - mae: 0.0735 - root_mean_squared_error: 0.2426 - val_loss: 0.0128 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0615 - root_mean_squared_error: 0.2439\n",
      "Epoch 78: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.0721 - mae: 0.0615 - root_mean_squared_error: 0.2439 - val_loss: 0.0128 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0694 - root_mean_squared_error: 0.2418\n",
      "Epoch 79: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0711 - mae: 0.0694 - root_mean_squared_error: 0.2418 - val_loss: 0.0128 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0731 - mae: 0.1011 - root_mean_squared_error: 0.2460\n",
      "Epoch 80: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.0731 - mae: 0.1011 - root_mean_squared_error: 0.2460 - val_loss: 0.0128 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0660 - root_mean_squared_error: 0.2433\n",
      "Epoch 81: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0718 - mae: 0.0660 - root_mean_squared_error: 0.2433 - val_loss: 0.0128 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0836 - root_mean_squared_error: 0.2417\n",
      "Epoch 82: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0710 - mae: 0.0836 - root_mean_squared_error: 0.2417 - val_loss: 0.0128 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0722 - mae: 0.0686 - root_mean_squared_error: 0.2441\n",
      "Epoch 83: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0722 - mae: 0.0686 - root_mean_squared_error: 0.2441 - val_loss: 0.0128 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0724 - mae: 0.0592 - root_mean_squared_error: 0.2446\n",
      "Epoch 84: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0724 - mae: 0.0592 - root_mean_squared_error: 0.2446 - val_loss: 0.0128 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0754 - root_mean_squared_error: 0.2437\n",
      "Epoch 85: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0720 - mae: 0.0754 - root_mean_squared_error: 0.2437 - val_loss: 0.0128 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0623 - root_mean_squared_error: 0.2438\n",
      "Epoch 86: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0720 - mae: 0.0623 - root_mean_squared_error: 0.2438 - val_loss: 0.0128 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0686 - root_mean_squared_error: 0.2430\n",
      "Epoch 87: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0716 - mae: 0.0686 - root_mean_squared_error: 0.2430 - val_loss: 0.0128 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0725 - mae: 0.0621 - root_mean_squared_error: 0.2447\n",
      "Epoch 88: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0725 - mae: 0.0621 - root_mean_squared_error: 0.2447 - val_loss: 0.0128 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0669 - root_mean_squared_error: 0.2432\n",
      "Epoch 89: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0718 - mae: 0.0669 - root_mean_squared_error: 0.2432 - val_loss: 0.0128 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0789 - root_mean_squared_error: 0.2424\n",
      "Epoch 90: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.0714 - mae: 0.0789 - root_mean_squared_error: 0.2424 - val_loss: 0.0128 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0785 - root_mean_squared_error: 0.2436\n",
      "Epoch 91: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0719 - mae: 0.0785 - root_mean_squared_error: 0.2436 - val_loss: 0.0128 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0811 - root_mean_squared_error: 0.2423\n",
      "Epoch 92: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0713 - mae: 0.0811 - root_mean_squared_error: 0.2423 - val_loss: 0.0128 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0707 - mae: 0.0812 - root_mean_squared_error: 0.2410\n",
      "Epoch 93: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0707 - mae: 0.0812 - root_mean_squared_error: 0.2410 - val_loss: 0.0128 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0662 - root_mean_squared_error: 0.2422\n",
      "Epoch 94: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 0.0713 - mae: 0.0662 - root_mean_squared_error: 0.2422 - val_loss: 0.0128 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0733 - root_mean_squared_error: 0.2436\n",
      "Epoch 95: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 0.0719 - mae: 0.0733 - root_mean_squared_error: 0.2436 - val_loss: 0.0128 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0721 - root_mean_squared_error: 0.2427\n",
      "Epoch 96: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0715 - mae: 0.0721 - root_mean_squared_error: 0.2427 - val_loss: 0.0128 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0734 - root_mean_squared_error: 0.2416\n",
      "Epoch 97: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0710 - mae: 0.0734 - root_mean_squared_error: 0.2416 - val_loss: 0.0128 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0712 - root_mean_squared_error: 0.2425\n",
      "Epoch 98: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0714 - mae: 0.0712 - root_mean_squared_error: 0.2425 - val_loss: 0.0128 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0657 - root_mean_squared_error: 0.2434\n",
      "Epoch 99: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0718 - mae: 0.0657 - root_mean_squared_error: 0.2434 - val_loss: 0.0128 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0733 - mae: 0.0567 - root_mean_squared_error: 0.2464\n",
      "Epoch 100: val_loss improved from 0.01285 to 0.01285, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.0733 - mae: 0.0567 - root_mean_squared_error: 0.2464 - val_loss: 0.0128 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "idx_tr: [ 0  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
      "idx_va: [1]\n",
      "train_mask: [ True False  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True]\n",
      "val_mask: [False  True False False False False False False False False False False\n",
      " False False False False]\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0726 - mae: 0.0882 - root_mean_squared_error: 0.2450\n",
      "Epoch 1: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.0726 - mae: 0.0882 - root_mean_squared_error: 0.2450 - val_loss: 0.0129 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0672 - root_mean_squared_error: 0.2428\n",
      "Epoch 2: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0716 - mae: 0.0672 - root_mean_squared_error: 0.2428 - val_loss: 0.0129 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0719 - root_mean_squared_error: 0.2415\n",
      "Epoch 3: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0709 - mae: 0.0719 - root_mean_squared_error: 0.2415 - val_loss: 0.0129 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0800 - root_mean_squared_error: 0.2425\n",
      "Epoch 4: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0714 - mae: 0.0800 - root_mean_squared_error: 0.2425 - val_loss: 0.0129 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0571 - root_mean_squared_error: 0.2440\n",
      "Epoch 5: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0721 - mae: 0.0571 - root_mean_squared_error: 0.2440 - val_loss: 0.0129 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0755 - root_mean_squared_error: 0.2425\n",
      "Epoch 6: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0714 - mae: 0.0755 - root_mean_squared_error: 0.2425 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0625 - root_mean_squared_error: 0.2429\n",
      "Epoch 7: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 0.0716 - mae: 0.0625 - root_mean_squared_error: 0.2429 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0682 - root_mean_squared_error: 0.2422\n",
      "Epoch 8: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0713 - mae: 0.0682 - root_mean_squared_error: 0.2422 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0627 - root_mean_squared_error: 0.2428\n",
      "Epoch 9: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0715 - mae: 0.0627 - root_mean_squared_error: 0.2428 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0754 - root_mean_squared_error: 0.2424\n",
      "Epoch 10: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0713 - mae: 0.0754 - root_mean_squared_error: 0.2424 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0730 - mae: 0.0871 - root_mean_squared_error: 0.2458\n",
      "Epoch 11: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0730 - mae: 0.0871 - root_mean_squared_error: 0.2458 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0761 - root_mean_squared_error: 0.2415\n",
      "Epoch 12: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 0.0709 - mae: 0.0761 - root_mean_squared_error: 0.2415 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0832 - root_mean_squared_error: 0.2427\n",
      "Epoch 13: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0715 - mae: 0.0832 - root_mean_squared_error: 0.2427 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0723 - root_mean_squared_error: 0.2436\n",
      "Epoch 14: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 0.0720 - mae: 0.0723 - root_mean_squared_error: 0.2436 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0723 - mae: 0.0643 - root_mean_squared_error: 0.2443\n",
      "Epoch 15: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 0.0723 - mae: 0.0643 - root_mean_squared_error: 0.2443 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0720 - root_mean_squared_error: 0.2436\n",
      "Epoch 16: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0719 - mae: 0.0720 - root_mean_squared_error: 0.2436 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0784 - root_mean_squared_error: 0.2426\n",
      "Epoch 17: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0715 - mae: 0.0784 - root_mean_squared_error: 0.2426 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0664 - root_mean_squared_error: 0.2434\n",
      "Epoch 18: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0719 - mae: 0.0664 - root_mean_squared_error: 0.2434 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0722 - mae: 0.0606 - root_mean_squared_error: 0.2442\n",
      "Epoch 19: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0722 - mae: 0.0606 - root_mean_squared_error: 0.2442 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0819 - root_mean_squared_error: 0.2431\n",
      "Epoch 20: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0717 - mae: 0.0819 - root_mean_squared_error: 0.2431 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0844 - mae: 0.1446 - root_mean_squared_error: 0.2680\n",
      "Epoch 21: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0844 - mae: 0.1446 - root_mean_squared_error: 0.2680 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0773 - mae: 0.1241 - root_mean_squared_error: 0.2544\n",
      "Epoch 22: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0773 - mae: 0.1241 - root_mean_squared_error: 0.2544 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0690 - root_mean_squared_error: 0.2430\n",
      "Epoch 23: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0717 - mae: 0.0690 - root_mean_squared_error: 0.2430 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0857 - root_mean_squared_error: 0.2416\n",
      "Epoch 24: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0709 - mae: 0.0857 - root_mean_squared_error: 0.2416 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0787 - root_mean_squared_error: 0.2434\n",
      "Epoch 25: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0719 - mae: 0.0787 - root_mean_squared_error: 0.2434 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0634 - root_mean_squared_error: 0.2437\n",
      "Epoch 26: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0720 - mae: 0.0634 - root_mean_squared_error: 0.2437 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0742 - root_mean_squared_error: 0.2422\n",
      "Epoch 27: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0713 - mae: 0.0742 - root_mean_squared_error: 0.2422 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0726 - mae: 0.0606 - root_mean_squared_error: 0.2451\n",
      "Epoch 28: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0726 - mae: 0.0606 - root_mean_squared_error: 0.2451 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0740 - root_mean_squared_error: 0.2424\n",
      "Epoch 29: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 0.0713 - mae: 0.0740 - root_mean_squared_error: 0.2424 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0786 - root_mean_squared_error: 0.2414\n",
      "Epoch 30: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 0.0709 - mae: 0.0786 - root_mean_squared_error: 0.2414 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0813 - root_mean_squared_error: 0.2419\n",
      "Epoch 31: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0711 - mae: 0.0813 - root_mean_squared_error: 0.2419 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0606 - root_mean_squared_error: 0.2433\n",
      "Epoch 32: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0718 - mae: 0.0606 - root_mean_squared_error: 0.2433 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0813 - root_mean_squared_error: 0.2426\n",
      "Epoch 33: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0714 - mae: 0.0813 - root_mean_squared_error: 0.2426 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0723 - mae: 0.0886 - root_mean_squared_error: 0.2443\n",
      "Epoch 34: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0723 - mae: 0.0886 - root_mean_squared_error: 0.2443 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0729 - mae: 0.1010 - root_mean_squared_error: 0.2456\n",
      "Epoch 35: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.0729 - mae: 0.1010 - root_mean_squared_error: 0.2456 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0783 - mae: 0.1000 - root_mean_squared_error: 0.2564\n",
      "Epoch 36: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0783 - mae: 0.1000 - root_mean_squared_error: 0.2564 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0724 - root_mean_squared_error: 0.2434\n",
      "Epoch 37: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0718 - mae: 0.0724 - root_mean_squared_error: 0.2434 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0838 - root_mean_squared_error: 0.2415\n",
      "Epoch 38: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0709 - mae: 0.0838 - root_mean_squared_error: 0.2415 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0820 - mae: 0.1376 - root_mean_squared_error: 0.2634\n",
      "Epoch 39: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0820 - mae: 0.1376 - root_mean_squared_error: 0.2634 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0721 - root_mean_squared_error: 0.2431\n",
      "Epoch 40: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0717 - mae: 0.0721 - root_mean_squared_error: 0.2431 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0635 - root_mean_squared_error: 0.2428\n",
      "Epoch 41: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0715 - mae: 0.0635 - root_mean_squared_error: 0.2428 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0666 - root_mean_squared_error: 0.2432\n",
      "Epoch 42: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0717 - mae: 0.0666 - root_mean_squared_error: 0.2432 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0696 - root_mean_squared_error: 0.2427\n",
      "Epoch 43: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0715 - mae: 0.0696 - root_mean_squared_error: 0.2427 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0781 - root_mean_squared_error: 0.2439\n",
      "Epoch 44: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0721 - mae: 0.0781 - root_mean_squared_error: 0.2439 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0901 - root_mean_squared_error: 0.2429\n",
      "Epoch 45: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0716 - mae: 0.0901 - root_mean_squared_error: 0.2429 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0810 - root_mean_squared_error: 0.2416\n",
      "Epoch 46: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0710 - mae: 0.0810 - root_mean_squared_error: 0.2416 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0819 - mae: 0.1420 - root_mean_squared_error: 0.2633\n",
      "Epoch 47: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0819 - mae: 0.1420 - root_mean_squared_error: 0.2633 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0637 - root_mean_squared_error: 0.2435\n",
      "Epoch 48: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 0.0719 - mae: 0.0637 - root_mean_squared_error: 0.2435 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0677 - root_mean_squared_error: 0.2440\n",
      "Epoch 49: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0721 - mae: 0.0677 - root_mean_squared_error: 0.2440 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0759 - root_mean_squared_error: 0.2424\n",
      "Epoch 50: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0713 - mae: 0.0759 - root_mean_squared_error: 0.2424 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0736 - root_mean_squared_error: 0.2428\n",
      "Epoch 51: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0715 - mae: 0.0736 - root_mean_squared_error: 0.2428 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0722 - mae: 0.0681 - root_mean_squared_error: 0.2441\n",
      "Epoch 52: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0722 - mae: 0.0681 - root_mean_squared_error: 0.2441 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0809 - root_mean_squared_error: 0.2415\n",
      "Epoch 53: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0709 - mae: 0.0809 - root_mean_squared_error: 0.2415 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0708 - mae: 0.0793 - root_mean_squared_error: 0.2413\n",
      "Epoch 54: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0708 - mae: 0.0793 - root_mean_squared_error: 0.2413 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0723 - mae: 0.0822 - root_mean_squared_error: 0.2444\n",
      "Epoch 55: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0723 - mae: 0.0822 - root_mean_squared_error: 0.2444 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0724 - root_mean_squared_error: 0.2414\n",
      "Epoch 56: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0709 - mae: 0.0724 - root_mean_squared_error: 0.2414 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0746 - root_mean_squared_error: 0.2424\n",
      "Epoch 57: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0714 - mae: 0.0746 - root_mean_squared_error: 0.2424 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0720 - root_mean_squared_error: 0.2426\n",
      "Epoch 58: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0715 - mae: 0.0720 - root_mean_squared_error: 0.2426 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0736 - mae: 0.1039 - root_mean_squared_error: 0.2470\n",
      "Epoch 59: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0736 - mae: 0.1039 - root_mean_squared_error: 0.2470 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0740 - root_mean_squared_error: 0.2436\n",
      "Epoch 60: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0719 - mae: 0.0740 - root_mean_squared_error: 0.2436 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0761 - root_mean_squared_error: 0.2434\n",
      "Epoch 61: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0718 - mae: 0.0761 - root_mean_squared_error: 0.2434 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0787 - root_mean_squared_error: 0.2425\n",
      "Epoch 62: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0714 - mae: 0.0787 - root_mean_squared_error: 0.2425 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0688 - root_mean_squared_error: 0.2429\n",
      "Epoch 63: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0716 - mae: 0.0688 - root_mean_squared_error: 0.2429 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0703 - root_mean_squared_error: 0.2427\n",
      "Epoch 64: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0715 - mae: 0.0703 - root_mean_squared_error: 0.2427 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0777 - root_mean_squared_error: 0.2424\n",
      "Epoch 65: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0713 - mae: 0.0777 - root_mean_squared_error: 0.2424 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0723 - mae: 0.0661 - root_mean_squared_error: 0.2444\n",
      "Epoch 66: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0723 - mae: 0.0661 - root_mean_squared_error: 0.2444 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0651 - root_mean_squared_error: 0.2428\n",
      "Epoch 67: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0715 - mae: 0.0651 - root_mean_squared_error: 0.2428 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0690 - root_mean_squared_error: 0.2417\n",
      "Epoch 68: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0710 - mae: 0.0690 - root_mean_squared_error: 0.2417 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0823 - root_mean_squared_error: 0.2416\n",
      "Epoch 69: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0710 - mae: 0.0823 - root_mean_squared_error: 0.2416 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0688 - root_mean_squared_error: 0.2429\n",
      "Epoch 70: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0716 - mae: 0.0688 - root_mean_squared_error: 0.2429 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0826 - root_mean_squared_error: 0.2435\n",
      "Epoch 71: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0719 - mae: 0.0826 - root_mean_squared_error: 0.2435 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0633 - root_mean_squared_error: 0.2440\n",
      "Epoch 72: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0721 - mae: 0.0633 - root_mean_squared_error: 0.2440 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0656 - root_mean_squared_error: 0.2431\n",
      "Epoch 73: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0717 - mae: 0.0656 - root_mean_squared_error: 0.2431 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0703 - root_mean_squared_error: 0.2439\n",
      "Epoch 74: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0721 - mae: 0.0703 - root_mean_squared_error: 0.2439 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0830 - root_mean_squared_error: 0.2416\n",
      "Epoch 75: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 0.0709 - mae: 0.0830 - root_mean_squared_error: 0.2416 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0775 - root_mean_squared_error: 0.2425\n",
      "Epoch 76: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0714 - mae: 0.0775 - root_mean_squared_error: 0.2425 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0767 - mae: 0.1199 - root_mean_squared_error: 0.2532\n",
      "Epoch 77: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0767 - mae: 0.1199 - root_mean_squared_error: 0.2532 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0726 - mae: 0.0932 - root_mean_squared_error: 0.2450\n",
      "Epoch 78: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0726 - mae: 0.0932 - root_mean_squared_error: 0.2450 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0719 - root_mean_squared_error: 0.2440\n",
      "Epoch 79: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0721 - mae: 0.0719 - root_mean_squared_error: 0.2440 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0709 - root_mean_squared_error: 0.2420\n",
      "Epoch 80: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0711 - mae: 0.0709 - root_mean_squared_error: 0.2420 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0866 - root_mean_squared_error: 0.2430\n",
      "Epoch 81: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0716 - mae: 0.0866 - root_mean_squared_error: 0.2430 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0628 - root_mean_squared_error: 0.2435\n",
      "Epoch 82: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0719 - mae: 0.0628 - root_mean_squared_error: 0.2435 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0727 - root_mean_squared_error: 0.2417\n",
      "Epoch 83: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0710 - mae: 0.0727 - root_mean_squared_error: 0.2417 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0836 - root_mean_squared_error: 0.2428\n",
      "Epoch 84: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0715 - mae: 0.0836 - root_mean_squared_error: 0.2428 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0667 - root_mean_squared_error: 0.2439\n",
      "Epoch 85: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0721 - mae: 0.0667 - root_mean_squared_error: 0.2439 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0630 - root_mean_squared_error: 0.2427\n",
      "Epoch 86: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0715 - mae: 0.0630 - root_mean_squared_error: 0.2427 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0751 - root_mean_squared_error: 0.2417\n",
      "Epoch 87: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.0710 - mae: 0.0751 - root_mean_squared_error: 0.2417 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0681 - root_mean_squared_error: 0.2431\n",
      "Epoch 88: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0717 - mae: 0.0681 - root_mean_squared_error: 0.2431 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0818 - root_mean_squared_error: 0.2418\n",
      "Epoch 89: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0711 - mae: 0.0818 - root_mean_squared_error: 0.2418 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2427 - lr: 1.0000e-05\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0760 - root_mean_squared_error: 0.2419\n",
      "Epoch 90: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0711 - mae: 0.0760 - root_mean_squared_error: 0.2419 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0725 - mae: 0.0642 - root_mean_squared_error: 0.2447\n",
      "Epoch 91: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0725 - mae: 0.0642 - root_mean_squared_error: 0.2447 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0705 - root_mean_squared_error: 0.2426\n",
      "Epoch 92: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0714 - mae: 0.0705 - root_mean_squared_error: 0.2426 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0821 - root_mean_squared_error: 0.2441\n",
      "Epoch 93: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0721 - mae: 0.0821 - root_mean_squared_error: 0.2441 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0724 - mae: 0.0637 - root_mean_squared_error: 0.2447\n",
      "Epoch 94: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0724 - mae: 0.0637 - root_mean_squared_error: 0.2447 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0706 - root_mean_squared_error: 0.2427\n",
      "Epoch 95: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0715 - mae: 0.0706 - root_mean_squared_error: 0.2427 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0683 - root_mean_squared_error: 0.2430\n",
      "Epoch 96: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0716 - mae: 0.0683 - root_mean_squared_error: 0.2430 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0720 - root_mean_squared_error: 0.2428\n",
      "Epoch 97: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 0.0715 - mae: 0.0720 - root_mean_squared_error: 0.2428 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0806 - root_mean_squared_error: 0.2426\n",
      "Epoch 98: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0714 - mae: 0.0806 - root_mean_squared_error: 0.2426 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0725 - mae: 0.0579 - root_mean_squared_error: 0.2448\n",
      "Epoch 99: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0725 - mae: 0.0579 - root_mean_squared_error: 0.2448 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0745 - root_mean_squared_error: 0.2436\n",
      "Epoch 100: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0719 - mae: 0.0745 - root_mean_squared_error: 0.2436 - val_loss: 0.0129 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "idx_tr: [ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
      "idx_va: [2]\n",
      "train_mask: [ True  True False  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True]\n",
      "val_mask: [False False  True False False False False False False False False False\n",
      " False False False False]\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0725 - mae: 0.0929 - root_mean_squared_error: 0.2449\n",
      "Epoch 1: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.0725 - mae: 0.0929 - root_mean_squared_error: 0.2449 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0711 - root_mean_squared_error: 0.2437\n",
      "Epoch 2: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0720 - mae: 0.0711 - root_mean_squared_error: 0.2437 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0730 - mae: 0.0628 - root_mean_squared_error: 0.2459\n",
      "Epoch 3: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0730 - mae: 0.0628 - root_mean_squared_error: 0.2459 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0594 - root_mean_squared_error: 0.2434\n",
      "Epoch 4: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0718 - mae: 0.0594 - root_mean_squared_error: 0.2434 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0723 - mae: 0.0644 - root_mean_squared_error: 0.2445\n",
      "Epoch 5: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0723 - mae: 0.0644 - root_mean_squared_error: 0.2445 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0756 - root_mean_squared_error: 0.2426\n",
      "Epoch 6: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0714 - mae: 0.0756 - root_mean_squared_error: 0.2426 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0820 - root_mean_squared_error: 0.2432\n",
      "Epoch 7: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0717 - mae: 0.0820 - root_mean_squared_error: 0.2432 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0813 - root_mean_squared_error: 0.2430\n",
      "Epoch 8: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0716 - mae: 0.0813 - root_mean_squared_error: 0.2430 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0718 - root_mean_squared_error: 0.2426\n",
      "Epoch 9: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 0.0714 - mae: 0.0718 - root_mean_squared_error: 0.2426 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0804 - mae: 0.1346 - root_mean_squared_error: 0.2605\n",
      "Epoch 10: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0804 - mae: 0.1346 - root_mean_squared_error: 0.2605 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0870 - root_mean_squared_error: 0.2437\n",
      "Epoch 11: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0720 - mae: 0.0870 - root_mean_squared_error: 0.2437 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0717 - root_mean_squared_error: 0.2426\n",
      "Epoch 12: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0714 - mae: 0.0717 - root_mean_squared_error: 0.2426 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0830 - root_mean_squared_error: 0.2414\n",
      "Epoch 13: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0709 - mae: 0.0830 - root_mean_squared_error: 0.2414 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0674 - root_mean_squared_error: 0.2419\n",
      "Epoch 14: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0711 - mae: 0.0674 - root_mean_squared_error: 0.2419 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0651 - root_mean_squared_error: 0.2428\n",
      "Epoch 15: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0715 - mae: 0.0651 - root_mean_squared_error: 0.2428 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0754 - root_mean_squared_error: 0.2432\n",
      "Epoch 16: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0717 - mae: 0.0754 - root_mean_squared_error: 0.2432 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0776 - root_mean_squared_error: 0.2414\n",
      "Epoch 17: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0709 - mae: 0.0776 - root_mean_squared_error: 0.2414 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0783 - root_mean_squared_error: 0.2426\n",
      "Epoch 18: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0714 - mae: 0.0783 - root_mean_squared_error: 0.2426 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0897 - root_mean_squared_error: 0.2434\n",
      "Epoch 19: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0718 - mae: 0.0897 - root_mean_squared_error: 0.2434 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0722 - mae: 0.0617 - root_mean_squared_error: 0.2441\n",
      "Epoch 20: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0722 - mae: 0.0617 - root_mean_squared_error: 0.2441 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0697 - root_mean_squared_error: 0.2430\n",
      "Epoch 21: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0716 - mae: 0.0697 - root_mean_squared_error: 0.2430 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0750 - mae: 0.1005 - root_mean_squared_error: 0.2498\n",
      "Epoch 22: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0750 - mae: 0.1005 - root_mean_squared_error: 0.2498 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0898 - root_mean_squared_error: 0.2422\n",
      "Epoch 23: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0712 - mae: 0.0898 - root_mean_squared_error: 0.2422 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0757 - root_mean_squared_error: 0.2416\n",
      "Epoch 24: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0709 - mae: 0.0757 - root_mean_squared_error: 0.2416 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0882 - root_mean_squared_error: 0.2431\n",
      "Epoch 25: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0717 - mae: 0.0882 - root_mean_squared_error: 0.2431 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0657 - root_mean_squared_error: 0.2431\n",
      "Epoch 26: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0717 - mae: 0.0657 - root_mean_squared_error: 0.2431 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0686 - root_mean_squared_error: 0.2441\n",
      "Epoch 27: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0721 - mae: 0.0686 - root_mean_squared_error: 0.2441 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0791 - root_mean_squared_error: 0.2426\n",
      "Epoch 28: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0714 - mae: 0.0791 - root_mean_squared_error: 0.2426 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0679 - root_mean_squared_error: 0.2439\n",
      "Epoch 29: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0721 - mae: 0.0679 - root_mean_squared_error: 0.2439 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0733 - mae: 0.0913 - root_mean_squared_error: 0.2464\n",
      "Epoch 30: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0733 - mae: 0.0913 - root_mean_squared_error: 0.2464 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0804 - mae: 0.1277 - root_mean_squared_error: 0.2605\n",
      "Epoch 31: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0804 - mae: 0.1277 - root_mean_squared_error: 0.2605 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0857 - root_mean_squared_error: 0.2440\n",
      "Epoch 32: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.0721 - mae: 0.0857 - root_mean_squared_error: 0.2440 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0792 - root_mean_squared_error: 0.2434\n",
      "Epoch 33: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0718 - mae: 0.0792 - root_mean_squared_error: 0.2434 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0778 - root_mean_squared_error: 0.2424\n",
      "Epoch 34: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0713 - mae: 0.0778 - root_mean_squared_error: 0.2424 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0748 - root_mean_squared_error: 0.2426\n",
      "Epoch 35: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0714 - mae: 0.0748 - root_mean_squared_error: 0.2426 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0755 - root_mean_squared_error: 0.2435\n",
      "Epoch 36: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0719 - mae: 0.0755 - root_mean_squared_error: 0.2435 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0724 - mae: 0.0865 - root_mean_squared_error: 0.2446\n",
      "Epoch 37: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0724 - mae: 0.0865 - root_mean_squared_error: 0.2446 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0666 - root_mean_squared_error: 0.2440\n",
      "Epoch 38: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0721 - mae: 0.0666 - root_mean_squared_error: 0.2440 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0910 - root_mean_squared_error: 0.2431\n",
      "Epoch 39: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0717 - mae: 0.0910 - root_mean_squared_error: 0.2431 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0713 - root_mean_squared_error: 0.2435\n",
      "Epoch 40: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0718 - mae: 0.0713 - root_mean_squared_error: 0.2435 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0736 - mae: 0.0772 - root_mean_squared_error: 0.2470\n",
      "Epoch 41: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.0736 - mae: 0.0772 - root_mean_squared_error: 0.2470 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0787 - root_mean_squared_error: 0.2415\n",
      "Epoch 42: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0709 - mae: 0.0787 - root_mean_squared_error: 0.2415 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0796 - root_mean_squared_error: 0.2426\n",
      "Epoch 43: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0714 - mae: 0.0796 - root_mean_squared_error: 0.2426 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0718 - root_mean_squared_error: 0.2427\n",
      "Epoch 44: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0715 - mae: 0.0718 - root_mean_squared_error: 0.2427 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0726 - mae: 0.0553 - root_mean_squared_error: 0.2451\n",
      "Epoch 45: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0726 - mae: 0.0553 - root_mean_squared_error: 0.2451 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0858 - root_mean_squared_error: 0.2419\n",
      "Epoch 46: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0711 - mae: 0.0858 - root_mean_squared_error: 0.2419 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0862 - root_mean_squared_error: 0.2439\n",
      "Epoch 47: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 0.0720 - mae: 0.0862 - root_mean_squared_error: 0.2439 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0627 - root_mean_squared_error: 0.2440\n",
      "Epoch 48: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0721 - mae: 0.0627 - root_mean_squared_error: 0.2440 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0914 - root_mean_squared_error: 0.2440\n",
      "Epoch 49: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0721 - mae: 0.0914 - root_mean_squared_error: 0.2440 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0672 - root_mean_squared_error: 0.2433\n",
      "Epoch 50: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0717 - mae: 0.0672 - root_mean_squared_error: 0.2433 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0734 - mae: 0.0954 - root_mean_squared_error: 0.2467\n",
      "Epoch 51: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.0734 - mae: 0.0954 - root_mean_squared_error: 0.2467 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0883 - root_mean_squared_error: 0.2432\n",
      "Epoch 52: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0717 - mae: 0.0883 - root_mean_squared_error: 0.2432 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0654 - root_mean_squared_error: 0.2419\n",
      "Epoch 53: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0711 - mae: 0.0654 - root_mean_squared_error: 0.2419 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0798 - root_mean_squared_error: 0.2417\n",
      "Epoch 54: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0710 - mae: 0.0798 - root_mean_squared_error: 0.2417 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0789 - root_mean_squared_error: 0.2426\n",
      "Epoch 55: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0714 - mae: 0.0789 - root_mean_squared_error: 0.2426 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0717 - root_mean_squared_error: 0.2427\n",
      "Epoch 56: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0715 - mae: 0.0717 - root_mean_squared_error: 0.2427 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0744 - mae: 0.0993 - root_mean_squared_error: 0.2487\n",
      "Epoch 57: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0744 - mae: 0.0993 - root_mean_squared_error: 0.2487 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0771 - root_mean_squared_error: 0.2433\n",
      "Epoch 58: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0717 - mae: 0.0771 - root_mean_squared_error: 0.2433 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0839 - root_mean_squared_error: 0.2422\n",
      "Epoch 59: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0712 - mae: 0.0839 - root_mean_squared_error: 0.2422 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0747 - mae: 0.1077 - root_mean_squared_error: 0.2492\n",
      "Epoch 60: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0747 - mae: 0.1077 - root_mean_squared_error: 0.2492 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0721 - root_mean_squared_error: 0.2417\n",
      "Epoch 61: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0710 - mae: 0.0721 - root_mean_squared_error: 0.2417 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0715 - root_mean_squared_error: 0.2419\n",
      "Epoch 62: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0711 - mae: 0.0715 - root_mean_squared_error: 0.2419 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0750 - root_mean_squared_error: 0.2418\n",
      "Epoch 63: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0710 - mae: 0.0750 - root_mean_squared_error: 0.2418 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0704 - root_mean_squared_error: 0.2428\n",
      "Epoch 64: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0715 - mae: 0.0704 - root_mean_squared_error: 0.2428 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0783 - root_mean_squared_error: 0.2436\n",
      "Epoch 65: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0719 - mae: 0.0783 - root_mean_squared_error: 0.2436 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0626 - root_mean_squared_error: 0.2425\n",
      "Epoch 66: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0714 - mae: 0.0626 - root_mean_squared_error: 0.2425 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0718 - root_mean_squared_error: 0.2417\n",
      "Epoch 67: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0710 - mae: 0.0718 - root_mean_squared_error: 0.2417 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0781 - root_mean_squared_error: 0.2425\n",
      "Epoch 68: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0714 - mae: 0.0781 - root_mean_squared_error: 0.2425 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0838 - root_mean_squared_error: 0.2437\n",
      "Epoch 69: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0719 - mae: 0.0838 - root_mean_squared_error: 0.2437 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0793 - root_mean_squared_error: 0.2437\n",
      "Epoch 70: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0719 - mae: 0.0793 - root_mean_squared_error: 0.2437 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0716 - root_mean_squared_error: 0.2415\n",
      "Epoch 71: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0709 - mae: 0.0716 - root_mean_squared_error: 0.2415 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0729 - mae: 0.0581 - root_mean_squared_error: 0.2456\n",
      "Epoch 72: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.0729 - mae: 0.0581 - root_mean_squared_error: 0.2456 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0726 - mae: 0.0800 - root_mean_squared_error: 0.2451\n",
      "Epoch 73: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0726 - mae: 0.0800 - root_mean_squared_error: 0.2451 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0842 - root_mean_squared_error: 0.2420\n",
      "Epoch 74: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0711 - mae: 0.0842 - root_mean_squared_error: 0.2420 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0772 - root_mean_squared_error: 0.2441\n",
      "Epoch 75: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0721 - mae: 0.0772 - root_mean_squared_error: 0.2441 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0819 - mae: 0.1435 - root_mean_squared_error: 0.2634\n",
      "Epoch 76: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0819 - mae: 0.1435 - root_mean_squared_error: 0.2634 - val_loss: 0.0185 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0734 - mae: 0.1066 - root_mean_squared_error: 0.2467\n",
      "Epoch 77: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0734 - mae: 0.1066 - root_mean_squared_error: 0.2467 - val_loss: 0.0185 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0695 - root_mean_squared_error: 0.2439\n",
      "Epoch 78: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0721 - mae: 0.0695 - root_mean_squared_error: 0.2439 - val_loss: 0.0185 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0685 - root_mean_squared_error: 0.2427\n",
      "Epoch 79: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0715 - mae: 0.0685 - root_mean_squared_error: 0.2427 - val_loss: 0.0185 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0737 - root_mean_squared_error: 0.2425\n",
      "Epoch 80: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0714 - mae: 0.0737 - root_mean_squared_error: 0.2425 - val_loss: 0.0185 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0786 - root_mean_squared_error: 0.2422\n",
      "Epoch 81: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0712 - mae: 0.0786 - root_mean_squared_error: 0.2422 - val_loss: 0.0185 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0769 - root_mean_squared_error: 0.2419\n",
      "Epoch 82: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.0711 - mae: 0.0769 - root_mean_squared_error: 0.2419 - val_loss: 0.0185 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0657 - root_mean_squared_error: 0.2425\n",
      "Epoch 83: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0714 - mae: 0.0657 - root_mean_squared_error: 0.2425 - val_loss: 0.0185 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0759 - root_mean_squared_error: 0.2417\n",
      "Epoch 84: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0710 - mae: 0.0759 - root_mean_squared_error: 0.2417 - val_loss: 0.0185 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0723 - mae: 0.0881 - root_mean_squared_error: 0.2444\n",
      "Epoch 85: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0723 - mae: 0.0881 - root_mean_squared_error: 0.2444 - val_loss: 0.0185 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0737 - mae: 0.1013 - root_mean_squared_error: 0.2474\n",
      "Epoch 86: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0737 - mae: 0.1013 - root_mean_squared_error: 0.2474 - val_loss: 0.0185 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0894 - root_mean_squared_error: 0.2429\n",
      "Epoch 87: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0715 - mae: 0.0894 - root_mean_squared_error: 0.2429 - val_loss: 0.0185 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0785 - root_mean_squared_error: 0.2415\n",
      "Epoch 88: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0709 - mae: 0.0785 - root_mean_squared_error: 0.2415 - val_loss: 0.0185 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0659 - root_mean_squared_error: 0.2435\n",
      "Epoch 89: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0718 - mae: 0.0659 - root_mean_squared_error: 0.2435 - val_loss: 0.0185 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0690 - root_mean_squared_error: 0.2418\n",
      "Epoch 90: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0710 - mae: 0.0690 - root_mean_squared_error: 0.2418 - val_loss: 0.0185 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0755 - mae: 0.1172 - root_mean_squared_error: 0.2509\n",
      "Epoch 91: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0755 - mae: 0.1172 - root_mean_squared_error: 0.2509 - val_loss: 0.0185 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0757 - root_mean_squared_error: 0.2423\n",
      "Epoch 92: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0713 - mae: 0.0757 - root_mean_squared_error: 0.2423 - val_loss: 0.0185 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0663 - root_mean_squared_error: 0.2432\n",
      "Epoch 93: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0717 - mae: 0.0663 - root_mean_squared_error: 0.2432 - val_loss: 0.0185 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0674 - root_mean_squared_error: 0.2431\n",
      "Epoch 94: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 0.0716 - mae: 0.0674 - root_mean_squared_error: 0.2431 - val_loss: 0.0185 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0722 - mae: 0.0587 - root_mean_squared_error: 0.2442\n",
      "Epoch 95: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0722 - mae: 0.0587 - root_mean_squared_error: 0.2442 - val_loss: 0.0185 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0765 - root_mean_squared_error: 0.2425\n",
      "Epoch 96: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0714 - mae: 0.0765 - root_mean_squared_error: 0.2425 - val_loss: 0.0185 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0848 - root_mean_squared_error: 0.2416\n",
      "Epoch 97: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0709 - mae: 0.0848 - root_mean_squared_error: 0.2416 - val_loss: 0.0185 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0818 - root_mean_squared_error: 0.2428\n",
      "Epoch 98: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0715 - mae: 0.0818 - root_mean_squared_error: 0.2428 - val_loss: 0.0185 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0726 - root_mean_squared_error: 0.2418\n",
      "Epoch 99: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0710 - mae: 0.0726 - root_mean_squared_error: 0.2418 - val_loss: 0.0185 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0690 - root_mean_squared_error: 0.2418\n",
      "Epoch 100: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0710 - mae: 0.0690 - root_mean_squared_error: 0.2418 - val_loss: 0.0185 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "idx_tr: [ 0  1  2  4  5  6  7  8  9 10 11 12 13 14 15]\n",
      "idx_va: [3]\n",
      "train_mask: [ True  True  True False  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True]\n",
      "val_mask: [False False False  True False False False False False False False False\n",
      " False False False False]\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0731 - root_mean_squared_error: 0.2416\n",
      "Epoch 1: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.0709 - mae: 0.0731 - root_mean_squared_error: 0.2416 - val_loss: 0.0131 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0736 - root_mean_squared_error: 0.2425\n",
      "Epoch 2: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0714 - mae: 0.0736 - root_mean_squared_error: 0.2425 - val_loss: 0.0131 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0756 - mae: 0.1084 - root_mean_squared_error: 0.2511\n",
      "Epoch 3: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0756 - mae: 0.1084 - root_mean_squared_error: 0.2511 - val_loss: 0.0131 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0676 - root_mean_squared_error: 0.2428\n",
      "Epoch 4: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0715 - mae: 0.0676 - root_mean_squared_error: 0.2428 - val_loss: 0.0131 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0712 - root_mean_squared_error: 0.2435\n",
      "Epoch 5: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 0.0718 - mae: 0.0712 - root_mean_squared_error: 0.2435 - val_loss: 0.0131 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0700 - root_mean_squared_error: 0.2420\n",
      "Epoch 6: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0711 - mae: 0.0700 - root_mean_squared_error: 0.2420 - val_loss: 0.0131 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0672 - root_mean_squared_error: 0.2420\n",
      "Epoch 7: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0711 - mae: 0.0672 - root_mean_squared_error: 0.2420 - val_loss: 0.0130 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0828 - root_mean_squared_error: 0.2428\n",
      "Epoch 8: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0715 - mae: 0.0828 - root_mean_squared_error: 0.2428 - val_loss: 0.0130 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0863 - root_mean_squared_error: 0.2430\n",
      "Epoch 9: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0716 - mae: 0.0863 - root_mean_squared_error: 0.2430 - val_loss: 0.0130 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0723 - root_mean_squared_error: 0.2427\n",
      "Epoch 10: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0715 - mae: 0.0723 - root_mean_squared_error: 0.2427 - val_loss: 0.0130 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0708 - mae: 0.0801 - root_mean_squared_error: 0.2413\n",
      "Epoch 11: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.0708 - mae: 0.0801 - root_mean_squared_error: 0.2413 - val_loss: 0.0130 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0651 - root_mean_squared_error: 0.2424\n",
      "Epoch 12: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0713 - mae: 0.0651 - root_mean_squared_error: 0.2424 - val_loss: 0.0130 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0686 - root_mean_squared_error: 0.2426\n",
      "Epoch 13: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0714 - mae: 0.0686 - root_mean_squared_error: 0.2426 - val_loss: 0.0130 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0907 - root_mean_squared_error: 0.2433\n",
      "Epoch 14: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.0717 - mae: 0.0907 - root_mean_squared_error: 0.2433 - val_loss: 0.0130 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0723 - mae: 0.0957 - root_mean_squared_error: 0.2444\n",
      "Epoch 15: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0723 - mae: 0.0957 - root_mean_squared_error: 0.2444 - val_loss: 0.0130 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0649 - root_mean_squared_error: 0.2433\n",
      "Epoch 16: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0718 - mae: 0.0649 - root_mean_squared_error: 0.2433 - val_loss: 0.0130 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0636 - root_mean_squared_error: 0.2435\n",
      "Epoch 17: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0718 - mae: 0.0636 - root_mean_squared_error: 0.2435 - val_loss: 0.0130 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0584 - root_mean_squared_error: 0.2437\n",
      "Epoch 18: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0719 - mae: 0.0584 - root_mean_squared_error: 0.2437 - val_loss: 0.0130 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0743 - root_mean_squared_error: 0.2425\n",
      "Epoch 19: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0714 - mae: 0.0743 - root_mean_squared_error: 0.2425 - val_loss: 0.0130 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0747 - root_mean_squared_error: 0.2438\n",
      "Epoch 20: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0720 - mae: 0.0747 - root_mean_squared_error: 0.2438 - val_loss: 0.0130 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0727 - root_mean_squared_error: 0.2428\n",
      "Epoch 21: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0715 - mae: 0.0727 - root_mean_squared_error: 0.2428 - val_loss: 0.0130 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0722 - mae: 0.0606 - root_mean_squared_error: 0.2443\n",
      "Epoch 22: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0722 - mae: 0.0606 - root_mean_squared_error: 0.2443 - val_loss: 0.0130 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0787 - root_mean_squared_error: 0.2435\n",
      "Epoch 23: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0718 - mae: 0.0787 - root_mean_squared_error: 0.2435 - val_loss: 0.0130 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0779 - root_mean_squared_error: 0.2424\n",
      "Epoch 24: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0713 - mae: 0.0779 - root_mean_squared_error: 0.2424 - val_loss: 0.0130 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0684 - root_mean_squared_error: 0.2428\n",
      "Epoch 25: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0715 - mae: 0.0684 - root_mean_squared_error: 0.2428 - val_loss: 0.0130 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0801 - root_mean_squared_error: 0.2434\n",
      "Epoch 26: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0718 - mae: 0.0801 - root_mean_squared_error: 0.2434 - val_loss: 0.0130 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0709 - root_mean_squared_error: 0.2429\n",
      "Epoch 27: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.0715 - mae: 0.0709 - root_mean_squared_error: 0.2429 - val_loss: 0.0130 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0740 - mae: 0.0974 - root_mean_squared_error: 0.2480\n",
      "Epoch 28: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.0740 - mae: 0.0974 - root_mean_squared_error: 0.2480 - val_loss: 0.0130 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0607 - root_mean_squared_error: 0.2440\n",
      "Epoch 29: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0721 - mae: 0.0607 - root_mean_squared_error: 0.2440 - val_loss: 0.0130 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0622 - root_mean_squared_error: 0.2422\n",
      "Epoch 30: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0712 - mae: 0.0622 - root_mean_squared_error: 0.2422 - val_loss: 0.0130 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0663 - root_mean_squared_error: 0.2438\n",
      "Epoch 31: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0720 - mae: 0.0663 - root_mean_squared_error: 0.2438 - val_loss: 0.0130 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0801 - root_mean_squared_error: 0.2425\n",
      "Epoch 32: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0713 - mae: 0.0801 - root_mean_squared_error: 0.2425 - val_loss: 0.0130 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0722 - mae: 0.0589 - root_mean_squared_error: 0.2443\n",
      "Epoch 33: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0722 - mae: 0.0589 - root_mean_squared_error: 0.2443 - val_loss: 0.0130 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0670 - root_mean_squared_error: 0.2428\n",
      "Epoch 34: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0715 - mae: 0.0670 - root_mean_squared_error: 0.2428 - val_loss: 0.0130 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0726 - root_mean_squared_error: 0.2441\n",
      "Epoch 35: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0721 - mae: 0.0726 - root_mean_squared_error: 0.2441 - val_loss: 0.0130 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0682 - root_mean_squared_error: 0.2420\n",
      "Epoch 36: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0711 - mae: 0.0682 - root_mean_squared_error: 0.2420 - val_loss: 0.0130 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0673 - root_mean_squared_error: 0.2441\n",
      "Epoch 37: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0721 - mae: 0.0673 - root_mean_squared_error: 0.2441 - val_loss: 0.0130 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0887 - root_mean_squared_error: 0.2423\n",
      "Epoch 38: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 0.0712 - mae: 0.0887 - root_mean_squared_error: 0.2423 - val_loss: 0.0130 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0715 - root_mean_squared_error: 0.2421\n",
      "Epoch 39: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0711 - mae: 0.0715 - root_mean_squared_error: 0.2421 - val_loss: 0.0130 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0735 - mae: 0.0812 - root_mean_squared_error: 0.2469\n",
      "Epoch 40: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 0.0735 - mae: 0.0812 - root_mean_squared_error: 0.2469 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0723 - mae: 0.0626 - root_mean_squared_error: 0.2445\n",
      "Epoch 41: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0723 - mae: 0.0626 - root_mean_squared_error: 0.2445 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0798 - root_mean_squared_error: 0.2437\n",
      "Epoch 42: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0719 - mae: 0.0798 - root_mean_squared_error: 0.2437 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0777 - root_mean_squared_error: 0.2427\n",
      "Epoch 43: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0715 - mae: 0.0777 - root_mean_squared_error: 0.2427 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0688 - root_mean_squared_error: 0.2428\n",
      "Epoch 44: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0715 - mae: 0.0688 - root_mean_squared_error: 0.2428 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0709 - root_mean_squared_error: 0.2419\n",
      "Epoch 45: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0710 - mae: 0.0709 - root_mean_squared_error: 0.2419 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0751 - root_mean_squared_error: 0.2425\n",
      "Epoch 46: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0714 - mae: 0.0751 - root_mean_squared_error: 0.2425 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0578 - root_mean_squared_error: 0.2438\n",
      "Epoch 47: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0720 - mae: 0.0578 - root_mean_squared_error: 0.2438 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0639 - root_mean_squared_error: 0.2433\n",
      "Epoch 48: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0717 - mae: 0.0639 - root_mean_squared_error: 0.2433 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0771 - root_mean_squared_error: 0.2427\n",
      "Epoch 49: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0714 - mae: 0.0771 - root_mean_squared_error: 0.2427 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0731 - root_mean_squared_error: 0.2418\n",
      "Epoch 50: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.0710 - mae: 0.0731 - root_mean_squared_error: 0.2418 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0735 - mae: 0.0856 - root_mean_squared_error: 0.2469\n",
      "Epoch 51: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0735 - mae: 0.0856 - root_mean_squared_error: 0.2469 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0790 - root_mean_squared_error: 0.2426\n",
      "Epoch 52: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0714 - mae: 0.0790 - root_mean_squared_error: 0.2426 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0761 - root_mean_squared_error: 0.2433\n",
      "Epoch 53: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0717 - mae: 0.0761 - root_mean_squared_error: 0.2433 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0766 - root_mean_squared_error: 0.2425\n",
      "Epoch 54: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0714 - mae: 0.0766 - root_mean_squared_error: 0.2425 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0771 - root_mean_squared_error: 0.2424\n",
      "Epoch 55: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0713 - mae: 0.0771 - root_mean_squared_error: 0.2424 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0830 - root_mean_squared_error: 0.2435\n",
      "Epoch 56: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0719 - mae: 0.0830 - root_mean_squared_error: 0.2435 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0784 - root_mean_squared_error: 0.2434\n",
      "Epoch 57: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0718 - mae: 0.0784 - root_mean_squared_error: 0.2434 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0860 - root_mean_squared_error: 0.2426\n",
      "Epoch 58: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0714 - mae: 0.0860 - root_mean_squared_error: 0.2426 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0777 - root_mean_squared_error: 0.2424\n",
      "Epoch 59: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0713 - mae: 0.0777 - root_mean_squared_error: 0.2424 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0746 - mae: 0.1056 - root_mean_squared_error: 0.2491\n",
      "Epoch 60: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0746 - mae: 0.1056 - root_mean_squared_error: 0.2491 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0880 - root_mean_squared_error: 0.2433\n",
      "Epoch 61: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0717 - mae: 0.0880 - root_mean_squared_error: 0.2433 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0703 - root_mean_squared_error: 0.2435\n",
      "Epoch 62: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0718 - mae: 0.0703 - root_mean_squared_error: 0.2435 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0750 - mae: 0.1016 - root_mean_squared_error: 0.2500\n",
      "Epoch 63: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 240ms/step - loss: 0.0750 - mae: 0.1016 - root_mean_squared_error: 0.2500 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0740 - root_mean_squared_error: 0.2424\n",
      "Epoch 64: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0713 - mae: 0.0740 - root_mean_squared_error: 0.2424 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0763 - root_mean_squared_error: 0.2426\n",
      "Epoch 65: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0714 - mae: 0.0763 - root_mean_squared_error: 0.2426 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0896 - root_mean_squared_error: 0.2434\n",
      "Epoch 66: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0718 - mae: 0.0896 - root_mean_squared_error: 0.2434 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0821 - root_mean_squared_error: 0.2427\n",
      "Epoch 67: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0714 - mae: 0.0821 - root_mean_squared_error: 0.2427 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0856 - root_mean_squared_error: 0.2415\n",
      "Epoch 68: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0709 - mae: 0.0856 - root_mean_squared_error: 0.2415 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0728 - mae: 0.0509 - root_mean_squared_error: 0.2454\n",
      "Epoch 69: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0728 - mae: 0.0509 - root_mean_squared_error: 0.2454 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0708 - mae: 0.0796 - root_mean_squared_error: 0.2414\n",
      "Epoch 70: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 0.0708 - mae: 0.0796 - root_mean_squared_error: 0.2414 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0646 - root_mean_squared_error: 0.2434\n",
      "Epoch 71: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0718 - mae: 0.0646 - root_mean_squared_error: 0.2434 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0722 - mae: 0.0602 - root_mean_squared_error: 0.2442\n",
      "Epoch 72: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0722 - mae: 0.0602 - root_mean_squared_error: 0.2442 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0762 - root_mean_squared_error: 0.2436\n",
      "Epoch 73: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0719 - mae: 0.0762 - root_mean_squared_error: 0.2436 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0723 - mae: 0.0636 - root_mean_squared_error: 0.2445\n",
      "Epoch 74: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0723 - mae: 0.0636 - root_mean_squared_error: 0.2445 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0743 - root_mean_squared_error: 0.2426\n",
      "Epoch 75: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0714 - mae: 0.0743 - root_mean_squared_error: 0.2426 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0726 - mae: 0.0591 - root_mean_squared_error: 0.2452\n",
      "Epoch 76: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0726 - mae: 0.0591 - root_mean_squared_error: 0.2452 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0724 - mae: 0.0863 - root_mean_squared_error: 0.2446\n",
      "Epoch 77: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 0.0724 - mae: 0.0863 - root_mean_squared_error: 0.2446 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0591 - root_mean_squared_error: 0.2433\n",
      "Epoch 78: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0717 - mae: 0.0591 - root_mean_squared_error: 0.2433 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0755 - root_mean_squared_error: 0.2418\n",
      "Epoch 79: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0710 - mae: 0.0755 - root_mean_squared_error: 0.2418 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0705 - mae: 0.0781 - root_mean_squared_error: 0.2408\n",
      "Epoch 80: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0705 - mae: 0.0781 - root_mean_squared_error: 0.2408 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0708 - mae: 0.0733 - root_mean_squared_error: 0.2414\n",
      "Epoch 81: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0708 - mae: 0.0733 - root_mean_squared_error: 0.2414 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0791 - root_mean_squared_error: 0.2425\n",
      "Epoch 82: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0713 - mae: 0.0791 - root_mean_squared_error: 0.2425 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0657 - root_mean_squared_error: 0.2442\n",
      "Epoch 83: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0721 - mae: 0.0657 - root_mean_squared_error: 0.2442 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0863 - root_mean_squared_error: 0.2438\n",
      "Epoch 84: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 0.0720 - mae: 0.0863 - root_mean_squared_error: 0.2438 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0735 - root_mean_squared_error: 0.2424\n",
      "Epoch 85: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0713 - mae: 0.0735 - root_mean_squared_error: 0.2424 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0781 - root_mean_squared_error: 0.2416\n",
      "Epoch 86: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0709 - mae: 0.0781 - root_mean_squared_error: 0.2416 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0795 - root_mean_squared_error: 0.2416\n",
      "Epoch 87: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0709 - mae: 0.0795 - root_mean_squared_error: 0.2416 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0812 - root_mean_squared_error: 0.2426\n",
      "Epoch 88: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0714 - mae: 0.0812 - root_mean_squared_error: 0.2426 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0723 - mae: 0.0638 - root_mean_squared_error: 0.2444\n",
      "Epoch 89: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0723 - mae: 0.0638 - root_mean_squared_error: 0.2444 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0836 - root_mean_squared_error: 0.2426\n",
      "Epoch 90: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.0714 - mae: 0.0836 - root_mean_squared_error: 0.2426 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0881 - root_mean_squared_error: 0.2441\n",
      "Epoch 91: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0721 - mae: 0.0881 - root_mean_squared_error: 0.2441 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0716 - root_mean_squared_error: 0.2425\n",
      "Epoch 92: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0713 - mae: 0.0716 - root_mean_squared_error: 0.2425 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0831 - root_mean_squared_error: 0.2436\n",
      "Epoch 93: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0719 - mae: 0.0831 - root_mean_squared_error: 0.2436 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0791 - root_mean_squared_error: 0.2438\n",
      "Epoch 94: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0720 - mae: 0.0791 - root_mean_squared_error: 0.2438 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0652 - root_mean_squared_error: 0.2423\n",
      "Epoch 95: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0713 - mae: 0.0652 - root_mean_squared_error: 0.2423 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0839 - root_mean_squared_error: 0.2424\n",
      "Epoch 96: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 0.0713 - mae: 0.0839 - root_mean_squared_error: 0.2424 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0845 - root_mean_squared_error: 0.2437\n",
      "Epoch 97: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0719 - mae: 0.0845 - root_mean_squared_error: 0.2437 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0577 - root_mean_squared_error: 0.2437\n",
      "Epoch 98: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0719 - mae: 0.0577 - root_mean_squared_error: 0.2437 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0711 - root_mean_squared_error: 0.2441\n",
      "Epoch 99: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0721 - mae: 0.0711 - root_mean_squared_error: 0.2441 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0607 - root_mean_squared_error: 0.2442\n",
      "Epoch 100: val_loss did not improve from 0.01285\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0721 - mae: 0.0607 - root_mean_squared_error: 0.2442 - val_loss: 0.0130 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "idx_tr: [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15]\n",
      "idx_va: [4]\n",
      "train_mask: [ True  True  True  True False  True  True  True  True  True  True  True\n",
      "  True  True  True  True]\n",
      "val_mask: [False False False False  True False False False False False False False\n",
      " False False False False]\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0652 - root_mean_squared_error: 0.2435\n",
      "Epoch 1: val_loss improved from 0.01285 to 0.01270, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.0718 - mae: 0.0652 - root_mean_squared_error: 0.2435 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0669 - root_mean_squared_error: 0.2417\n",
      "Epoch 2: val_loss improved from 0.01270 to 0.01270, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 0.0709 - mae: 0.0669 - root_mean_squared_error: 0.2417 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0703 - root_mean_squared_error: 0.2437\n",
      "Epoch 3: val_loss improved from 0.01270 to 0.01270, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0719 - mae: 0.0703 - root_mean_squared_error: 0.2437 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0799 - root_mean_squared_error: 0.2424\n",
      "Epoch 4: val_loss improved from 0.01270 to 0.01270, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0713 - mae: 0.0799 - root_mean_squared_error: 0.2424 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0744 - root_mean_squared_error: 0.2425\n",
      "Epoch 5: val_loss improved from 0.01270 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.0713 - mae: 0.0744 - root_mean_squared_error: 0.2425 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0703 - root_mean_squared_error: 0.2438\n",
      "Epoch 6: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0719 - mae: 0.0703 - root_mean_squared_error: 0.2438 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0705 - root_mean_squared_error: 0.2438\n",
      "Epoch 7: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.0720 - mae: 0.0705 - root_mean_squared_error: 0.2438 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0762 - root_mean_squared_error: 0.2435\n",
      "Epoch 8: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 0.0718 - mae: 0.0762 - root_mean_squared_error: 0.2435 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0726 - mae: 0.0889 - root_mean_squared_error: 0.2451\n",
      "Epoch 9: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0726 - mae: 0.0889 - root_mean_squared_error: 0.2451 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0699 - root_mean_squared_error: 0.2436\n",
      "Epoch 10: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0719 - mae: 0.0699 - root_mean_squared_error: 0.2436 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0742 - root_mean_squared_error: 0.2426\n",
      "Epoch 11: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0714 - mae: 0.0742 - root_mean_squared_error: 0.2426 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0664 - root_mean_squared_error: 0.2441\n",
      "Epoch 12: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0721 - mae: 0.0664 - root_mean_squared_error: 0.2441 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0811 - root_mean_squared_error: 0.2427\n",
      "Epoch 13: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0714 - mae: 0.0811 - root_mean_squared_error: 0.2427 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0869 - root_mean_squared_error: 0.2441\n",
      "Epoch 14: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 0.0721 - mae: 0.0869 - root_mean_squared_error: 0.2441 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0651 - root_mean_squared_error: 0.2420\n",
      "Epoch 15: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0711 - mae: 0.0651 - root_mean_squared_error: 0.2420 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0684 - root_mean_squared_error: 0.2440\n",
      "Epoch 16: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0720 - mae: 0.0684 - root_mean_squared_error: 0.2440 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0781 - root_mean_squared_error: 0.2425\n",
      "Epoch 17: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.0713 - mae: 0.0781 - root_mean_squared_error: 0.2425 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0725 - mae: 0.0575 - root_mean_squared_error: 0.2449\n",
      "Epoch 18: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0725 - mae: 0.0575 - root_mean_squared_error: 0.2449 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0724 - root_mean_squared_error: 0.2440\n",
      "Epoch 19: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 0.0720 - mae: 0.0724 - root_mean_squared_error: 0.2440 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0708 - mae: 0.0795 - root_mean_squared_error: 0.2415\n",
      "Epoch 20: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0708 - mae: 0.0795 - root_mean_squared_error: 0.2415 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0762 - root_mean_squared_error: 0.2422\n",
      "Epoch 21: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0712 - mae: 0.0762 - root_mean_squared_error: 0.2422 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0678 - root_mean_squared_error: 0.2422\n",
      "Epoch 22: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0712 - mae: 0.0678 - root_mean_squared_error: 0.2422 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0723 - root_mean_squared_error: 0.2422\n",
      "Epoch 23: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0712 - mae: 0.0723 - root_mean_squared_error: 0.2422 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0679 - root_mean_squared_error: 0.2431\n",
      "Epoch 24: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0716 - mae: 0.0679 - root_mean_squared_error: 0.2431 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0735 - mae: 0.1021 - root_mean_squared_error: 0.2470\n",
      "Epoch 25: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.0735 - mae: 0.1021 - root_mean_squared_error: 0.2470 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0710 - root_mean_squared_error: 0.2417\n",
      "Epoch 26: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0709 - mae: 0.0710 - root_mean_squared_error: 0.2417 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0812 - root_mean_squared_error: 0.2429\n",
      "Epoch 27: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.0715 - mae: 0.0812 - root_mean_squared_error: 0.2429 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0807 - mae: 0.1280 - root_mean_squared_error: 0.2611\n",
      "Epoch 28: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0807 - mae: 0.1280 - root_mean_squared_error: 0.2611 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0601 - root_mean_squared_error: 0.2438\n",
      "Epoch 29: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 0.0720 - mae: 0.0601 - root_mean_squared_error: 0.2438 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0728 - mae: 0.0956 - root_mean_squared_error: 0.2454\n",
      "Epoch 30: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0728 - mae: 0.0956 - root_mean_squared_error: 0.2454 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0806 - root_mean_squared_error: 0.2437\n",
      "Epoch 31: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.0719 - mae: 0.0806 - root_mean_squared_error: 0.2437 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0822 - root_mean_squared_error: 0.2431\n",
      "Epoch 32: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.0716 - mae: 0.0822 - root_mean_squared_error: 0.2431 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0745 - root_mean_squared_error: 0.2423\n",
      "Epoch 33: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0712 - mae: 0.0745 - root_mean_squared_error: 0.2423 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0613 - root_mean_squared_error: 0.2439\n",
      "Epoch 34: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.0720 - mae: 0.0613 - root_mean_squared_error: 0.2439 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0689 - root_mean_squared_error: 0.2438\n",
      "Epoch 35: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0720 - mae: 0.0689 - root_mean_squared_error: 0.2438 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0728 - mae: 0.0625 - root_mean_squared_error: 0.2456\n",
      "Epoch 36: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0728 - mae: 0.0625 - root_mean_squared_error: 0.2456 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0843 - root_mean_squared_error: 0.2423\n",
      "Epoch 37: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0712 - mae: 0.0843 - root_mean_squared_error: 0.2423 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0714 - root_mean_squared_error: 0.2437\n",
      "Epoch 38: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0719 - mae: 0.0714 - root_mean_squared_error: 0.2437 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0705 - mae: 0.0782 - root_mean_squared_error: 0.2408\n",
      "Epoch 39: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 0.0705 - mae: 0.0782 - root_mean_squared_error: 0.2408 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0730 - root_mean_squared_error: 0.2425\n",
      "Epoch 40: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0713 - mae: 0.0730 - root_mean_squared_error: 0.2425 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0691 - root_mean_squared_error: 0.2429\n",
      "Epoch 41: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.0715 - mae: 0.0691 - root_mean_squared_error: 0.2429 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0686 - root_mean_squared_error: 0.2422\n",
      "Epoch 42: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0712 - mae: 0.0686 - root_mean_squared_error: 0.2422 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0642 - root_mean_squared_error: 0.2436\n",
      "Epoch 43: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.0718 - mae: 0.0642 - root_mean_squared_error: 0.2436 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0788 - root_mean_squared_error: 0.2419\n",
      "Epoch 44: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 0.0711 - mae: 0.0788 - root_mean_squared_error: 0.2419 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0932 - root_mean_squared_error: 0.2437\n",
      "Epoch 45: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 0.0719 - mae: 0.0932 - root_mean_squared_error: 0.2437 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0743 - root_mean_squared_error: 0.2416\n",
      "Epoch 46: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0709 - mae: 0.0743 - root_mean_squared_error: 0.2416 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0750 - root_mean_squared_error: 0.2435\n",
      "Epoch 47: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0718 - mae: 0.0750 - root_mean_squared_error: 0.2435 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0721 - root_mean_squared_error: 0.2428\n",
      "Epoch 48: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0715 - mae: 0.0721 - root_mean_squared_error: 0.2428 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0645 - root_mean_squared_error: 0.2434\n",
      "Epoch 49: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 0.0717 - mae: 0.0645 - root_mean_squared_error: 0.2434 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0708 - mae: 0.0792 - root_mean_squared_error: 0.2415\n",
      "Epoch 50: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0708 - mae: 0.0792 - root_mean_squared_error: 0.2415 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0723 - mae: 0.0878 - root_mean_squared_error: 0.2446\n",
      "Epoch 51: val_loss improved from 0.01269 to 0.01269, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0723 - mae: 0.0878 - root_mean_squared_error: 0.2446 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0829 - root_mean_squared_error: 0.2437\n",
      "Epoch 52: val_loss improved from 0.01269 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0719 - mae: 0.0829 - root_mean_squared_error: 0.2437 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0750 - root_mean_squared_error: 0.2439\n",
      "Epoch 53: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.0720 - mae: 0.0750 - root_mean_squared_error: 0.2439 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0783 - root_mean_squared_error: 0.2429\n",
      "Epoch 54: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 0.0715 - mae: 0.0783 - root_mean_squared_error: 0.2429 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0748 - mae: 0.0994 - root_mean_squared_error: 0.2495\n",
      "Epoch 55: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 0.0748 - mae: 0.0994 - root_mean_squared_error: 0.2495 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0771 - mae: 0.1018 - root_mean_squared_error: 0.2540\n",
      "Epoch 56: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0771 - mae: 0.1018 - root_mean_squared_error: 0.2540 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0711 - root_mean_squared_error: 0.2426\n",
      "Epoch 57: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.0714 - mae: 0.0711 - root_mean_squared_error: 0.2426 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0942 - root_mean_squared_error: 0.2431\n",
      "Epoch 58: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0716 - mae: 0.0942 - root_mean_squared_error: 0.2431 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0736 - mae: 0.0736 - root_mean_squared_error: 0.2472\n",
      "Epoch 59: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 0.0736 - mae: 0.0736 - root_mean_squared_error: 0.2472 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0800 - mae: 0.1215 - root_mean_squared_error: 0.2597\n",
      "Epoch 60: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0800 - mae: 0.1215 - root_mean_squared_error: 0.2597 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0732 - mae: 0.1032 - root_mean_squared_error: 0.2463\n",
      "Epoch 61: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0732 - mae: 0.1032 - root_mean_squared_error: 0.2463 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0690 - root_mean_squared_error: 0.2433\n",
      "Epoch 62: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0717 - mae: 0.0690 - root_mean_squared_error: 0.2433 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0823 - root_mean_squared_error: 0.2425\n",
      "Epoch 63: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0713 - mae: 0.0823 - root_mean_squared_error: 0.2425 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0700 - root_mean_squared_error: 0.2429\n",
      "Epoch 64: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 0.0715 - mae: 0.0700 - root_mean_squared_error: 0.2429 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0880 - root_mean_squared_error: 0.2442\n",
      "Epoch 65: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0721 - mae: 0.0880 - root_mean_squared_error: 0.2442 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0694 - root_mean_squared_error: 0.2430\n",
      "Epoch 66: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.0715 - mae: 0.0694 - root_mean_squared_error: 0.2430 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0744 - mae: 0.0850 - root_mean_squared_error: 0.2488\n",
      "Epoch 67: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0744 - mae: 0.0850 - root_mean_squared_error: 0.2488 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0842 - root_mean_squared_error: 0.2435\n",
      "Epoch 68: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0718 - mae: 0.0842 - root_mean_squared_error: 0.2435 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0739 - root_mean_squared_error: 0.2417\n",
      "Epoch 69: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 0.0709 - mae: 0.0739 - root_mean_squared_error: 0.2417 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0766 - root_mean_squared_error: 0.2435\n",
      "Epoch 70: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.0718 - mae: 0.0766 - root_mean_squared_error: 0.2435 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0781 - root_mean_squared_error: 0.2431\n",
      "Epoch 71: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0716 - mae: 0.0781 - root_mean_squared_error: 0.2431 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0814 - root_mean_squared_error: 0.2417\n",
      "Epoch 72: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0709 - mae: 0.0814 - root_mean_squared_error: 0.2417 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0717 - root_mean_squared_error: 0.2419\n",
      "Epoch 73: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0710 - mae: 0.0717 - root_mean_squared_error: 0.2419 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0669 - root_mean_squared_error: 0.2441\n",
      "Epoch 74: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.0721 - mae: 0.0669 - root_mean_squared_error: 0.2441 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0671 - root_mean_squared_error: 0.2430\n",
      "Epoch 75: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.0716 - mae: 0.0671 - root_mean_squared_error: 0.2430 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0730 - root_mean_squared_error: 0.2424\n",
      "Epoch 76: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0713 - mae: 0.0730 - root_mean_squared_error: 0.2424 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0724 - mae: 0.0541 - root_mean_squared_error: 0.2447\n",
      "Epoch 77: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0724 - mae: 0.0541 - root_mean_squared_error: 0.2447 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0726 - mae: 0.0982 - root_mean_squared_error: 0.2452\n",
      "Epoch 78: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.0726 - mae: 0.0982 - root_mean_squared_error: 0.2452 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0792 - root_mean_squared_error: 0.2428\n",
      "Epoch 79: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0714 - mae: 0.0792 - root_mean_squared_error: 0.2428 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0633 - root_mean_squared_error: 0.2437\n",
      "Epoch 80: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.0719 - mae: 0.0633 - root_mean_squared_error: 0.2437 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0838 - root_mean_squared_error: 0.2436\n",
      "Epoch 81: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0718 - mae: 0.0838 - root_mean_squared_error: 0.2436 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0753 - root_mean_squared_error: 0.2411\n",
      "Epoch 82: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0706 - mae: 0.0753 - root_mean_squared_error: 0.2411 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0728 - root_mean_squared_error: 0.2437\n",
      "Epoch 83: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.0719 - mae: 0.0728 - root_mean_squared_error: 0.2437 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0774 - root_mean_squared_error: 0.2418\n",
      "Epoch 84: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 0.0710 - mae: 0.0774 - root_mean_squared_error: 0.2418 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0689 - root_mean_squared_error: 0.2440\n",
      "Epoch 85: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.0720 - mae: 0.0689 - root_mean_squared_error: 0.2440 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0756 - root_mean_squared_error: 0.2435\n",
      "Epoch 86: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.0718 - mae: 0.0756 - root_mean_squared_error: 0.2435 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0708 - root_mean_squared_error: 0.2435\n",
      "Epoch 87: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0718 - mae: 0.0708 - root_mean_squared_error: 0.2435 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0755 - mae: 0.1079 - root_mean_squared_error: 0.2510\n",
      "Epoch 88: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.0755 - mae: 0.1079 - root_mean_squared_error: 0.2510 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0672 - root_mean_squared_error: 0.2441\n",
      "Epoch 89: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.0721 - mae: 0.0672 - root_mean_squared_error: 0.2441 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0725 - root_mean_squared_error: 0.2416\n",
      "Epoch 90: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.0709 - mae: 0.0725 - root_mean_squared_error: 0.2416 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0633 - root_mean_squared_error: 0.2427\n",
      "Epoch 91: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.0714 - mae: 0.0633 - root_mean_squared_error: 0.2427 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0776 - root_mean_squared_error: 0.2432\n",
      "Epoch 92: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0717 - mae: 0.0776 - root_mean_squared_error: 0.2432 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0874 - root_mean_squared_error: 0.2428\n",
      "Epoch 93: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.0714 - mae: 0.0874 - root_mean_squared_error: 0.2428 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0729 - root_mean_squared_error: 0.2433\n",
      "Epoch 94: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0717 - mae: 0.0729 - root_mean_squared_error: 0.2433 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0698 - root_mean_squared_error: 0.2432\n",
      "Epoch 95: val_loss improved from 0.01268 to 0.01268, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0716 - mae: 0.0698 - root_mean_squared_error: 0.2432 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0860 - root_mean_squared_error: 0.2427\n",
      "Epoch 96: val_loss improved from 0.01268 to 0.01267, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.0714 - mae: 0.0860 - root_mean_squared_error: 0.2427 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0770 - root_mean_squared_error: 0.2422\n",
      "Epoch 97: val_loss improved from 0.01267 to 0.01267, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0712 - mae: 0.0770 - root_mean_squared_error: 0.2422 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0718 - root_mean_squared_error: 0.2426\n",
      "Epoch 98: val_loss improved from 0.01267 to 0.01267, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.0714 - mae: 0.0718 - root_mean_squared_error: 0.2426 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0645 - root_mean_squared_error: 0.2422\n",
      "Epoch 99: val_loss improved from 0.01267 to 0.01267, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0712 - mae: 0.0645 - root_mean_squared_error: 0.2422 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0815 - root_mean_squared_error: 0.2416\n",
      "Epoch 100: val_loss improved from 0.01267 to 0.01267, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0709 - mae: 0.0815 - root_mean_squared_error: 0.2416 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "idx_tr: [ 0  1  2  3  4  6  7  8  9 10 11 12 13 14 15]\n",
      "idx_va: [5]\n",
      "train_mask: [ True  True  True  True  True False  True  True  True  True  True  True\n",
      "  True  True  True  True]\n",
      "val_mask: [False False False False False  True False False False False False False\n",
      " False False False False]\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0872 - root_mean_squared_error: 0.2431\n",
      "Epoch 1: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.0716 - mae: 0.0872 - root_mean_squared_error: 0.2431 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0698 - root_mean_squared_error: 0.2426\n",
      "Epoch 2: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0714 - mae: 0.0698 - root_mean_squared_error: 0.2426 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0719 - root_mean_squared_error: 0.2426\n",
      "Epoch 3: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.0714 - mae: 0.0719 - root_mean_squared_error: 0.2426 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0761 - root_mean_squared_error: 0.2429\n",
      "Epoch 4: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0715 - mae: 0.0761 - root_mean_squared_error: 0.2429 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0725 - root_mean_squared_error: 0.2424\n",
      "Epoch 5: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0712 - mae: 0.0725 - root_mean_squared_error: 0.2424 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0639 - root_mean_squared_error: 0.2437\n",
      "Epoch 6: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0719 - mae: 0.0639 - root_mean_squared_error: 0.2437 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0841 - root_mean_squared_error: 0.2436\n",
      "Epoch 7: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0719 - mae: 0.0841 - root_mean_squared_error: 0.2436 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0614 - root_mean_squared_error: 0.2439\n",
      "Epoch 8: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.0720 - mae: 0.0614 - root_mean_squared_error: 0.2439 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0745 - mae: 0.1068 - root_mean_squared_error: 0.2490\n",
      "Epoch 9: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0745 - mae: 0.1068 - root_mean_squared_error: 0.2490 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0737 - root_mean_squared_error: 0.2427\n",
      "Epoch 10: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.0714 - mae: 0.0737 - root_mean_squared_error: 0.2427 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0854 - root_mean_squared_error: 0.2429\n",
      "Epoch 11: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 0.0715 - mae: 0.0854 - root_mean_squared_error: 0.2429 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0742 - mae: 0.0449 - root_mean_squared_error: 0.2484\n",
      "Epoch 12: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0742 - mae: 0.0449 - root_mean_squared_error: 0.2484 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0705 - root_mean_squared_error: 0.2427\n",
      "Epoch 13: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0714 - mae: 0.0705 - root_mean_squared_error: 0.2427 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0733 - mae: 0.0997 - root_mean_squared_error: 0.2466\n",
      "Epoch 14: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0733 - mae: 0.0997 - root_mean_squared_error: 0.2466 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0748 - mae: 0.1053 - root_mean_squared_error: 0.2496\n",
      "Epoch 15: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0748 - mae: 0.1053 - root_mean_squared_error: 0.2496 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0836 - root_mean_squared_error: 0.2427\n",
      "Epoch 16: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0714 - mae: 0.0836 - root_mean_squared_error: 0.2427 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0725 - mae: 0.0617 - root_mean_squared_error: 0.2450\n",
      "Epoch 17: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0725 - mae: 0.0617 - root_mean_squared_error: 0.2450 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0884 - root_mean_squared_error: 0.2430\n",
      "Epoch 18: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0715 - mae: 0.0884 - root_mean_squared_error: 0.2430 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0676 - root_mean_squared_error: 0.2436\n",
      "Epoch 19: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0718 - mae: 0.0676 - root_mean_squared_error: 0.2436 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0728 - root_mean_squared_error: 0.2433\n",
      "Epoch 20: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.0717 - mae: 0.0728 - root_mean_squared_error: 0.2433 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0722 - mae: 0.0909 - root_mean_squared_error: 0.2444\n",
      "Epoch 21: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.0722 - mae: 0.0909 - root_mean_squared_error: 0.2444 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0735 - mae: 0.0510 - root_mean_squared_error: 0.2470\n",
      "Epoch 22: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0735 - mae: 0.0510 - root_mean_squared_error: 0.2470 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0638 - root_mean_squared_error: 0.2425\n",
      "Epoch 23: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0713 - mae: 0.0638 - root_mean_squared_error: 0.2425 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0736 - mae: 0.0478 - root_mean_squared_error: 0.2473\n",
      "Epoch 24: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0736 - mae: 0.0478 - root_mean_squared_error: 0.2473 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0724 - mae: 0.0643 - root_mean_squared_error: 0.2447\n",
      "Epoch 25: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0724 - mae: 0.0643 - root_mean_squared_error: 0.2447 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0778 - root_mean_squared_error: 0.2420\n",
      "Epoch 26: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 0.0710 - mae: 0.0778 - root_mean_squared_error: 0.2420 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0664 - root_mean_squared_error: 0.2431\n",
      "Epoch 27: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0716 - mae: 0.0664 - root_mean_squared_error: 0.2431 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0702 - root_mean_squared_error: 0.2430\n",
      "Epoch 28: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0715 - mae: 0.0702 - root_mean_squared_error: 0.2430 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0758 - root_mean_squared_error: 0.2435\n",
      "Epoch 29: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0718 - mae: 0.0758 - root_mean_squared_error: 0.2435 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0667 - root_mean_squared_error: 0.2419\n",
      "Epoch 30: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0710 - mae: 0.0667 - root_mean_squared_error: 0.2419 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0793 - root_mean_squared_error: 0.2432\n",
      "Epoch 31: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0717 - mae: 0.0793 - root_mean_squared_error: 0.2432 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0805 - root_mean_squared_error: 0.2429\n",
      "Epoch 32: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0715 - mae: 0.0805 - root_mean_squared_error: 0.2429 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0632 - root_mean_squared_error: 0.2435\n",
      "Epoch 33: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0718 - mae: 0.0632 - root_mean_squared_error: 0.2435 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0834 - root_mean_squared_error: 0.2426\n",
      "Epoch 34: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0713 - mae: 0.0834 - root_mean_squared_error: 0.2426 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0724 - root_mean_squared_error: 0.2426\n",
      "Epoch 35: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0714 - mae: 0.0724 - root_mean_squared_error: 0.2426 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0735 - mae: 0.0811 - root_mean_squared_error: 0.2470\n",
      "Epoch 36: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0735 - mae: 0.0811 - root_mean_squared_error: 0.2470 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0730 - root_mean_squared_error: 0.2423\n",
      "Epoch 37: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0712 - mae: 0.0730 - root_mean_squared_error: 0.2423 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0835 - root_mean_squared_error: 0.2425\n",
      "Epoch 38: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0713 - mae: 0.0835 - root_mean_squared_error: 0.2425 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0612 - root_mean_squared_error: 0.2440\n",
      "Epoch 39: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0720 - mae: 0.0612 - root_mean_squared_error: 0.2440 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0735 - root_mean_squared_error: 0.2436\n",
      "Epoch 40: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0718 - mae: 0.0735 - root_mean_squared_error: 0.2436 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0660 - root_mean_squared_error: 0.2434\n",
      "Epoch 41: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0717 - mae: 0.0660 - root_mean_squared_error: 0.2434 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0826 - root_mean_squared_error: 0.2425\n",
      "Epoch 42: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0713 - mae: 0.0826 - root_mean_squared_error: 0.2425 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0714 - root_mean_squared_error: 0.2425\n",
      "Epoch 43: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0713 - mae: 0.0714 - root_mean_squared_error: 0.2425 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0842 - root_mean_squared_error: 0.2428\n",
      "Epoch 44: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0715 - mae: 0.0842 - root_mean_squared_error: 0.2428 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0771 - root_mean_squared_error: 0.2431\n",
      "Epoch 45: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 0.0716 - mae: 0.0771 - root_mean_squared_error: 0.2431 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0792 - root_mean_squared_error: 0.2417\n",
      "Epoch 46: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0709 - mae: 0.0792 - root_mean_squared_error: 0.2417 - val_loss: 0.0127 - val_mae: 0.0718 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0708 - mae: 0.0730 - root_mean_squared_error: 0.2415\n",
      "Epoch 47: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0708 - mae: 0.0730 - root_mean_squared_error: 0.2415 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0668 - root_mean_squared_error: 0.2429\n",
      "Epoch 48: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0715 - mae: 0.0668 - root_mean_squared_error: 0.2429 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0699 - root_mean_squared_error: 0.2426\n",
      "Epoch 49: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0714 - mae: 0.0699 - root_mean_squared_error: 0.2426 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0727 - mae: 0.0587 - root_mean_squared_error: 0.2454\n",
      "Epoch 50: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0727 - mae: 0.0587 - root_mean_squared_error: 0.2454 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0767 - root_mean_squared_error: 0.2436\n",
      "Epoch 51: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0718 - mae: 0.0767 - root_mean_squared_error: 0.2436 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0622 - root_mean_squared_error: 0.2426\n",
      "Epoch 52: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0714 - mae: 0.0622 - root_mean_squared_error: 0.2426 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0762 - root_mean_squared_error: 0.2418\n",
      "Epoch 53: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0710 - mae: 0.0762 - root_mean_squared_error: 0.2418 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0640 - root_mean_squared_error: 0.2431\n",
      "Epoch 54: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0716 - mae: 0.0640 - root_mean_squared_error: 0.2431 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0786 - root_mean_squared_error: 0.2436\n",
      "Epoch 55: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0718 - mae: 0.0786 - root_mean_squared_error: 0.2436 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0679 - root_mean_squared_error: 0.2419\n",
      "Epoch 56: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0710 - mae: 0.0679 - root_mean_squared_error: 0.2419 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0723 - mae: 0.0852 - root_mean_squared_error: 0.2445\n",
      "Epoch 57: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0723 - mae: 0.0852 - root_mean_squared_error: 0.2445 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0708 - mae: 0.0790 - root_mean_squared_error: 0.2415\n",
      "Epoch 58: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0708 - mae: 0.0790 - root_mean_squared_error: 0.2415 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0727 - mae: 0.0515 - root_mean_squared_error: 0.2453\n",
      "Epoch 59: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0727 - mae: 0.0515 - root_mean_squared_error: 0.2453 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0656 - root_mean_squared_error: 0.2429\n",
      "Epoch 60: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0715 - mae: 0.0656 - root_mean_squared_error: 0.2429 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0732 - mae: 0.0748 - root_mean_squared_error: 0.2464\n",
      "Epoch 61: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0732 - mae: 0.0748 - root_mean_squared_error: 0.2464 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0649 - root_mean_squared_error: 0.2441\n",
      "Epoch 62: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0721 - mae: 0.0649 - root_mean_squared_error: 0.2441 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0730 - mae: 0.0981 - root_mean_squared_error: 0.2460\n",
      "Epoch 63: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0730 - mae: 0.0981 - root_mean_squared_error: 0.2460 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0736 - mae: 0.0481 - root_mean_squared_error: 0.2473\n",
      "Epoch 64: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0736 - mae: 0.0481 - root_mean_squared_error: 0.2473 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0589 - root_mean_squared_error: 0.2433\n",
      "Epoch 65: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0717 - mae: 0.0589 - root_mean_squared_error: 0.2433 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0731 - root_mean_squared_error: 0.2422\n",
      "Epoch 66: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0712 - mae: 0.0731 - root_mean_squared_error: 0.2422 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0824 - root_mean_squared_error: 0.2430\n",
      "Epoch 67: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0715 - mae: 0.0824 - root_mean_squared_error: 0.2430 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0718 - root_mean_squared_error: 0.2434\n",
      "Epoch 68: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0717 - mae: 0.0718 - root_mean_squared_error: 0.2434 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0708 - mae: 0.0742 - root_mean_squared_error: 0.2416\n",
      "Epoch 69: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0708 - mae: 0.0742 - root_mean_squared_error: 0.2416 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0658 - root_mean_squared_error: 0.2442\n",
      "Epoch 70: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0721 - mae: 0.0658 - root_mean_squared_error: 0.2442 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0651 - root_mean_squared_error: 0.2430\n",
      "Epoch 71: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0715 - mae: 0.0651 - root_mean_squared_error: 0.2430 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0746 - mae: 0.1000 - root_mean_squared_error: 0.2492\n",
      "Epoch 72: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0746 - mae: 0.1000 - root_mean_squared_error: 0.2492 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0751 - root_mean_squared_error: 0.2424\n",
      "Epoch 73: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0713 - mae: 0.0751 - root_mean_squared_error: 0.2424 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0748 - root_mean_squared_error: 0.2426\n",
      "Epoch 74: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.0713 - mae: 0.0748 - root_mean_squared_error: 0.2426 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0688 - root_mean_squared_error: 0.2427\n",
      "Epoch 75: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0714 - mae: 0.0688 - root_mean_squared_error: 0.2427 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0728 - root_mean_squared_error: 0.2429\n",
      "Epoch 76: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.0715 - mae: 0.0728 - root_mean_squared_error: 0.2429 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0865 - root_mean_squared_error: 0.2437\n",
      "Epoch 77: val_loss improved from 0.01267 to 0.01267, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0719 - mae: 0.0865 - root_mean_squared_error: 0.2437 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0791 - root_mean_squared_error: 0.2435\n",
      "Epoch 78: val_loss improved from 0.01267 to 0.01267, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.0718 - mae: 0.0791 - root_mean_squared_error: 0.2435 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0708 - mae: 0.0760 - root_mean_squared_error: 0.2415\n",
      "Epoch 79: val_loss improved from 0.01267 to 0.01267, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.0708 - mae: 0.0760 - root_mean_squared_error: 0.2415 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0805 - mae: 0.1334 - root_mean_squared_error: 0.2607\n",
      "Epoch 80: val_loss improved from 0.01267 to 0.01267, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.0805 - mae: 0.1334 - root_mean_squared_error: 0.2607 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0645 - root_mean_squared_error: 0.2434\n",
      "Epoch 81: val_loss improved from 0.01267 to 0.01267, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0717 - mae: 0.0645 - root_mean_squared_error: 0.2434 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0719 - root_mean_squared_error: 0.2427\n",
      "Epoch 82: val_loss improved from 0.01267 to 0.01267, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.0714 - mae: 0.0719 - root_mean_squared_error: 0.2427 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0777 - root_mean_squared_error: 0.2428\n",
      "Epoch 83: val_loss improved from 0.01267 to 0.01267, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 0.0715 - mae: 0.0777 - root_mean_squared_error: 0.2428 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0821 - root_mean_squared_error: 0.2416\n",
      "Epoch 84: val_loss improved from 0.01267 to 0.01267, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0709 - mae: 0.0821 - root_mean_squared_error: 0.2416 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0849 - root_mean_squared_error: 0.2427\n",
      "Epoch 85: val_loss improved from 0.01267 to 0.01267, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0714 - mae: 0.0849 - root_mean_squared_error: 0.2427 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0657 - root_mean_squared_error: 0.2438\n",
      "Epoch 86: val_loss improved from 0.01267 to 0.01267, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0719 - mae: 0.0657 - root_mean_squared_error: 0.2438 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0811 - root_mean_squared_error: 0.2427\n",
      "Epoch 87: val_loss improved from 0.01267 to 0.01267, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 0.0714 - mae: 0.0811 - root_mean_squared_error: 0.2427 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0708 - mae: 0.0734 - root_mean_squared_error: 0.2415\n",
      "Epoch 88: val_loss improved from 0.01267 to 0.01267, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.0708 - mae: 0.0734 - root_mean_squared_error: 0.2415 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0663 - root_mean_squared_error: 0.2423\n",
      "Epoch 89: val_loss improved from 0.01267 to 0.01267, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0712 - mae: 0.0663 - root_mean_squared_error: 0.2423 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0685 - root_mean_squared_error: 0.2420\n",
      "Epoch 90: val_loss improved from 0.01267 to 0.01267, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.0711 - mae: 0.0685 - root_mean_squared_error: 0.2420 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0737 - mae: 0.0996 - root_mean_squared_error: 0.2475\n",
      "Epoch 91: val_loss improved from 0.01267 to 0.01267, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.0737 - mae: 0.0996 - root_mean_squared_error: 0.2475 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0807 - root_mean_squared_error: 0.2424\n",
      "Epoch 92: val_loss improved from 0.01267 to 0.01267, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.0712 - mae: 0.0807 - root_mean_squared_error: 0.2424 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0679 - root_mean_squared_error: 0.2438\n",
      "Epoch 93: val_loss improved from 0.01267 to 0.01267, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0719 - mae: 0.0679 - root_mean_squared_error: 0.2438 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0688 - root_mean_squared_error: 0.2439\n",
      "Epoch 94: val_loss improved from 0.01267 to 0.01267, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.0720 - mae: 0.0688 - root_mean_squared_error: 0.2439 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0695 - root_mean_squared_error: 0.2438\n",
      "Epoch 95: val_loss improved from 0.01267 to 0.01267, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 0.0719 - mae: 0.0695 - root_mean_squared_error: 0.2438 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0692 - root_mean_squared_error: 0.2429\n",
      "Epoch 96: val_loss improved from 0.01267 to 0.01267, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0715 - mae: 0.0692 - root_mean_squared_error: 0.2429 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0747 - root_mean_squared_error: 0.2426\n",
      "Epoch 97: val_loss improved from 0.01267 to 0.01267, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0714 - mae: 0.0747 - root_mean_squared_error: 0.2426 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0804 - mae: 0.1297 - root_mean_squared_error: 0.2606\n",
      "Epoch 98: val_loss improved from 0.01267 to 0.01267, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.0804 - mae: 0.1297 - root_mean_squared_error: 0.2606 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0779 - root_mean_squared_error: 0.2426\n",
      "Epoch 99: val_loss improved from 0.01267 to 0.01267, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0714 - mae: 0.0779 - root_mean_squared_error: 0.2426 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0639 - root_mean_squared_error: 0.2435\n",
      "Epoch 100: val_loss improved from 0.01267 to 0.01267, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 0.0718 - mae: 0.0639 - root_mean_squared_error: 0.2435 - val_loss: 0.0127 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "idx_tr: [ 0  1  2  3  4  5  7  8  9 10 11 12 13 14 15]\n",
      "idx_va: [6]\n",
      "train_mask: [ True  True  True  True  True  True False  True  True  True  True  True\n",
      "  True  True  True  True]\n",
      "val_mask: [False False False False False False  True False False False False False\n",
      " False False False False]\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0689 - root_mean_squared_error: 0.2430\n",
      "Epoch 1: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.0715 - mae: 0.0689 - root_mean_squared_error: 0.2430 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0781 - root_mean_squared_error: 0.2423\n",
      "Epoch 2: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0712 - mae: 0.0781 - root_mean_squared_error: 0.2423 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0702 - root_mean_squared_error: 0.2422\n",
      "Epoch 3: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0712 - mae: 0.0702 - root_mean_squared_error: 0.2422 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0752 - mae: 0.1165 - root_mean_squared_error: 0.2504\n",
      "Epoch 4: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 0.0752 - mae: 0.1165 - root_mean_squared_error: 0.2504 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0690 - root_mean_squared_error: 0.2424\n",
      "Epoch 5: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.0712 - mae: 0.0690 - root_mean_squared_error: 0.2424 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0662 - root_mean_squared_error: 0.2422\n",
      "Epoch 6: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0711 - mae: 0.0662 - root_mean_squared_error: 0.2422 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0890 - root_mean_squared_error: 0.2429\n",
      "Epoch 7: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0715 - mae: 0.0890 - root_mean_squared_error: 0.2429 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0727 - mae: 0.0545 - root_mean_squared_error: 0.2454\n",
      "Epoch 8: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 0.0727 - mae: 0.0545 - root_mean_squared_error: 0.2454 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0724 - mae: 0.0904 - root_mean_squared_error: 0.2448\n",
      "Epoch 9: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0724 - mae: 0.0904 - root_mean_squared_error: 0.2448 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0801 - root_mean_squared_error: 0.2425\n",
      "Epoch 10: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0713 - mae: 0.0801 - root_mean_squared_error: 0.2425 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0678 - root_mean_squared_error: 0.2432\n",
      "Epoch 11: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.0716 - mae: 0.0678 - root_mean_squared_error: 0.2432 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0663 - root_mean_squared_error: 0.2441\n",
      "Epoch 12: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0721 - mae: 0.0663 - root_mean_squared_error: 0.2441 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0685 - root_mean_squared_error: 0.2420\n",
      "Epoch 13: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0710 - mae: 0.0685 - root_mean_squared_error: 0.2420 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0748 - root_mean_squared_error: 0.2416\n",
      "Epoch 14: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0709 - mae: 0.0748 - root_mean_squared_error: 0.2416 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0744 - root_mean_squared_error: 0.2416\n",
      "Epoch 15: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0709 - mae: 0.0744 - root_mean_squared_error: 0.2416 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0584 - root_mean_squared_error: 0.2435\n",
      "Epoch 16: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 0.0718 - mae: 0.0584 - root_mean_squared_error: 0.2435 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0801 - root_mean_squared_error: 0.2441\n",
      "Epoch 17: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0721 - mae: 0.0801 - root_mean_squared_error: 0.2441 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0774 - root_mean_squared_error: 0.2435\n",
      "Epoch 18: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0718 - mae: 0.0774 - root_mean_squared_error: 0.2435 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0726 - mae: 0.0890 - root_mean_squared_error: 0.2451\n",
      "Epoch 19: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0726 - mae: 0.0890 - root_mean_squared_error: 0.2451 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0664 - root_mean_squared_error: 0.2432\n",
      "Epoch 20: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0716 - mae: 0.0664 - root_mean_squared_error: 0.2432 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0708 - mae: 0.0773 - root_mean_squared_error: 0.2414\n",
      "Epoch 21: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0708 - mae: 0.0773 - root_mean_squared_error: 0.2414 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0911 - root_mean_squared_error: 0.2442\n",
      "Epoch 22: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0721 - mae: 0.0911 - root_mean_squared_error: 0.2442 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0674 - root_mean_squared_error: 0.2432\n",
      "Epoch 23: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0716 - mae: 0.0674 - root_mean_squared_error: 0.2432 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0797 - root_mean_squared_error: 0.2411\n",
      "Epoch 24: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0706 - mae: 0.0797 - root_mean_squared_error: 0.2411 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0828 - root_mean_squared_error: 0.2426\n",
      "Epoch 25: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0713 - mae: 0.0828 - root_mean_squared_error: 0.2426 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0705 - root_mean_squared_error: 0.2427\n",
      "Epoch 26: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0714 - mae: 0.0705 - root_mean_squared_error: 0.2427 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0759 - mae: 0.1047 - root_mean_squared_error: 0.2518\n",
      "Epoch 27: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0759 - mae: 0.1047 - root_mean_squared_error: 0.2518 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0707 - mae: 0.0769 - root_mean_squared_error: 0.2413\n",
      "Epoch 28: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0707 - mae: 0.0769 - root_mean_squared_error: 0.2413 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0726 - mae: 0.0870 - root_mean_squared_error: 0.2452\n",
      "Epoch 29: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0726 - mae: 0.0870 - root_mean_squared_error: 0.2452 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0859 - root_mean_squared_error: 0.2417\n",
      "Epoch 30: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0709 - mae: 0.0859 - root_mean_squared_error: 0.2417 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0669 - root_mean_squared_error: 0.2432\n",
      "Epoch 31: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0716 - mae: 0.0669 - root_mean_squared_error: 0.2432 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0722 - mae: 0.0576 - root_mean_squared_error: 0.2445\n",
      "Epoch 32: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0722 - mae: 0.0576 - root_mean_squared_error: 0.2445 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0749 - root_mean_squared_error: 0.2425\n",
      "Epoch 33: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0713 - mae: 0.0749 - root_mean_squared_error: 0.2425 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0696 - root_mean_squared_error: 0.2437\n",
      "Epoch 34: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0719 - mae: 0.0696 - root_mean_squared_error: 0.2437 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0718 - root_mean_squared_error: 0.2426\n",
      "Epoch 35: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0714 - mae: 0.0718 - root_mean_squared_error: 0.2426 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0724 - mae: 0.0756 - root_mean_squared_error: 0.2448\n",
      "Epoch 36: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0724 - mae: 0.0756 - root_mean_squared_error: 0.2448 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0733 - mae: 0.0542 - root_mean_squared_error: 0.2466\n",
      "Epoch 37: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0733 - mae: 0.0542 - root_mean_squared_error: 0.2466 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0856 - root_mean_squared_error: 0.2429\n",
      "Epoch 38: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0715 - mae: 0.0856 - root_mean_squared_error: 0.2429 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0723 - mae: 0.0577 - root_mean_squared_error: 0.2445\n",
      "Epoch 39: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0723 - mae: 0.0577 - root_mean_squared_error: 0.2445 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0713 - root_mean_squared_error: 0.2418\n",
      "Epoch 40: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0709 - mae: 0.0713 - root_mean_squared_error: 0.2418 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0692 - root_mean_squared_error: 0.2430\n",
      "Epoch 41: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0715 - mae: 0.0692 - root_mean_squared_error: 0.2430 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0706 - root_mean_squared_error: 0.2432\n",
      "Epoch 42: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0716 - mae: 0.0706 - root_mean_squared_error: 0.2432 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0699 - root_mean_squared_error: 0.2427\n",
      "Epoch 43: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0714 - mae: 0.0699 - root_mean_squared_error: 0.2427 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0888 - root_mean_squared_error: 0.2432\n",
      "Epoch 44: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.0716 - mae: 0.0888 - root_mean_squared_error: 0.2432 - val_loss: 0.0128 - val_mae: 0.0719 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0731 - root_mean_squared_error: 0.2430\n",
      "Epoch 45: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0715 - mae: 0.0731 - root_mean_squared_error: 0.2430 - val_loss: 0.0128 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0676 - root_mean_squared_error: 0.2428\n",
      "Epoch 46: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0714 - mae: 0.0676 - root_mean_squared_error: 0.2428 - val_loss: 0.0128 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0708 - mae: 0.0779 - root_mean_squared_error: 0.2415\n",
      "Epoch 47: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0708 - mae: 0.0779 - root_mean_squared_error: 0.2415 - val_loss: 0.0128 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0779 - root_mean_squared_error: 0.2436\n",
      "Epoch 48: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0718 - mae: 0.0779 - root_mean_squared_error: 0.2436 - val_loss: 0.0128 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0820 - root_mean_squared_error: 0.2428\n",
      "Epoch 49: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0714 - mae: 0.0820 - root_mean_squared_error: 0.2428 - val_loss: 0.0128 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0695 - root_mean_squared_error: 0.2428\n",
      "Epoch 50: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0714 - mae: 0.0695 - root_mean_squared_error: 0.2428 - val_loss: 0.0128 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0722 - mae: 0.0595 - root_mean_squared_error: 0.2444\n",
      "Epoch 51: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0722 - mae: 0.0595 - root_mean_squared_error: 0.2444 - val_loss: 0.0128 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0774 - root_mean_squared_error: 0.2429\n",
      "Epoch 52: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0715 - mae: 0.0774 - root_mean_squared_error: 0.2429 - val_loss: 0.0128 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0717 - root_mean_squared_error: 0.2419\n",
      "Epoch 53: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0710 - mae: 0.0717 - root_mean_squared_error: 0.2419 - val_loss: 0.0128 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0744 - root_mean_squared_error: 0.2427\n",
      "Epoch 54: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0714 - mae: 0.0744 - root_mean_squared_error: 0.2427 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0768 - root_mean_squared_error: 0.2424\n",
      "Epoch 55: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0712 - mae: 0.0768 - root_mean_squared_error: 0.2424 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0707 - mae: 0.0791 - root_mean_squared_error: 0.2413\n",
      "Epoch 56: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0707 - mae: 0.0791 - root_mean_squared_error: 0.2413 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0722 - mae: 0.0657 - root_mean_squared_error: 0.2443\n",
      "Epoch 57: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0722 - mae: 0.0657 - root_mean_squared_error: 0.2443 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0699 - root_mean_squared_error: 0.2438\n",
      "Epoch 58: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0719 - mae: 0.0699 - root_mean_squared_error: 0.2438 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0638 - root_mean_squared_error: 0.2438\n",
      "Epoch 59: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0719 - mae: 0.0638 - root_mean_squared_error: 0.2438 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0734 - mae: 0.0769 - root_mean_squared_error: 0.2468\n",
      "Epoch 60: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0734 - mae: 0.0769 - root_mean_squared_error: 0.2468 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0806 - mae: 0.1337 - root_mean_squared_error: 0.2611\n",
      "Epoch 61: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0806 - mae: 0.1337 - root_mean_squared_error: 0.2611 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0700 - root_mean_squared_error: 0.2428\n",
      "Epoch 62: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0714 - mae: 0.0700 - root_mean_squared_error: 0.2428 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0816 - root_mean_squared_error: 0.2417\n",
      "Epoch 63: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0709 - mae: 0.0816 - root_mean_squared_error: 0.2417 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0711 - root_mean_squared_error: 0.2417\n",
      "Epoch 64: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0709 - mae: 0.0711 - root_mean_squared_error: 0.2417 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0651 - root_mean_squared_error: 0.2435\n",
      "Epoch 65: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0717 - mae: 0.0651 - root_mean_squared_error: 0.2435 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0722 - mae: 0.0885 - root_mean_squared_error: 0.2443\n",
      "Epoch 66: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.0722 - mae: 0.0885 - root_mean_squared_error: 0.2443 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0715 - root_mean_squared_error: 0.2427\n",
      "Epoch 67: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0714 - mae: 0.0715 - root_mean_squared_error: 0.2427 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0604 - root_mean_squared_error: 0.2429\n",
      "Epoch 68: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0715 - mae: 0.0604 - root_mean_squared_error: 0.2429 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0664 - root_mean_squared_error: 0.2431\n",
      "Epoch 69: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0716 - mae: 0.0664 - root_mean_squared_error: 0.2431 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0722 - mae: 0.0940 - root_mean_squared_error: 0.2443\n",
      "Epoch 70: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0722 - mae: 0.0940 - root_mean_squared_error: 0.2443 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0776 - root_mean_squared_error: 0.2439\n",
      "Epoch 71: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 0.0719 - mae: 0.0776 - root_mean_squared_error: 0.2439 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0691 - root_mean_squared_error: 0.2420\n",
      "Epoch 72: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0710 - mae: 0.0691 - root_mean_squared_error: 0.2420 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0677 - root_mean_squared_error: 0.2429\n",
      "Epoch 73: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0715 - mae: 0.0677 - root_mean_squared_error: 0.2429 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0728 - root_mean_squared_error: 0.2428\n",
      "Epoch 74: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.0714 - mae: 0.0728 - root_mean_squared_error: 0.2428 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0721 - root_mean_squared_error: 0.2423\n",
      "Epoch 75: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 0.0712 - mae: 0.0721 - root_mean_squared_error: 0.2423 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0688 - root_mean_squared_error: 0.2429\n",
      "Epoch 76: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0715 - mae: 0.0688 - root_mean_squared_error: 0.2429 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0730 - mae: 0.0650 - root_mean_squared_error: 0.2460\n",
      "Epoch 77: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0730 - mae: 0.0650 - root_mean_squared_error: 0.2460 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0668 - root_mean_squared_error: 0.2441\n",
      "Epoch 78: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0720 - mae: 0.0668 - root_mean_squared_error: 0.2441 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0981 - root_mean_squared_error: 0.2440\n",
      "Epoch 79: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0720 - mae: 0.0981 - root_mean_squared_error: 0.2440 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0786 - root_mean_squared_error: 0.2424\n",
      "Epoch 80: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0712 - mae: 0.0786 - root_mean_squared_error: 0.2424 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0659 - root_mean_squared_error: 0.2442\n",
      "Epoch 81: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0721 - mae: 0.0659 - root_mean_squared_error: 0.2442 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0705 - root_mean_squared_error: 0.2417\n",
      "Epoch 82: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0709 - mae: 0.0705 - root_mean_squared_error: 0.2417 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0664 - root_mean_squared_error: 0.2420\n",
      "Epoch 83: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0710 - mae: 0.0664 - root_mean_squared_error: 0.2420 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0650 - root_mean_squared_error: 0.2423\n",
      "Epoch 84: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 242ms/step - loss: 0.0712 - mae: 0.0650 - root_mean_squared_error: 0.2423 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0801 - root_mean_squared_error: 0.2435\n",
      "Epoch 85: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.0717 - mae: 0.0801 - root_mean_squared_error: 0.2435 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0747 - root_mean_squared_error: 0.2434\n",
      "Epoch 86: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0717 - mae: 0.0747 - root_mean_squared_error: 0.2434 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0700 - root_mean_squared_error: 0.2436\n",
      "Epoch 87: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0718 - mae: 0.0700 - root_mean_squared_error: 0.2436 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0710 - root_mean_squared_error: 0.2437\n",
      "Epoch 88: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0718 - mae: 0.0710 - root_mean_squared_error: 0.2437 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0769 - root_mean_squared_error: 0.2419\n",
      "Epoch 89: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.0710 - mae: 0.0769 - root_mean_squared_error: 0.2419 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0798 - root_mean_squared_error: 0.2435\n",
      "Epoch 90: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0718 - mae: 0.0798 - root_mean_squared_error: 0.2435 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0801 - root_mean_squared_error: 0.2424\n",
      "Epoch 91: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0712 - mae: 0.0801 - root_mean_squared_error: 0.2424 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0670 - root_mean_squared_error: 0.2433\n",
      "Epoch 92: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0716 - mae: 0.0670 - root_mean_squared_error: 0.2433 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0821 - root_mean_squared_error: 0.2437\n",
      "Epoch 93: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.0719 - mae: 0.0821 - root_mean_squared_error: 0.2437 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0745 - root_mean_squared_error: 0.2431\n",
      "Epoch 94: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0716 - mae: 0.0745 - root_mean_squared_error: 0.2431 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0695 - root_mean_squared_error: 0.2438\n",
      "Epoch 95: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.0719 - mae: 0.0695 - root_mean_squared_error: 0.2438 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0689 - root_mean_squared_error: 0.2431\n",
      "Epoch 96: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0716 - mae: 0.0689 - root_mean_squared_error: 0.2431 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0824 - root_mean_squared_error: 0.2428\n",
      "Epoch 97: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0714 - mae: 0.0824 - root_mean_squared_error: 0.2428 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0741 - root_mean_squared_error: 0.2426\n",
      "Epoch 98: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0713 - mae: 0.0741 - root_mean_squared_error: 0.2426 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0730 - mae: 0.0535 - root_mean_squared_error: 0.2461\n",
      "Epoch 99: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0730 - mae: 0.0535 - root_mean_squared_error: 0.2461 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0708 - mae: 0.0648 - root_mean_squared_error: 0.2416\n",
      "Epoch 100: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0708 - mae: 0.0648 - root_mean_squared_error: 0.2416 - val_loss: 0.0127 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "idx_tr: [ 0  1  2  3  4  5  6  8  9 10 11 12 13 14 15]\n",
      "idx_va: [7]\n",
      "train_mask: [ True  True  True  True  True  True  True False  True  True  True  True\n",
      "  True  True  True  True]\n",
      "val_mask: [False False False False False False False  True False False False False\n",
      " False False False False]\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0647 - root_mean_squared_error: 0.2443\n",
      "Epoch 1: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 0.0721 - mae: 0.0647 - root_mean_squared_error: 0.2443 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0701 - root_mean_squared_error: 0.2438\n",
      "Epoch 2: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0719 - mae: 0.0701 - root_mean_squared_error: 0.2438 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0741 - mae: 0.1044 - root_mean_squared_error: 0.2483\n",
      "Epoch 3: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0741 - mae: 0.1044 - root_mean_squared_error: 0.2483 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0697 - root_mean_squared_error: 0.2420\n",
      "Epoch 4: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0710 - mae: 0.0697 - root_mean_squared_error: 0.2420 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0853 - root_mean_squared_error: 0.2440\n",
      "Epoch 5: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0720 - mae: 0.0853 - root_mean_squared_error: 0.2440 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0842 - root_mean_squared_error: 0.2418\n",
      "Epoch 6: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0709 - mae: 0.0842 - root_mean_squared_error: 0.2418 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0709 - root_mean_squared_error: 0.2438\n",
      "Epoch 7: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0719 - mae: 0.0709 - root_mean_squared_error: 0.2438 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0746 - mae: 0.0912 - root_mean_squared_error: 0.2493\n",
      "Epoch 8: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.0746 - mae: 0.0912 - root_mean_squared_error: 0.2493 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0680 - root_mean_squared_error: 0.2419\n",
      "Epoch 9: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0710 - mae: 0.0680 - root_mean_squared_error: 0.2419 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0742 - root_mean_squared_error: 0.2436\n",
      "Epoch 10: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0718 - mae: 0.0742 - root_mean_squared_error: 0.2436 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0707 - mae: 0.0742 - root_mean_squared_error: 0.2413\n",
      "Epoch 11: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 0.0707 - mae: 0.0742 - root_mean_squared_error: 0.2413 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0662 - root_mean_squared_error: 0.2433\n",
      "Epoch 12: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0716 - mae: 0.0662 - root_mean_squared_error: 0.2433 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0723 - mae: 0.0623 - root_mean_squared_error: 0.2446\n",
      "Epoch 13: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 0.0723 - mae: 0.0623 - root_mean_squared_error: 0.2446 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0622 - root_mean_squared_error: 0.2437\n",
      "Epoch 14: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0719 - mae: 0.0622 - root_mean_squared_error: 0.2437 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0819 - root_mean_squared_error: 0.2423\n",
      "Epoch 15: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.0712 - mae: 0.0819 - root_mean_squared_error: 0.2423 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0724 - mae: 0.0619 - root_mean_squared_error: 0.2448\n",
      "Epoch 16: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0724 - mae: 0.0619 - root_mean_squared_error: 0.2448 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0841 - mae: 0.1421 - root_mean_squared_error: 0.2677\n",
      "Epoch 17: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0841 - mae: 0.1421 - root_mean_squared_error: 0.2677 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0668 - root_mean_squared_error: 0.2439\n",
      "Epoch 18: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0719 - mae: 0.0668 - root_mean_squared_error: 0.2439 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0685 - root_mean_squared_error: 0.2421\n",
      "Epoch 19: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0711 - mae: 0.0685 - root_mean_squared_error: 0.2421 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0618 - root_mean_squared_error: 0.2437\n",
      "Epoch 20: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0719 - mae: 0.0618 - root_mean_squared_error: 0.2437 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0732 - root_mean_squared_error: 0.2439\n",
      "Epoch 21: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0720 - mae: 0.0732 - root_mean_squared_error: 0.2439 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0725 - root_mean_squared_error: 0.2437\n",
      "Epoch 22: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0718 - mae: 0.0725 - root_mean_squared_error: 0.2437 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0781 - root_mean_squared_error: 0.2431\n",
      "Epoch 23: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.0715 - mae: 0.0781 - root_mean_squared_error: 0.2431 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0864 - root_mean_squared_error: 0.2442\n",
      "Epoch 24: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0721 - mae: 0.0864 - root_mean_squared_error: 0.2442 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0673 - root_mean_squared_error: 0.2439\n",
      "Epoch 25: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0720 - mae: 0.0673 - root_mean_squared_error: 0.2439 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0708 - mae: 0.0710 - root_mean_squared_error: 0.2416\n",
      "Epoch 26: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0708 - mae: 0.0710 - root_mean_squared_error: 0.2416 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0702 - root_mean_squared_error: 0.2438\n",
      "Epoch 27: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0719 - mae: 0.0702 - root_mean_squared_error: 0.2438 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0722 - root_mean_squared_error: 0.2436\n",
      "Epoch 28: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0718 - mae: 0.0722 - root_mean_squared_error: 0.2436 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0828 - root_mean_squared_error: 0.2425\n",
      "Epoch 29: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0712 - mae: 0.0828 - root_mean_squared_error: 0.2425 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0728 - root_mean_squared_error: 0.2438\n",
      "Epoch 30: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0719 - mae: 0.0728 - root_mean_squared_error: 0.2438 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0685 - root_mean_squared_error: 0.2418\n",
      "Epoch 31: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0709 - mae: 0.0685 - root_mean_squared_error: 0.2418 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0738 - mae: 0.0973 - root_mean_squared_error: 0.2477\n",
      "Epoch 32: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0738 - mae: 0.0973 - root_mean_squared_error: 0.2477 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0773 - root_mean_squared_error: 0.2426\n",
      "Epoch 33: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0713 - mae: 0.0773 - root_mean_squared_error: 0.2426 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0707 - mae: 0.0776 - root_mean_squared_error: 0.2413\n",
      "Epoch 34: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0707 - mae: 0.0776 - root_mean_squared_error: 0.2413 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0855 - root_mean_squared_error: 0.2419\n",
      "Epoch 35: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0710 - mae: 0.0855 - root_mean_squared_error: 0.2419 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0707 - mae: 0.0806 - root_mean_squared_error: 0.2414\n",
      "Epoch 36: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.0707 - mae: 0.0806 - root_mean_squared_error: 0.2414 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0765 - root_mean_squared_error: 0.2424\n",
      "Epoch 37: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0712 - mae: 0.0765 - root_mean_squared_error: 0.2424 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0658 - root_mean_squared_error: 0.2434\n",
      "Epoch 38: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0717 - mae: 0.0658 - root_mean_squared_error: 0.2434 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0736 - root_mean_squared_error: 0.2432\n",
      "Epoch 39: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0716 - mae: 0.0736 - root_mean_squared_error: 0.2432 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0707 - mae: 0.0842 - root_mean_squared_error: 0.2414\n",
      "Epoch 40: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0707 - mae: 0.0842 - root_mean_squared_error: 0.2414 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0731 - root_mean_squared_error: 0.2417\n",
      "Epoch 41: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0709 - mae: 0.0731 - root_mean_squared_error: 0.2417 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0813 - root_mean_squared_error: 0.2425\n",
      "Epoch 42: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0713 - mae: 0.0813 - root_mean_squared_error: 0.2425 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0714 - root_mean_squared_error: 0.2422\n",
      "Epoch 43: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0711 - mae: 0.0714 - root_mean_squared_error: 0.2422 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0643 - root_mean_squared_error: 0.2424\n",
      "Epoch 44: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0712 - mae: 0.0643 - root_mean_squared_error: 0.2424 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0831 - root_mean_squared_error: 0.2431\n",
      "Epoch 45: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0715 - mae: 0.0831 - root_mean_squared_error: 0.2431 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0722 - mae: 0.0867 - root_mean_squared_error: 0.2444\n",
      "Epoch 46: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0722 - mae: 0.0867 - root_mean_squared_error: 0.2444 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0647 - root_mean_squared_error: 0.2428\n",
      "Epoch 47: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0714 - mae: 0.0647 - root_mean_squared_error: 0.2428 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0764 - root_mean_squared_error: 0.2423\n",
      "Epoch 48: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0712 - mae: 0.0764 - root_mean_squared_error: 0.2423 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0716 - root_mean_squared_error: 0.2434\n",
      "Epoch 49: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0717 - mae: 0.0716 - root_mean_squared_error: 0.2434 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0682 - root_mean_squared_error: 0.2432\n",
      "Epoch 50: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0716 - mae: 0.0682 - root_mean_squared_error: 0.2432 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0615 - root_mean_squared_error: 0.2439\n",
      "Epoch 51: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0719 - mae: 0.0615 - root_mean_squared_error: 0.2439 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0725 - root_mean_squared_error: 0.2425\n",
      "Epoch 52: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.0713 - mae: 0.0725 - root_mean_squared_error: 0.2425 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0820 - root_mean_squared_error: 0.2425\n",
      "Epoch 53: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0712 - mae: 0.0820 - root_mean_squared_error: 0.2425 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0723 - mae: 0.0843 - root_mean_squared_error: 0.2447\n",
      "Epoch 54: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0723 - mae: 0.0843 - root_mean_squared_error: 0.2447 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0677 - root_mean_squared_error: 0.2432\n",
      "Epoch 55: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0716 - mae: 0.0677 - root_mean_squared_error: 0.2432 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0708 - mae: 0.0763 - root_mean_squared_error: 0.2415\n",
      "Epoch 56: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0708 - mae: 0.0763 - root_mean_squared_error: 0.2415 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0708 - mae: 0.0700 - root_mean_squared_error: 0.2416\n",
      "Epoch 57: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0708 - mae: 0.0700 - root_mean_squared_error: 0.2416 - val_loss: 0.0141 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0796 - root_mean_squared_error: 0.2435\n",
      "Epoch 58: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0717 - mae: 0.0796 - root_mean_squared_error: 0.2435 - val_loss: 0.0141 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0751 - root_mean_squared_error: 0.2424\n",
      "Epoch 59: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0712 - mae: 0.0751 - root_mean_squared_error: 0.2424 - val_loss: 0.0141 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0629 - root_mean_squared_error: 0.2431\n",
      "Epoch 60: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 0.0715 - mae: 0.0629 - root_mean_squared_error: 0.2431 - val_loss: 0.0141 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0723 - mae: 0.0606 - root_mean_squared_error: 0.2446\n",
      "Epoch 61: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0723 - mae: 0.0606 - root_mean_squared_error: 0.2446 - val_loss: 0.0141 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0799 - root_mean_squared_error: 0.2411\n",
      "Epoch 62: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0706 - mae: 0.0799 - root_mean_squared_error: 0.2411 - val_loss: 0.0141 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0740 - root_mean_squared_error: 0.2434\n",
      "Epoch 63: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0717 - mae: 0.0740 - root_mean_squared_error: 0.2434 - val_loss: 0.0141 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0868 - root_mean_squared_error: 0.2426\n",
      "Epoch 64: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 0.0713 - mae: 0.0868 - root_mean_squared_error: 0.2426 - val_loss: 0.0141 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0692 - root_mean_squared_error: 0.2429\n",
      "Epoch 65: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0715 - mae: 0.0692 - root_mean_squared_error: 0.2429 - val_loss: 0.0141 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0736 - root_mean_squared_error: 0.2417\n",
      "Epoch 66: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0709 - mae: 0.0736 - root_mean_squared_error: 0.2417 - val_loss: 0.0141 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0722 - root_mean_squared_error: 0.2425\n",
      "Epoch 67: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0712 - mae: 0.0722 - root_mean_squared_error: 0.2425 - val_loss: 0.0141 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0889 - root_mean_squared_error: 0.2428\n",
      "Epoch 68: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0714 - mae: 0.0889 - root_mean_squared_error: 0.2428 - val_loss: 0.0141 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0722 - mae: 0.0807 - root_mean_squared_error: 0.2445\n",
      "Epoch 69: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0722 - mae: 0.0807 - root_mean_squared_error: 0.2445 - val_loss: 0.0141 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0722 - root_mean_squared_error: 0.2428\n",
      "Epoch 70: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.0714 - mae: 0.0722 - root_mean_squared_error: 0.2428 - val_loss: 0.0141 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0819 - root_mean_squared_error: 0.2437\n",
      "Epoch 71: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0718 - mae: 0.0819 - root_mean_squared_error: 0.2437 - val_loss: 0.0141 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0724 - mae: 0.0612 - root_mean_squared_error: 0.2450\n",
      "Epoch 72: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0724 - mae: 0.0612 - root_mean_squared_error: 0.2450 - val_loss: 0.0141 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0708 - mae: 0.0780 - root_mean_squared_error: 0.2415\n",
      "Epoch 73: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0708 - mae: 0.0780 - root_mean_squared_error: 0.2415 - val_loss: 0.0141 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0791 - root_mean_squared_error: 0.2425\n",
      "Epoch 74: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0712 - mae: 0.0791 - root_mean_squared_error: 0.2425 - val_loss: 0.0141 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0677 - root_mean_squared_error: 0.2431\n",
      "Epoch 75: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0715 - mae: 0.0677 - root_mean_squared_error: 0.2431 - val_loss: 0.0141 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0787 - root_mean_squared_error: 0.2422\n",
      "Epoch 76: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 0.0711 - mae: 0.0787 - root_mean_squared_error: 0.2422 - val_loss: 0.0141 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0732 - mae: 0.0963 - root_mean_squared_error: 0.2465\n",
      "Epoch 77: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0732 - mae: 0.0963 - root_mean_squared_error: 0.2465 - val_loss: 0.0141 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0747 - mae: 0.0799 - root_mean_squared_error: 0.2495\n",
      "Epoch 78: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0747 - mae: 0.0799 - root_mean_squared_error: 0.2495 - val_loss: 0.0141 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0731 - mae: 0.0970 - root_mean_squared_error: 0.2463\n",
      "Epoch 79: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0731 - mae: 0.0970 - root_mean_squared_error: 0.2463 - val_loss: 0.0141 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0893 - root_mean_squared_error: 0.2443\n",
      "Epoch 80: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.0721 - mae: 0.0893 - root_mean_squared_error: 0.2443 - val_loss: 0.0141 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0707 - mae: 0.0736 - root_mean_squared_error: 0.2415\n",
      "Epoch 81: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.0707 - mae: 0.0736 - root_mean_squared_error: 0.2415 - val_loss: 0.0141 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0727 - mae: 0.0590 - root_mean_squared_error: 0.2455\n",
      "Epoch 82: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0727 - mae: 0.0590 - root_mean_squared_error: 0.2455 - val_loss: 0.0141 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0734 - mae: 0.0518 - root_mean_squared_error: 0.2469\n",
      "Epoch 83: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0734 - mae: 0.0518 - root_mean_squared_error: 0.2469 - val_loss: 0.0141 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0601 - root_mean_squared_error: 0.2441\n",
      "Epoch 84: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0720 - mae: 0.0601 - root_mean_squared_error: 0.2441 - val_loss: 0.0141 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0822 - root_mean_squared_error: 0.2425\n",
      "Epoch 85: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0713 - mae: 0.0822 - root_mean_squared_error: 0.2425 - val_loss: 0.0141 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0712 - root_mean_squared_error: 0.2427\n",
      "Epoch 86: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0713 - mae: 0.0712 - root_mean_squared_error: 0.2427 - val_loss: 0.0141 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0700 - root_mean_squared_error: 0.2417\n",
      "Epoch 87: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0709 - mae: 0.0700 - root_mean_squared_error: 0.2417 - val_loss: 0.0141 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0724 - mae: 0.0872 - root_mean_squared_error: 0.2448\n",
      "Epoch 88: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0724 - mae: 0.0872 - root_mean_squared_error: 0.2448 - val_loss: 0.0141 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0754 - root_mean_squared_error: 0.2422\n",
      "Epoch 89: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0711 - mae: 0.0754 - root_mean_squared_error: 0.2422 - val_loss: 0.0141 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0823 - root_mean_squared_error: 0.2436\n",
      "Epoch 90: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0718 - mae: 0.0823 - root_mean_squared_error: 0.2436 - val_loss: 0.0141 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0863 - root_mean_squared_error: 0.2429\n",
      "Epoch 91: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0715 - mae: 0.0863 - root_mean_squared_error: 0.2429 - val_loss: 0.0141 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0708 - mae: 0.0814 - root_mean_squared_error: 0.2415\n",
      "Epoch 92: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0708 - mae: 0.0814 - root_mean_squared_error: 0.2415 - val_loss: 0.0141 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0707 - mae: 0.0768 - root_mean_squared_error: 0.2415\n",
      "Epoch 93: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0707 - mae: 0.0768 - root_mean_squared_error: 0.2415 - val_loss: 0.0141 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0710 - root_mean_squared_error: 0.2438\n",
      "Epoch 94: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0719 - mae: 0.0710 - root_mean_squared_error: 0.2438 - val_loss: 0.0141 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0816 - root_mean_squared_error: 0.2423\n",
      "Epoch 95: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0712 - mae: 0.0816 - root_mean_squared_error: 0.2423 - val_loss: 0.0141 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0862 - root_mean_squared_error: 0.2430\n",
      "Epoch 96: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0715 - mae: 0.0862 - root_mean_squared_error: 0.2430 - val_loss: 0.0141 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0763 - root_mean_squared_error: 0.2432\n",
      "Epoch 97: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0716 - mae: 0.0763 - root_mean_squared_error: 0.2432 - val_loss: 0.0141 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0694 - root_mean_squared_error: 0.2412\n",
      "Epoch 98: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0706 - mae: 0.0694 - root_mean_squared_error: 0.2412 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0652 - root_mean_squared_error: 0.2423\n",
      "Epoch 99: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0712 - mae: 0.0652 - root_mean_squared_error: 0.2423 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0694 - root_mean_squared_error: 0.2418\n",
      "Epoch 100: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0709 - mae: 0.0694 - root_mean_squared_error: 0.2418 - val_loss: 0.0141 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "idx_tr: [ 0  1  2  3  4  5  6  7  9 10 11 12 13 14 15]\n",
      "idx_va: [8]\n",
      "train_mask: [ True  True  True  True  True  True  True  True False  True  True  True\n",
      "  True  True  True  True]\n",
      "val_mask: [False False False False False False False False  True False False False\n",
      " False False False False]\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0708 - mae: 0.0765 - root_mean_squared_error: 0.2415\n",
      "Epoch 1: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0708 - mae: 0.0765 - root_mean_squared_error: 0.2415 - val_loss: 0.0138 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0814 - root_mean_squared_error: 0.2426\n",
      "Epoch 2: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0713 - mae: 0.0814 - root_mean_squared_error: 0.2426 - val_loss: 0.0138 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0794 - root_mean_squared_error: 0.2434\n",
      "Epoch 3: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0717 - mae: 0.0794 - root_mean_squared_error: 0.2434 - val_loss: 0.0138 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0725 - mae: 0.0562 - root_mean_squared_error: 0.2451\n",
      "Epoch 4: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0725 - mae: 0.0562 - root_mean_squared_error: 0.2451 - val_loss: 0.0138 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0723 - mae: 0.0576 - root_mean_squared_error: 0.2446\n",
      "Epoch 5: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0723 - mae: 0.0576 - root_mean_squared_error: 0.2446 - val_loss: 0.0138 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0837 - root_mean_squared_error: 0.2443\n",
      "Epoch 6: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0721 - mae: 0.0837 - root_mean_squared_error: 0.2443 - val_loss: 0.0138 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0837 - root_mean_squared_error: 0.2424\n",
      "Epoch 7: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0712 - mae: 0.0837 - root_mean_squared_error: 0.2424 - val_loss: 0.0138 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0699 - root_mean_squared_error: 0.2429\n",
      "Epoch 8: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.0714 - mae: 0.0699 - root_mean_squared_error: 0.2429 - val_loss: 0.0138 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0718 - root_mean_squared_error: 0.2434\n",
      "Epoch 9: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0717 - mae: 0.0718 - root_mean_squared_error: 0.2434 - val_loss: 0.0138 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0726 - mae: 0.0590 - root_mean_squared_error: 0.2452\n",
      "Epoch 10: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0726 - mae: 0.0590 - root_mean_squared_error: 0.2452 - val_loss: 0.0138 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0683 - root_mean_squared_error: 0.2439\n",
      "Epoch 11: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0719 - mae: 0.0683 - root_mean_squared_error: 0.2439 - val_loss: 0.0138 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0734 - mae: 0.0715 - root_mean_squared_error: 0.2470\n",
      "Epoch 12: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0734 - mae: 0.0715 - root_mean_squared_error: 0.2470 - val_loss: 0.0138 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0715 - root_mean_squared_error: 0.2438\n",
      "Epoch 13: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0719 - mae: 0.0715 - root_mean_squared_error: 0.2438 - val_loss: 0.0138 - val_mae: 0.0720 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0690 - root_mean_squared_error: 0.2427\n",
      "Epoch 14: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0713 - mae: 0.0690 - root_mean_squared_error: 0.2427 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0673 - root_mean_squared_error: 0.2431\n",
      "Epoch 15: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0715 - mae: 0.0673 - root_mean_squared_error: 0.2431 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0751 - mae: 0.0837 - root_mean_squared_error: 0.2504\n",
      "Epoch 16: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0751 - mae: 0.0837 - root_mean_squared_error: 0.2504 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0710 - root_mean_squared_error: 0.2427\n",
      "Epoch 17: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0713 - mae: 0.0710 - root_mean_squared_error: 0.2427 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0741 - root_mean_squared_error: 0.2426\n",
      "Epoch 18: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0713 - mae: 0.0741 - root_mean_squared_error: 0.2426 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0581 - root_mean_squared_error: 0.2437\n",
      "Epoch 19: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0718 - mae: 0.0581 - root_mean_squared_error: 0.2437 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0760 - root_mean_squared_error: 0.2437\n",
      "Epoch 20: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0718 - mae: 0.0760 - root_mean_squared_error: 0.2437 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0723 - mae: 0.0695 - root_mean_squared_error: 0.2448\n",
      "Epoch 21: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0723 - mae: 0.0695 - root_mean_squared_error: 0.2448 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0723 - mae: 0.0917 - root_mean_squared_error: 0.2446\n",
      "Epoch 22: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0723 - mae: 0.0917 - root_mean_squared_error: 0.2446 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0720 - root_mean_squared_error: 0.2431\n",
      "Epoch 23: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0715 - mae: 0.0720 - root_mean_squared_error: 0.2431 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0760 - root_mean_squared_error: 0.2421\n",
      "Epoch 24: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0711 - mae: 0.0760 - root_mean_squared_error: 0.2421 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0707 - mae: 0.0775 - root_mean_squared_error: 0.2414\n",
      "Epoch 25: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.0707 - mae: 0.0775 - root_mean_squared_error: 0.2414 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0684 - root_mean_squared_error: 0.2440\n",
      "Epoch 26: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0720 - mae: 0.0684 - root_mean_squared_error: 0.2440 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0625 - root_mean_squared_error: 0.2437\n",
      "Epoch 27: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0718 - mae: 0.0625 - root_mean_squared_error: 0.2437 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0769 - root_mean_squared_error: 0.2429\n",
      "Epoch 28: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0714 - mae: 0.0769 - root_mean_squared_error: 0.2429 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0743 - mae: 0.0909 - root_mean_squared_error: 0.2487\n",
      "Epoch 29: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0743 - mae: 0.0909 - root_mean_squared_error: 0.2487 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0704 - mae: 0.0710 - root_mean_squared_error: 0.2409\n",
      "Epoch 30: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0704 - mae: 0.0710 - root_mean_squared_error: 0.2409 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0768 - root_mean_squared_error: 0.2434\n",
      "Epoch 31: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0717 - mae: 0.0768 - root_mean_squared_error: 0.2434 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0773 - mae: 0.1173 - root_mean_squared_error: 0.2547\n",
      "Epoch 32: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0773 - mae: 0.1173 - root_mean_squared_error: 0.2547 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0727 - root_mean_squared_error: 0.2425\n",
      "Epoch 33: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0712 - mae: 0.0727 - root_mean_squared_error: 0.2425 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0728 - mae: 0.0850 - root_mean_squared_error: 0.2457\n",
      "Epoch 34: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0728 - mae: 0.0850 - root_mean_squared_error: 0.2457 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0709 - root_mean_squared_error: 0.2426\n",
      "Epoch 35: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0713 - mae: 0.0709 - root_mean_squared_error: 0.2426 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0850 - root_mean_squared_error: 0.2427\n",
      "Epoch 36: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0713 - mae: 0.0850 - root_mean_squared_error: 0.2427 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0700 - root_mean_squared_error: 0.2424\n",
      "Epoch 37: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0712 - mae: 0.0700 - root_mean_squared_error: 0.2424 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0729 - mae: 0.0840 - root_mean_squared_error: 0.2460\n",
      "Epoch 38: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0729 - mae: 0.0840 - root_mean_squared_error: 0.2460 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0762 - mae: 0.1111 - root_mean_squared_error: 0.2525\n",
      "Epoch 39: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.0762 - mae: 0.1111 - root_mean_squared_error: 0.2525 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0687 - root_mean_squared_error: 0.2428\n",
      "Epoch 40: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0714 - mae: 0.0687 - root_mean_squared_error: 0.2428 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0769 - root_mean_squared_error: 0.2438\n",
      "Epoch 41: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0719 - mae: 0.0769 - root_mean_squared_error: 0.2438 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0653 - root_mean_squared_error: 0.2432\n",
      "Epoch 42: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0716 - mae: 0.0653 - root_mean_squared_error: 0.2432 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0800 - root_mean_squared_error: 0.2436\n",
      "Epoch 43: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0718 - mae: 0.0800 - root_mean_squared_error: 0.2436 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0747 - root_mean_squared_error: 0.2412\n",
      "Epoch 44: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0706 - mae: 0.0747 - root_mean_squared_error: 0.2412 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0758 - root_mean_squared_error: 0.2436\n",
      "Epoch 45: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0718 - mae: 0.0758 - root_mean_squared_error: 0.2436 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0730 - mae: 0.0867 - root_mean_squared_error: 0.2461\n",
      "Epoch 46: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0730 - mae: 0.0867 - root_mean_squared_error: 0.2461 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0707 - mae: 0.0796 - root_mean_squared_error: 0.2415\n",
      "Epoch 47: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0707 - mae: 0.0796 - root_mean_squared_error: 0.2415 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0707 - root_mean_squared_error: 0.2438\n",
      "Epoch 48: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0718 - mae: 0.0707 - root_mean_squared_error: 0.2438 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0736 - root_mean_squared_error: 0.2436\n",
      "Epoch 49: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0718 - mae: 0.0736 - root_mean_squared_error: 0.2436 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0708 - mae: 0.0773 - root_mean_squared_error: 0.2415\n",
      "Epoch 50: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0708 - mae: 0.0773 - root_mean_squared_error: 0.2415 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0750 - root_mean_squared_error: 0.2436\n",
      "Epoch 51: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0718 - mae: 0.0750 - root_mean_squared_error: 0.2436 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0734 - mae: 0.0928 - root_mean_squared_error: 0.2470\n",
      "Epoch 52: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0734 - mae: 0.0928 - root_mean_squared_error: 0.2470 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0748 - root_mean_squared_error: 0.2424\n",
      "Epoch 53: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0712 - mae: 0.0748 - root_mean_squared_error: 0.2424 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0713 - root_mean_squared_error: 0.2433\n",
      "Epoch 54: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0716 - mae: 0.0713 - root_mean_squared_error: 0.2433 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0722 - root_mean_squared_error: 0.2423\n",
      "Epoch 55: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0711 - mae: 0.0722 - root_mean_squared_error: 0.2423 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0694 - root_mean_squared_error: 0.2438\n",
      "Epoch 56: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0718 - mae: 0.0694 - root_mean_squared_error: 0.2438 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0742 - root_mean_squared_error: 0.2427\n",
      "Epoch 57: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0713 - mae: 0.0742 - root_mean_squared_error: 0.2427 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0768 - root_mean_squared_error: 0.2431\n",
      "Epoch 58: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0715 - mae: 0.0768 - root_mean_squared_error: 0.2431 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0728 - mae: 0.0946 - root_mean_squared_error: 0.2458\n",
      "Epoch 59: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0728 - mae: 0.0946 - root_mean_squared_error: 0.2458 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0722 - mae: 0.0653 - root_mean_squared_error: 0.2445\n",
      "Epoch 60: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.0722 - mae: 0.0653 - root_mean_squared_error: 0.2445 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0740 - mae: 0.1100 - root_mean_squared_error: 0.2482\n",
      "Epoch 61: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0740 - mae: 0.1100 - root_mean_squared_error: 0.2482 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0722 - mae: 0.0842 - root_mean_squared_error: 0.2445\n",
      "Epoch 62: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0722 - mae: 0.0842 - root_mean_squared_error: 0.2445 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0629 - root_mean_squared_error: 0.2428\n",
      "Epoch 63: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0714 - mae: 0.0629 - root_mean_squared_error: 0.2428 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0812 - root_mean_squared_error: 0.2443\n",
      "Epoch 64: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.0721 - mae: 0.0812 - root_mean_squared_error: 0.2443 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0837 - root_mean_squared_error: 0.2424\n",
      "Epoch 65: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0712 - mae: 0.0837 - root_mean_squared_error: 0.2424 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0724 - mae: 0.0944 - root_mean_squared_error: 0.2448\n",
      "Epoch 66: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0724 - mae: 0.0944 - root_mean_squared_error: 0.2448 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0763 - mae: 0.1120 - root_mean_squared_error: 0.2527\n",
      "Epoch 67: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0763 - mae: 0.1120 - root_mean_squared_error: 0.2527 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0723 - mae: 0.0871 - root_mean_squared_error: 0.2447\n",
      "Epoch 68: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.0723 - mae: 0.0871 - root_mean_squared_error: 0.2447 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0707 - mae: 0.0758 - root_mean_squared_error: 0.2414\n",
      "Epoch 69: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0707 - mae: 0.0758 - root_mean_squared_error: 0.2414 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0791 - root_mean_squared_error: 0.2428\n",
      "Epoch 70: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0714 - mae: 0.0791 - root_mean_squared_error: 0.2428 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0722 - mae: 0.0543 - root_mean_squared_error: 0.2446\n",
      "Epoch 71: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 0.0722 - mae: 0.0543 - root_mean_squared_error: 0.2446 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0607 - root_mean_squared_error: 0.2441\n",
      "Epoch 72: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0720 - mae: 0.0607 - root_mean_squared_error: 0.2441 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0674 - root_mean_squared_error: 0.2432\n",
      "Epoch 73: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 0.0716 - mae: 0.0674 - root_mean_squared_error: 0.2432 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0900 - root_mean_squared_error: 0.2421\n",
      "Epoch 74: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0710 - mae: 0.0900 - root_mean_squared_error: 0.2421 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0729 - mae: 0.0568 - root_mean_squared_error: 0.2460\n",
      "Epoch 75: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0729 - mae: 0.0568 - root_mean_squared_error: 0.2460 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0655 - root_mean_squared_error: 0.2434\n",
      "Epoch 76: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0716 - mae: 0.0655 - root_mean_squared_error: 0.2434 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0721 - root_mean_squared_error: 0.2423\n",
      "Epoch 77: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0711 - mae: 0.0721 - root_mean_squared_error: 0.2423 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0723 - mae: 0.0822 - root_mean_squared_error: 0.2447\n",
      "Epoch 78: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0723 - mae: 0.0822 - root_mean_squared_error: 0.2447 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0657 - root_mean_squared_error: 0.2432\n",
      "Epoch 79: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0715 - mae: 0.0657 - root_mean_squared_error: 0.2432 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0703 - mae: 0.0710 - root_mean_squared_error: 0.2406\n",
      "Epoch 80: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0703 - mae: 0.0710 - root_mean_squared_error: 0.2406 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0873 - root_mean_squared_error: 0.2428\n",
      "Epoch 81: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0713 - mae: 0.0873 - root_mean_squared_error: 0.2428 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0947 - root_mean_squared_error: 0.2440\n",
      "Epoch 82: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0719 - mae: 0.0947 - root_mean_squared_error: 0.2440 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0819 - root_mean_squared_error: 0.2434\n",
      "Epoch 83: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0717 - mae: 0.0819 - root_mean_squared_error: 0.2434 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0722 - mae: 0.0637 - root_mean_squared_error: 0.2444\n",
      "Epoch 84: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0722 - mae: 0.0637 - root_mean_squared_error: 0.2444 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0632 - root_mean_squared_error: 0.2435\n",
      "Epoch 85: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0717 - mae: 0.0632 - root_mean_squared_error: 0.2435 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0704 - root_mean_squared_error: 0.2431\n",
      "Epoch 86: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0715 - mae: 0.0704 - root_mean_squared_error: 0.2431 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0748 - root_mean_squared_error: 0.2436\n",
      "Epoch 87: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0718 - mae: 0.0748 - root_mean_squared_error: 0.2436 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0669 - root_mean_squared_error: 0.2442\n",
      "Epoch 88: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0720 - mae: 0.0669 - root_mean_squared_error: 0.2442 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0707 - mae: 0.0769 - root_mean_squared_error: 0.2414\n",
      "Epoch 89: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0707 - mae: 0.0769 - root_mean_squared_error: 0.2414 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0636 - root_mean_squared_error: 0.2436\n",
      "Epoch 90: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 0.0717 - mae: 0.0636 - root_mean_squared_error: 0.2436 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0727 - root_mean_squared_error: 0.2423\n",
      "Epoch 91: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0711 - mae: 0.0727 - root_mean_squared_error: 0.2423 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0766 - root_mean_squared_error: 0.2423\n",
      "Epoch 92: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0711 - mae: 0.0766 - root_mean_squared_error: 0.2423 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0725 - mae: 0.0765 - root_mean_squared_error: 0.2452\n",
      "Epoch 93: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0725 - mae: 0.0765 - root_mean_squared_error: 0.2452 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0817 - root_mean_squared_error: 0.2424\n",
      "Epoch 94: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0712 - mae: 0.0817 - root_mean_squared_error: 0.2424 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0834 - root_mean_squared_error: 0.2419\n",
      "Epoch 95: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0709 - mae: 0.0834 - root_mean_squared_error: 0.2419 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0734 - root_mean_squared_error: 0.2440\n",
      "Epoch 96: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.0719 - mae: 0.0734 - root_mean_squared_error: 0.2440 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0728 - mae: 0.0850 - root_mean_squared_error: 0.2457\n",
      "Epoch 97: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.0728 - mae: 0.0850 - root_mean_squared_error: 0.2457 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0704 - root_mean_squared_error: 0.2437\n",
      "Epoch 98: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0718 - mae: 0.0704 - root_mean_squared_error: 0.2437 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0646 - root_mean_squared_error: 0.2425\n",
      "Epoch 99: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0712 - mae: 0.0646 - root_mean_squared_error: 0.2425 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0838 - root_mean_squared_error: 0.2419\n",
      "Epoch 100: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0709 - mae: 0.0838 - root_mean_squared_error: 0.2419 - val_loss: 0.0138 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "idx_tr: [ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 15]\n",
      "idx_va: [9]\n",
      "train_mask: [ True  True  True  True  True  True  True  True  True False  True  True\n",
      "  True  True  True  True]\n",
      "val_mask: [False False False False False False False False False  True False False\n",
      " False False False False]\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0725 - mae: 0.0588 - root_mean_squared_error: 0.2452\n",
      "Epoch 1: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 0.0725 - mae: 0.0588 - root_mean_squared_error: 0.2452 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0708 - mae: 0.0807 - root_mean_squared_error: 0.2416\n",
      "Epoch 2: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0708 - mae: 0.0807 - root_mean_squared_error: 0.2416 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0857 - root_mean_squared_error: 0.2428\n",
      "Epoch 3: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0713 - mae: 0.0857 - root_mean_squared_error: 0.2428 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0707 - mae: 0.0791 - root_mean_squared_error: 0.2414\n",
      "Epoch 4: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0707 - mae: 0.0791 - root_mean_squared_error: 0.2414 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0723 - mae: 0.0566 - root_mean_squared_error: 0.2447\n",
      "Epoch 5: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0723 - mae: 0.0566 - root_mean_squared_error: 0.2447 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0722 - mae: 0.0576 - root_mean_squared_error: 0.2445\n",
      "Epoch 6: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0722 - mae: 0.0576 - root_mean_squared_error: 0.2445 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0705 - mae: 0.0809 - root_mean_squared_error: 0.2409\n",
      "Epoch 7: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0705 - mae: 0.0809 - root_mean_squared_error: 0.2409 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0799 - root_mean_squared_error: 0.2433\n",
      "Epoch 8: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 0.0716 - mae: 0.0799 - root_mean_squared_error: 0.2433 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0901 - root_mean_squared_error: 0.2433\n",
      "Epoch 9: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0716 - mae: 0.0901 - root_mean_squared_error: 0.2433 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0822 - root_mean_squared_error: 0.2437\n",
      "Epoch 10: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0718 - mae: 0.0822 - root_mean_squared_error: 0.2437 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0708 - root_mean_squared_error: 0.2438\n",
      "Epoch 11: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0718 - mae: 0.0708 - root_mean_squared_error: 0.2438 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0708 - mae: 0.0706 - root_mean_squared_error: 0.2417\n",
      "Epoch 12: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.0708 - mae: 0.0706 - root_mean_squared_error: 0.2417 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0697 - root_mean_squared_error: 0.2433\n",
      "Epoch 13: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0716 - mae: 0.0697 - root_mean_squared_error: 0.2433 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0723 - mae: 0.0574 - root_mean_squared_error: 0.2448\n",
      "Epoch 14: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.0723 - mae: 0.0574 - root_mean_squared_error: 0.2448 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0914 - root_mean_squared_error: 0.2435\n",
      "Epoch 15: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0717 - mae: 0.0914 - root_mean_squared_error: 0.2435 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0803 - root_mean_squared_error: 0.2422\n",
      "Epoch 16: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0711 - mae: 0.0803 - root_mean_squared_error: 0.2422 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0713 - root_mean_squared_error: 0.2433\n",
      "Epoch 17: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0716 - mae: 0.0713 - root_mean_squared_error: 0.2433 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0729 - mae: 0.0840 - root_mean_squared_error: 0.2460\n",
      "Epoch 18: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0729 - mae: 0.0840 - root_mean_squared_error: 0.2460 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0653 - root_mean_squared_error: 0.2430\n",
      "Epoch 19: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0715 - mae: 0.0653 - root_mean_squared_error: 0.2430 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0758 - root_mean_squared_error: 0.2423\n",
      "Epoch 20: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0711 - mae: 0.0758 - root_mean_squared_error: 0.2423 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0652 - root_mean_squared_error: 0.2422\n",
      "Epoch 21: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0711 - mae: 0.0652 - root_mean_squared_error: 0.2422 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0731 - root_mean_squared_error: 0.2427\n",
      "Epoch 22: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0713 - mae: 0.0731 - root_mean_squared_error: 0.2427 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0725 - mae: 0.0944 - root_mean_squared_error: 0.2450\n",
      "Epoch 23: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 242ms/step - loss: 0.0725 - mae: 0.0944 - root_mean_squared_error: 0.2450 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0638 - root_mean_squared_error: 0.2425\n",
      "Epoch 24: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0712 - mae: 0.0638 - root_mean_squared_error: 0.2425 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0587 - root_mean_squared_error: 0.2444\n",
      "Epoch 25: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0721 - mae: 0.0587 - root_mean_squared_error: 0.2444 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0752 - mae: 0.1010 - root_mean_squared_error: 0.2506\n",
      "Epoch 26: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0752 - mae: 0.1010 - root_mean_squared_error: 0.2506 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0721 - root_mean_squared_error: 0.2423\n",
      "Epoch 27: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0711 - mae: 0.0721 - root_mean_squared_error: 0.2423 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0843 - root_mean_squared_error: 0.2426\n",
      "Epoch 28: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0713 - mae: 0.0843 - root_mean_squared_error: 0.2426 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0833 - root_mean_squared_error: 0.2435\n",
      "Epoch 29: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0717 - mae: 0.0833 - root_mean_squared_error: 0.2435 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0671 - root_mean_squared_error: 0.2431\n",
      "Epoch 30: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0715 - mae: 0.0671 - root_mean_squared_error: 0.2431 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0686 - root_mean_squared_error: 0.2420\n",
      "Epoch 31: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0709 - mae: 0.0686 - root_mean_squared_error: 0.2420 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0625 - root_mean_squared_error: 0.2431\n",
      "Epoch 32: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0715 - mae: 0.0625 - root_mean_squared_error: 0.2431 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0702 - root_mean_squared_error: 0.2436\n",
      "Epoch 33: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0717 - mae: 0.0702 - root_mean_squared_error: 0.2436 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0663 - root_mean_squared_error: 0.2442\n",
      "Epoch 34: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 0.0721 - mae: 0.0663 - root_mean_squared_error: 0.2442 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0703 - root_mean_squared_error: 0.2423\n",
      "Epoch 35: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0711 - mae: 0.0703 - root_mean_squared_error: 0.2423 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0802 - root_mean_squared_error: 0.2425\n",
      "Epoch 36: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0712 - mae: 0.0802 - root_mean_squared_error: 0.2425 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0662 - root_mean_squared_error: 0.2432\n",
      "Epoch 37: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0715 - mae: 0.0662 - root_mean_squared_error: 0.2432 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0771 - root_mean_squared_error: 0.2425\n",
      "Epoch 38: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0712 - mae: 0.0771 - root_mean_squared_error: 0.2425 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0730 - root_mean_squared_error: 0.2435\n",
      "Epoch 39: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0717 - mae: 0.0730 - root_mean_squared_error: 0.2435 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0813 - root_mean_squared_error: 0.2427\n",
      "Epoch 40: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0713 - mae: 0.0813 - root_mean_squared_error: 0.2427 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0655 - root_mean_squared_error: 0.2435\n",
      "Epoch 41: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0717 - mae: 0.0655 - root_mean_squared_error: 0.2435 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0702 - root_mean_squared_error: 0.2427\n",
      "Epoch 42: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.0713 - mae: 0.0702 - root_mean_squared_error: 0.2427 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0724 - mae: 0.0568 - root_mean_squared_error: 0.2449\n",
      "Epoch 43: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0724 - mae: 0.0568 - root_mean_squared_error: 0.2449 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0707 - mae: 0.0770 - root_mean_squared_error: 0.2415\n",
      "Epoch 44: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0707 - mae: 0.0770 - root_mean_squared_error: 0.2415 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0619 - root_mean_squared_error: 0.2439\n",
      "Epoch 45: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 0.0719 - mae: 0.0619 - root_mean_squared_error: 0.2439 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0722 - root_mean_squared_error: 0.2437\n",
      "Epoch 46: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0718 - mae: 0.0722 - root_mean_squared_error: 0.2437 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0803 - root_mean_squared_error: 0.2425\n",
      "Epoch 47: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0712 - mae: 0.0803 - root_mean_squared_error: 0.2425 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0895 - root_mean_squared_error: 0.2422\n",
      "Epoch 48: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0711 - mae: 0.0895 - root_mean_squared_error: 0.2422 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0869 - root_mean_squared_error: 0.2443\n",
      "Epoch 49: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0721 - mae: 0.0869 - root_mean_squared_error: 0.2443 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0734 - mae: 0.0548 - root_mean_squared_error: 0.2470\n",
      "Epoch 50: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0734 - mae: 0.0548 - root_mean_squared_error: 0.2470 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0711 - root_mean_squared_error: 0.2427\n",
      "Epoch 51: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0713 - mae: 0.0711 - root_mean_squared_error: 0.2427 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0758 - root_mean_squared_error: 0.2426\n",
      "Epoch 52: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0712 - mae: 0.0758 - root_mean_squared_error: 0.2426 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0875 - root_mean_squared_error: 0.2419\n",
      "Epoch 53: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0709 - mae: 0.0875 - root_mean_squared_error: 0.2419 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0854 - root_mean_squared_error: 0.2425\n",
      "Epoch 54: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0712 - mae: 0.0854 - root_mean_squared_error: 0.2425 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0786 - root_mean_squared_error: 0.2426\n",
      "Epoch 55: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0713 - mae: 0.0786 - root_mean_squared_error: 0.2426 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0723 - mae: 0.0592 - root_mean_squared_error: 0.2447\n",
      "Epoch 56: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0723 - mae: 0.0592 - root_mean_squared_error: 0.2447 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0712 - root_mean_squared_error: 0.2422\n",
      "Epoch 57: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0711 - mae: 0.0712 - root_mean_squared_error: 0.2422 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0728 - mae: 0.0575 - root_mean_squared_error: 0.2458\n",
      "Epoch 58: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.0728 - mae: 0.0575 - root_mean_squared_error: 0.2458 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0706 - root_mean_squared_error: 0.2428\n",
      "Epoch 59: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.0713 - mae: 0.0706 - root_mean_squared_error: 0.2428 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0679 - root_mean_squared_error: 0.2444\n",
      "Epoch 60: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0721 - mae: 0.0679 - root_mean_squared_error: 0.2444 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0688 - root_mean_squared_error: 0.2429\n",
      "Epoch 61: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0714 - mae: 0.0688 - root_mean_squared_error: 0.2429 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0716 - root_mean_squared_error: 0.2426\n",
      "Epoch 62: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0713 - mae: 0.0716 - root_mean_squared_error: 0.2426 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0717 - root_mean_squared_error: 0.2439\n",
      "Epoch 63: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0719 - mae: 0.0717 - root_mean_squared_error: 0.2439 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0732 - mae: 0.0506 - root_mean_squared_error: 0.2466\n",
      "Epoch 64: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0732 - mae: 0.0506 - root_mean_squared_error: 0.2466 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0834 - root_mean_squared_error: 0.2433\n",
      "Epoch 65: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0716 - mae: 0.0834 - root_mean_squared_error: 0.2433 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0853 - root_mean_squared_error: 0.2444\n",
      "Epoch 66: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.0721 - mae: 0.0853 - root_mean_squared_error: 0.2444 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0734 - root_mean_squared_error: 0.2427\n",
      "Epoch 67: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0713 - mae: 0.0734 - root_mean_squared_error: 0.2427 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0735 - mae: 0.0964 - root_mean_squared_error: 0.2473\n",
      "Epoch 68: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0735 - mae: 0.0964 - root_mean_squared_error: 0.2473 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0708 - mae: 0.0718 - root_mean_squared_error: 0.2416\n",
      "Epoch 69: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0708 - mae: 0.0718 - root_mean_squared_error: 0.2416 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0768 - mae: 0.1194 - root_mean_squared_error: 0.2537\n",
      "Epoch 70: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.0768 - mae: 0.1194 - root_mean_squared_error: 0.2537 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0795 - root_mean_squared_error: 0.2434\n",
      "Epoch 71: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0717 - mae: 0.0795 - root_mean_squared_error: 0.2434 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0786 - root_mean_squared_error: 0.2435\n",
      "Epoch 72: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0717 - mae: 0.0786 - root_mean_squared_error: 0.2435 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0708 - mae: 0.0785 - root_mean_squared_error: 0.2416\n",
      "Epoch 73: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0708 - mae: 0.0785 - root_mean_squared_error: 0.2416 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0739 - root_mean_squared_error: 0.2423\n",
      "Epoch 74: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0711 - mae: 0.0739 - root_mean_squared_error: 0.2423 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0839 - root_mean_squared_error: 0.2438\n",
      "Epoch 75: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0718 - mae: 0.0839 - root_mean_squared_error: 0.2438 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0684 - root_mean_squared_error: 0.2437\n",
      "Epoch 76: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 0.0718 - mae: 0.0684 - root_mean_squared_error: 0.2437 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0786 - root_mean_squared_error: 0.2425\n",
      "Epoch 77: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0712 - mae: 0.0786 - root_mean_squared_error: 0.2425 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0818 - root_mean_squared_error: 0.2419\n",
      "Epoch 78: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0709 - mae: 0.0818 - root_mean_squared_error: 0.2419 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0766 - mae: 0.1188 - root_mean_squared_error: 0.2533\n",
      "Epoch 79: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.0766 - mae: 0.1188 - root_mean_squared_error: 0.2533 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0707 - mae: 0.0729 - root_mean_squared_error: 0.2415\n",
      "Epoch 80: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0707 - mae: 0.0729 - root_mean_squared_error: 0.2415 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0754 - root_mean_squared_error: 0.2427\n",
      "Epoch 81: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0713 - mae: 0.0754 - root_mean_squared_error: 0.2427 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0769 - root_mean_squared_error: 0.2413\n",
      "Epoch 82: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0706 - mae: 0.0769 - root_mean_squared_error: 0.2413 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0704 - mae: 0.0793 - root_mean_squared_error: 0.2408\n",
      "Epoch 83: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.0704 - mae: 0.0793 - root_mean_squared_error: 0.2408 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0597 - root_mean_squared_error: 0.2443\n",
      "Epoch 84: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0721 - mae: 0.0597 - root_mean_squared_error: 0.2443 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0610 - root_mean_squared_error: 0.2441\n",
      "Epoch 85: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0720 - mae: 0.0610 - root_mean_squared_error: 0.2441 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0718 - root_mean_squared_error: 0.2439\n",
      "Epoch 86: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0719 - mae: 0.0718 - root_mean_squared_error: 0.2439 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0633 - root_mean_squared_error: 0.2436\n",
      "Epoch 87: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0717 - mae: 0.0633 - root_mean_squared_error: 0.2436 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0735 - root_mean_squared_error: 0.2423\n",
      "Epoch 88: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0711 - mae: 0.0735 - root_mean_squared_error: 0.2423 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0707 - mae: 0.0718 - root_mean_squared_error: 0.2415\n",
      "Epoch 89: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0707 - mae: 0.0718 - root_mean_squared_error: 0.2415 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0714 - root_mean_squared_error: 0.2438\n",
      "Epoch 90: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0718 - mae: 0.0714 - root_mean_squared_error: 0.2438 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0743 - root_mean_squared_error: 0.2424\n",
      "Epoch 91: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0711 - mae: 0.0743 - root_mean_squared_error: 0.2424 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0641 - root_mean_squared_error: 0.2434\n",
      "Epoch 92: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0716 - mae: 0.0641 - root_mean_squared_error: 0.2434 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0681 - root_mean_squared_error: 0.2442\n",
      "Epoch 93: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0720 - mae: 0.0681 - root_mean_squared_error: 0.2442 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0831 - root_mean_squared_error: 0.2419\n",
      "Epoch 94: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0709 - mae: 0.0831 - root_mean_squared_error: 0.2419 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0702 - root_mean_squared_error: 0.2441\n",
      "Epoch 95: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0720 - mae: 0.0702 - root_mean_squared_error: 0.2441 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0763 - root_mean_squared_error: 0.2426\n",
      "Epoch 96: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0712 - mae: 0.0763 - root_mean_squared_error: 0.2426 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0871 - root_mean_squared_error: 0.2429\n",
      "Epoch 97: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0714 - mae: 0.0871 - root_mean_squared_error: 0.2429 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0734 - root_mean_squared_error: 0.2424\n",
      "Epoch 98: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0712 - mae: 0.0734 - root_mean_squared_error: 0.2424 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0603 - root_mean_squared_error: 0.2432\n",
      "Epoch 99: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0715 - mae: 0.0603 - root_mean_squared_error: 0.2432 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0802 - root_mean_squared_error: 0.2425\n",
      "Epoch 100: val_loss did not improve from 0.01267\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0712 - mae: 0.0802 - root_mean_squared_error: 0.2425 - val_loss: 0.0146 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "idx_tr: [ 0  1  2  3  4  5  6  7  8  9 11 12 13 14 15]\n",
      "idx_va: [10]\n",
      "train_mask: [ True  True  True  True  True  True  True  True  True  True False  True\n",
      "  True  True  True  True]\n",
      "val_mask: [False False False False False False False False False False  True False\n",
      " False False False False]\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0904 - root_mean_squared_error: 0.2443\n",
      "Epoch 1: val_loss improved from 0.01267 to 0.01258, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 0.0721 - mae: 0.0904 - root_mean_squared_error: 0.2443 - val_loss: 0.0126 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0692 - root_mean_squared_error: 0.2419\n",
      "Epoch 2: val_loss improved from 0.01258 to 0.01258, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 0.0709 - mae: 0.0692 - root_mean_squared_error: 0.2419 - val_loss: 0.0126 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0778 - mae: 0.1278 - root_mean_squared_error: 0.2558\n",
      "Epoch 3: val_loss improved from 0.01258 to 0.01258, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.0778 - mae: 0.1278 - root_mean_squared_error: 0.2558 - val_loss: 0.0126 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0615 - root_mean_squared_error: 0.2439\n",
      "Epoch 4: val_loss improved from 0.01258 to 0.01258, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0718 - mae: 0.0615 - root_mean_squared_error: 0.2439 - val_loss: 0.0126 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0723 - mae: 0.0876 - root_mean_squared_error: 0.2448\n",
      "Epoch 5: val_loss improved from 0.01258 to 0.01258, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 0.0723 - mae: 0.0876 - root_mean_squared_error: 0.2448 - val_loss: 0.0126 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0612 - root_mean_squared_error: 0.2439\n",
      "Epoch 6: val_loss improved from 0.01258 to 0.01258, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0719 - mae: 0.0612 - root_mean_squared_error: 0.2439 - val_loss: 0.0126 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0814 - root_mean_squared_error: 0.2424\n",
      "Epoch 7: val_loss improved from 0.01258 to 0.01258, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0711 - mae: 0.0814 - root_mean_squared_error: 0.2424 - val_loss: 0.0126 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0691 - root_mean_squared_error: 0.2429\n",
      "Epoch 8: val_loss improved from 0.01258 to 0.01258, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 363ms/step - loss: 0.0714 - mae: 0.0691 - root_mean_squared_error: 0.2429 - val_loss: 0.0126 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0681 - root_mean_squared_error: 0.2442\n",
      "Epoch 9: val_loss improved from 0.01258 to 0.01258, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.0720 - mae: 0.0681 - root_mean_squared_error: 0.2442 - val_loss: 0.0126 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0610 - root_mean_squared_error: 0.2441\n",
      "Epoch 10: val_loss improved from 0.01258 to 0.01258, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0720 - mae: 0.0610 - root_mean_squared_error: 0.2441 - val_loss: 0.0126 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0790 - root_mean_squared_error: 0.2412\n",
      "Epoch 11: val_loss improved from 0.01258 to 0.01258, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 0.0706 - mae: 0.0790 - root_mean_squared_error: 0.2412 - val_loss: 0.0126 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0885 - root_mean_squared_error: 0.2434\n",
      "Epoch 12: val_loss improved from 0.01258 to 0.01258, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0716 - mae: 0.0885 - root_mean_squared_error: 0.2434 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0738 - root_mean_squared_error: 0.2413\n",
      "Epoch 13: val_loss improved from 0.01258 to 0.01258, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0706 - mae: 0.0738 - root_mean_squared_error: 0.2413 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0705 - mae: 0.0795 - root_mean_squared_error: 0.2410\n",
      "Epoch 14: val_loss improved from 0.01258 to 0.01258, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 0.0705 - mae: 0.0795 - root_mean_squared_error: 0.2410 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0733 - root_mean_squared_error: 0.2428\n",
      "Epoch 15: val_loss improved from 0.01258 to 0.01258, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0714 - mae: 0.0733 - root_mean_squared_error: 0.2428 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0713 - root_mean_squared_error: 0.2424\n",
      "Epoch 16: val_loss improved from 0.01258 to 0.01258, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.0711 - mae: 0.0713 - root_mean_squared_error: 0.2424 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0766 - root_mean_squared_error: 0.2434\n",
      "Epoch 17: val_loss improved from 0.01258 to 0.01258, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 421ms/step - loss: 0.0716 - mae: 0.0766 - root_mean_squared_error: 0.2434 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0869 - root_mean_squared_error: 0.2433\n",
      "Epoch 18: val_loss improved from 0.01258 to 0.01258, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0716 - mae: 0.0869 - root_mean_squared_error: 0.2433 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0720 - root_mean_squared_error: 0.2426\n",
      "Epoch 19: val_loss improved from 0.01258 to 0.01258, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.0712 - mae: 0.0720 - root_mean_squared_error: 0.2426 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0738 - mae: 0.0876 - root_mean_squared_error: 0.2479\n",
      "Epoch 20: val_loss improved from 0.01258 to 0.01258, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0738 - mae: 0.0876 - root_mean_squared_error: 0.2479 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0715 - root_mean_squared_error: 0.2437\n",
      "Epoch 21: val_loss improved from 0.01258 to 0.01258, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.0718 - mae: 0.0715 - root_mean_squared_error: 0.2437 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0704 - root_mean_squared_error: 0.2444\n",
      "Epoch 22: val_loss improved from 0.01258 to 0.01258, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.0721 - mae: 0.0704 - root_mean_squared_error: 0.2444 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0708 - mae: 0.0700 - root_mean_squared_error: 0.2416\n",
      "Epoch 23: val_loss improved from 0.01258 to 0.01258, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.0708 - mae: 0.0700 - root_mean_squared_error: 0.2416 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0738 - root_mean_squared_error: 0.2413\n",
      "Epoch 24: val_loss improved from 0.01258 to 0.01258, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0706 - mae: 0.0738 - root_mean_squared_error: 0.2413 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0725 - mae: 0.0637 - root_mean_squared_error: 0.2451\n",
      "Epoch 25: val_loss improved from 0.01258 to 0.01257, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.0725 - mae: 0.0637 - root_mean_squared_error: 0.2451 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0767 - root_mean_squared_error: 0.2437\n",
      "Epoch 26: val_loss improved from 0.01257 to 0.01257, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 0.0718 - mae: 0.0767 - root_mean_squared_error: 0.2437 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0729 - mae: 0.0896 - root_mean_squared_error: 0.2459\n",
      "Epoch 27: val_loss improved from 0.01257 to 0.01257, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0729 - mae: 0.0896 - root_mean_squared_error: 0.2459 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0725 - root_mean_squared_error: 0.2428\n",
      "Epoch 28: val_loss improved from 0.01257 to 0.01257, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.0713 - mae: 0.0725 - root_mean_squared_error: 0.2428 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0842 - root_mean_squared_error: 0.2431\n",
      "Epoch 29: val_loss improved from 0.01257 to 0.01257, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0715 - mae: 0.0842 - root_mean_squared_error: 0.2431 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0748 - root_mean_squared_error: 0.2427\n",
      "Epoch 30: val_loss improved from 0.01257 to 0.01257, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0713 - mae: 0.0748 - root_mean_squared_error: 0.2427 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0842 - root_mean_squared_error: 0.2424\n",
      "Epoch 31: val_loss improved from 0.01257 to 0.01257, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0711 - mae: 0.0842 - root_mean_squared_error: 0.2424 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0712 - root_mean_squared_error: 0.2423\n",
      "Epoch 32: val_loss improved from 0.01257 to 0.01257, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0711 - mae: 0.0712 - root_mean_squared_error: 0.2423 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0728 - mae: 0.0952 - root_mean_squared_error: 0.2458\n",
      "Epoch 33: val_loss improved from 0.01257 to 0.01257, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 0.0728 - mae: 0.0952 - root_mean_squared_error: 0.2458 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0644 - root_mean_squared_error: 0.2436\n",
      "Epoch 34: val_loss improved from 0.01257 to 0.01257, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0717 - mae: 0.0644 - root_mean_squared_error: 0.2436 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0727 - mae: 0.0914 - root_mean_squared_error: 0.2456\n",
      "Epoch 35: val_loss improved from 0.01257 to 0.01257, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0727 - mae: 0.0914 - root_mean_squared_error: 0.2456 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0708 - mae: 0.0701 - root_mean_squared_error: 0.2417\n",
      "Epoch 36: val_loss improved from 0.01257 to 0.01257, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 0.0708 - mae: 0.0701 - root_mean_squared_error: 0.2417 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0819 - root_mean_squared_error: 0.2422\n",
      "Epoch 37: val_loss improved from 0.01257 to 0.01257, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0710 - mae: 0.0819 - root_mean_squared_error: 0.2422 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0619 - root_mean_squared_error: 0.2429\n",
      "Epoch 38: val_loss improved from 0.01257 to 0.01257, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.0714 - mae: 0.0619 - root_mean_squared_error: 0.2429 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0760 - root_mean_squared_error: 0.2430\n",
      "Epoch 39: val_loss improved from 0.01257 to 0.01257, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.0714 - mae: 0.0760 - root_mean_squared_error: 0.2430 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0595 - root_mean_squared_error: 0.2441\n",
      "Epoch 40: val_loss improved from 0.01257 to 0.01257, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0719 - mae: 0.0595 - root_mean_squared_error: 0.2441 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0708 - mae: 0.0771 - root_mean_squared_error: 0.2416\n",
      "Epoch 41: val_loss improved from 0.01257 to 0.01257, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.0708 - mae: 0.0771 - root_mean_squared_error: 0.2416 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0810 - mae: 0.1390 - root_mean_squared_error: 0.2620\n",
      "Epoch 42: val_loss improved from 0.01257 to 0.01257, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0810 - mae: 0.1390 - root_mean_squared_error: 0.2620 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0731 - root_mean_squared_error: 0.2424\n",
      "Epoch 43: val_loss improved from 0.01257 to 0.01257, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 0.0711 - mae: 0.0731 - root_mean_squared_error: 0.2424 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0663 - root_mean_squared_error: 0.2430\n",
      "Epoch 44: val_loss improved from 0.01257 to 0.01257, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 0.0714 - mae: 0.0663 - root_mean_squared_error: 0.2430 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0707 - mae: 0.0721 - root_mean_squared_error: 0.2415\n",
      "Epoch 45: val_loss improved from 0.01257 to 0.01257, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0707 - mae: 0.0721 - root_mean_squared_error: 0.2415 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0707 - mae: 0.0782 - root_mean_squared_error: 0.2414\n",
      "Epoch 46: val_loss improved from 0.01257 to 0.01257, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0707 - mae: 0.0782 - root_mean_squared_error: 0.2414 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0685 - root_mean_squared_error: 0.2430\n",
      "Epoch 47: val_loss improved from 0.01257 to 0.01257, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.0714 - mae: 0.0685 - root_mean_squared_error: 0.2430 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0750 - root_mean_squared_error: 0.2426\n",
      "Epoch 48: val_loss improved from 0.01257 to 0.01257, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.0712 - mae: 0.0750 - root_mean_squared_error: 0.2426 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0744 - root_mean_squared_error: 0.2419\n",
      "Epoch 49: val_loss improved from 0.01257 to 0.01257, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0709 - mae: 0.0744 - root_mean_squared_error: 0.2419 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0827 - root_mean_squared_error: 0.2437\n",
      "Epoch 50: val_loss improved from 0.01257 to 0.01257, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0718 - mae: 0.0827 - root_mean_squared_error: 0.2437 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0809 - root_mean_squared_error: 0.2435\n",
      "Epoch 51: val_loss improved from 0.01257 to 0.01257, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.0717 - mae: 0.0809 - root_mean_squared_error: 0.2435 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0686 - root_mean_squared_error: 0.2442\n",
      "Epoch 52: val_loss improved from 0.01257 to 0.01257, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 0.0720 - mae: 0.0686 - root_mean_squared_error: 0.2442 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0869 - root_mean_squared_error: 0.2431\n",
      "Epoch 53: val_loss improved from 0.01257 to 0.01257, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0715 - mae: 0.0869 - root_mean_squared_error: 0.2431 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0708 - mae: 0.0721 - root_mean_squared_error: 0.2416\n",
      "Epoch 54: val_loss improved from 0.01257 to 0.01257, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 0.0708 - mae: 0.0721 - root_mean_squared_error: 0.2416 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0705 - mae: 0.0795 - root_mean_squared_error: 0.2412\n",
      "Epoch 55: val_loss improved from 0.01257 to 0.01257, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 0.0705 - mae: 0.0795 - root_mean_squared_error: 0.2412 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0659 - root_mean_squared_error: 0.2432\n",
      "Epoch 56: val_loss improved from 0.01257 to 0.01257, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0715 - mae: 0.0659 - root_mean_squared_error: 0.2432 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0845 - root_mean_squared_error: 0.2428\n",
      "Epoch 57: val_loss improved from 0.01257 to 0.01257, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.0713 - mae: 0.0845 - root_mean_squared_error: 0.2428 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0692 - root_mean_squared_error: 0.2437\n",
      "Epoch 58: val_loss improved from 0.01257 to 0.01257, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.0718 - mae: 0.0692 - root_mean_squared_error: 0.2437 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0831 - root_mean_squared_error: 0.2421\n",
      "Epoch 59: val_loss improved from 0.01257 to 0.01257, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0710 - mae: 0.0831 - root_mean_squared_error: 0.2421 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0776 - root_mean_squared_error: 0.2422\n",
      "Epoch 60: val_loss improved from 0.01257 to 0.01257, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.0710 - mae: 0.0776 - root_mean_squared_error: 0.2422 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0722 - mae: 0.0970 - root_mean_squared_error: 0.2446\n",
      "Epoch 61: val_loss improved from 0.01257 to 0.01257, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 0.0722 - mae: 0.0970 - root_mean_squared_error: 0.2446 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0768 - root_mean_squared_error: 0.2436\n",
      "Epoch 62: val_loss improved from 0.01257 to 0.01257, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0717 - mae: 0.0768 - root_mean_squared_error: 0.2436 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0708 - mae: 0.0710 - root_mean_squared_error: 0.2417\n",
      "Epoch 63: val_loss improved from 0.01257 to 0.01256, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0708 - mae: 0.0710 - root_mean_squared_error: 0.2417 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0726 - mae: 0.0942 - root_mean_squared_error: 0.2455\n",
      "Epoch 64: val_loss improved from 0.01256 to 0.01256, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.0726 - mae: 0.0942 - root_mean_squared_error: 0.2455 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0729 - root_mean_squared_error: 0.2428\n",
      "Epoch 65: val_loss improved from 0.01256 to 0.01256, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 0.0713 - mae: 0.0729 - root_mean_squared_error: 0.2428 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0799 - root_mean_squared_error: 0.2425\n",
      "Epoch 66: val_loss improved from 0.01256 to 0.01256, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.0712 - mae: 0.0799 - root_mean_squared_error: 0.2425 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0705 - root_mean_squared_error: 0.2437\n",
      "Epoch 67: val_loss improved from 0.01256 to 0.01256, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0718 - mae: 0.0705 - root_mean_squared_error: 0.2437 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0637 - root_mean_squared_error: 0.2428\n",
      "Epoch 68: val_loss improved from 0.01256 to 0.01256, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0713 - mae: 0.0637 - root_mean_squared_error: 0.2428 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0832 - root_mean_squared_error: 0.2435\n",
      "Epoch 69: val_loss improved from 0.01256 to 0.01256, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0717 - mae: 0.0832 - root_mean_squared_error: 0.2435 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0778 - root_mean_squared_error: 0.2422\n",
      "Epoch 70: val_loss improved from 0.01256 to 0.01256, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.0710 - mae: 0.0778 - root_mean_squared_error: 0.2422 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0717 - root_mean_squared_error: 0.2426\n",
      "Epoch 71: val_loss improved from 0.01256 to 0.01256, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0712 - mae: 0.0717 - root_mean_squared_error: 0.2426 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0901 - root_mean_squared_error: 0.2422\n",
      "Epoch 72: val_loss improved from 0.01256 to 0.01256, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.0710 - mae: 0.0901 - root_mean_squared_error: 0.2422 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0585 - root_mean_squared_error: 0.2444\n",
      "Epoch 73: val_loss improved from 0.01256 to 0.01256, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.0721 - mae: 0.0585 - root_mean_squared_error: 0.2444 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0878 - root_mean_squared_error: 0.2424\n",
      "Epoch 74: val_loss improved from 0.01256 to 0.01256, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0711 - mae: 0.0878 - root_mean_squared_error: 0.2424 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0728 - mae: 0.0685 - root_mean_squared_error: 0.2459\n",
      "Epoch 75: val_loss improved from 0.01256 to 0.01256, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0728 - mae: 0.0685 - root_mean_squared_error: 0.2459 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0745 - root_mean_squared_error: 0.2435\n",
      "Epoch 76: val_loss improved from 0.01256 to 0.01256, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0717 - mae: 0.0745 - root_mean_squared_error: 0.2435 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0759 - mae: 0.1014 - root_mean_squared_error: 0.2521\n",
      "Epoch 77: val_loss improved from 0.01256 to 0.01256, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.0759 - mae: 0.1014 - root_mean_squared_error: 0.2521 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0676 - root_mean_squared_error: 0.2441\n",
      "Epoch 78: val_loss improved from 0.01256 to 0.01256, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0719 - mae: 0.0676 - root_mean_squared_error: 0.2441 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0632 - root_mean_squared_error: 0.2432\n",
      "Epoch 79: val_loss improved from 0.01256 to 0.01256, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 0.0715 - mae: 0.0632 - root_mean_squared_error: 0.2432 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0662 - root_mean_squared_error: 0.2423\n",
      "Epoch 80: val_loss improved from 0.01256 to 0.01256, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0711 - mae: 0.0662 - root_mean_squared_error: 0.2423 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0630 - root_mean_squared_error: 0.2443\n",
      "Epoch 81: val_loss improved from 0.01256 to 0.01256, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.0721 - mae: 0.0630 - root_mean_squared_error: 0.2443 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0799 - root_mean_squared_error: 0.2426\n",
      "Epoch 82: val_loss improved from 0.01256 to 0.01256, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0712 - mae: 0.0799 - root_mean_squared_error: 0.2426 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0796 - root_mean_squared_error: 0.2435\n",
      "Epoch 83: val_loss improved from 0.01256 to 0.01256, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 0.0717 - mae: 0.0796 - root_mean_squared_error: 0.2435 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0707 - mae: 0.0718 - root_mean_squared_error: 0.2415\n",
      "Epoch 84: val_loss improved from 0.01256 to 0.01256, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.0707 - mae: 0.0718 - root_mean_squared_error: 0.2415 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0798 - root_mean_squared_error: 0.2425\n",
      "Epoch 85: val_loss improved from 0.01256 to 0.01256, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0712 - mae: 0.0798 - root_mean_squared_error: 0.2425 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0704 - root_mean_squared_error: 0.2427\n",
      "Epoch 86: val_loss improved from 0.01256 to 0.01256, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.0712 - mae: 0.0704 - root_mean_squared_error: 0.2427 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0685 - root_mean_squared_error: 0.2428\n",
      "Epoch 87: val_loss improved from 0.01256 to 0.01256, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.0713 - mae: 0.0685 - root_mean_squared_error: 0.2428 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0639 - root_mean_squared_error: 0.2422\n",
      "Epoch 88: val_loss improved from 0.01256 to 0.01256, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.0710 - mae: 0.0639 - root_mean_squared_error: 0.2422 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0830 - root_mean_squared_error: 0.2438\n",
      "Epoch 89: val_loss improved from 0.01256 to 0.01256, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0718 - mae: 0.0830 - root_mean_squared_error: 0.2438 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0634 - root_mean_squared_error: 0.2423\n",
      "Epoch 90: val_loss improved from 0.01256 to 0.01256, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.0710 - mae: 0.0634 - root_mean_squared_error: 0.2423 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0937 - root_mean_squared_error: 0.2436\n",
      "Epoch 91: val_loss improved from 0.01256 to 0.01256, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0717 - mae: 0.0937 - root_mean_squared_error: 0.2436 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0692 - root_mean_squared_error: 0.2438\n",
      "Epoch 92: val_loss improved from 0.01256 to 0.01256, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 0.0718 - mae: 0.0692 - root_mean_squared_error: 0.2438 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0835 - mae: 0.1410 - root_mean_squared_error: 0.2667\n",
      "Epoch 93: val_loss improved from 0.01256 to 0.01256, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0835 - mae: 0.1410 - root_mean_squared_error: 0.2667 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0708 - mae: 0.0847 - root_mean_squared_error: 0.2417\n",
      "Epoch 94: val_loss improved from 0.01256 to 0.01256, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.0708 - mae: 0.0847 - root_mean_squared_error: 0.2417 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0843 - root_mean_squared_error: 0.2435\n",
      "Epoch 95: val_loss improved from 0.01256 to 0.01256, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0717 - mae: 0.0843 - root_mean_squared_error: 0.2435 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0779 - root_mean_squared_error: 0.2424\n",
      "Epoch 96: val_loss improved from 0.01256 to 0.01256, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.0711 - mae: 0.0779 - root_mean_squared_error: 0.2424 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0600 - root_mean_squared_error: 0.2441\n",
      "Epoch 97: val_loss improved from 0.01256 to 0.01256, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0720 - mae: 0.0600 - root_mean_squared_error: 0.2441 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0762 - root_mean_squared_error: 0.2432\n",
      "Epoch 98: val_loss improved from 0.01256 to 0.01256, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.0715 - mae: 0.0762 - root_mean_squared_error: 0.2432 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0695 - root_mean_squared_error: 0.2427\n",
      "Epoch 99: val_loss improved from 0.01256 to 0.01255, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0713 - mae: 0.0695 - root_mean_squared_error: 0.2427 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0767 - mae: 0.1213 - root_mean_squared_error: 0.2537\n",
      "Epoch 100: val_loss improved from 0.01255 to 0.01255, saving model to Model_Training_24/02/model1_Mar08_12-20-24/ckpts/cp.ckpt\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0767 - mae: 0.1213 - root_mean_squared_error: 0.2537 - val_loss: 0.0126 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "idx_tr: [ 0  1  2  3  4  5  6  7  8  9 10 12 13 14 15]\n",
      "idx_va: [11]\n",
      "train_mask: [ True  True  True  True  True  True  True  True  True  True  True False\n",
      "  True  True  True  True]\n",
      "val_mask: [False False False False False False False False False False False  True\n",
      " False False False False]\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0789 - root_mean_squared_error: 0.2426\n",
      "Epoch 1: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 0.0712 - mae: 0.0789 - root_mean_squared_error: 0.2426 - val_loss: 0.0155 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0656 - root_mean_squared_error: 0.2431\n",
      "Epoch 2: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0715 - mae: 0.0656 - root_mean_squared_error: 0.2431 - val_loss: 0.0155 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0593 - root_mean_squared_error: 0.2445\n",
      "Epoch 3: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.0721 - mae: 0.0593 - root_mean_squared_error: 0.2445 - val_loss: 0.0155 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0723 - mae: 0.0843 - root_mean_squared_error: 0.2448\n",
      "Epoch 4: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0723 - mae: 0.0843 - root_mean_squared_error: 0.2448 - val_loss: 0.0155 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0720 - root_mean_squared_error: 0.2438\n",
      "Epoch 5: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0718 - mae: 0.0720 - root_mean_squared_error: 0.2438 - val_loss: 0.0155 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0759 - root_mean_squared_error: 0.2435\n",
      "Epoch 6: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.0716 - mae: 0.0759 - root_mean_squared_error: 0.2435 - val_loss: 0.0155 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0707 - mae: 0.0790 - root_mean_squared_error: 0.2415\n",
      "Epoch 7: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0707 - mae: 0.0790 - root_mean_squared_error: 0.2415 - val_loss: 0.0155 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0775 - root_mean_squared_error: 0.2424\n",
      "Epoch 8: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0711 - mae: 0.0775 - root_mean_squared_error: 0.2424 - val_loss: 0.0155 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0735 - root_mean_squared_error: 0.2424\n",
      "Epoch 9: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0711 - mae: 0.0735 - root_mean_squared_error: 0.2424 - val_loss: 0.0155 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0707 - mae: 0.0695 - root_mean_squared_error: 0.2415\n",
      "Epoch 10: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0707 - mae: 0.0695 - root_mean_squared_error: 0.2415 - val_loss: 0.0155 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0707 - root_mean_squared_error: 0.2437\n",
      "Epoch 11: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0717 - mae: 0.0707 - root_mean_squared_error: 0.2437 - val_loss: 0.0155 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0831 - mae: 0.1499 - root_mean_squared_error: 0.2659\n",
      "Epoch 12: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0831 - mae: 0.1499 - root_mean_squared_error: 0.2659 - val_loss: 0.0155 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0739 - mae: 0.1112 - root_mean_squared_error: 0.2480\n",
      "Epoch 13: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 0.0739 - mae: 0.1112 - root_mean_squared_error: 0.2480 - val_loss: 0.0155 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0578 - root_mean_squared_error: 0.2444\n",
      "Epoch 14: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0721 - mae: 0.0578 - root_mean_squared_error: 0.2444 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0895 - root_mean_squared_error: 0.2436\n",
      "Epoch 15: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0717 - mae: 0.0895 - root_mean_squared_error: 0.2436 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0723 - mae: 0.0626 - root_mean_squared_error: 0.2448\n",
      "Epoch 16: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0723 - mae: 0.0626 - root_mean_squared_error: 0.2448 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0652 - root_mean_squared_error: 0.2434\n",
      "Epoch 17: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0716 - mae: 0.0652 - root_mean_squared_error: 0.2434 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0724 - mae: 0.0561 - root_mean_squared_error: 0.2450\n",
      "Epoch 18: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0724 - mae: 0.0561 - root_mean_squared_error: 0.2450 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0845 - root_mean_squared_error: 0.2428\n",
      "Epoch 19: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0713 - mae: 0.0845 - root_mean_squared_error: 0.2428 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0725 - mae: 0.0556 - root_mean_squared_error: 0.2452\n",
      "Epoch 20: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0725 - mae: 0.0556 - root_mean_squared_error: 0.2452 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0740 - mae: 0.0908 - root_mean_squared_error: 0.2482\n",
      "Epoch 21: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0740 - mae: 0.0908 - root_mean_squared_error: 0.2482 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0571 - root_mean_squared_error: 0.2437\n",
      "Epoch 22: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0717 - mae: 0.0571 - root_mean_squared_error: 0.2437 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0953 - root_mean_squared_error: 0.2439\n",
      "Epoch 23: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0718 - mae: 0.0953 - root_mean_squared_error: 0.2439 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0779 - root_mean_squared_error: 0.2422\n",
      "Epoch 24: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0710 - mae: 0.0779 - root_mean_squared_error: 0.2422 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0710 - root_mean_squared_error: 0.2427\n",
      "Epoch 25: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0713 - mae: 0.0710 - root_mean_squared_error: 0.2427 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0771 - root_mean_squared_error: 0.2430\n",
      "Epoch 26: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0714 - mae: 0.0771 - root_mean_squared_error: 0.2430 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0809 - root_mean_squared_error: 0.2443\n",
      "Epoch 27: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0720 - mae: 0.0809 - root_mean_squared_error: 0.2443 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0590 - root_mean_squared_error: 0.2443\n",
      "Epoch 28: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0720 - mae: 0.0590 - root_mean_squared_error: 0.2443 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0614 - root_mean_squared_error: 0.2438\n",
      "Epoch 29: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0718 - mae: 0.0614 - root_mean_squared_error: 0.2438 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0786 - root_mean_squared_error: 0.2422\n",
      "Epoch 30: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0710 - mae: 0.0786 - root_mean_squared_error: 0.2422 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0672 - root_mean_squared_error: 0.2439\n",
      "Epoch 31: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0719 - mae: 0.0672 - root_mean_squared_error: 0.2439 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0663 - root_mean_squared_error: 0.2435\n",
      "Epoch 32: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0717 - mae: 0.0663 - root_mean_squared_error: 0.2435 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0724 - root_mean_squared_error: 0.2426\n",
      "Epoch 33: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0712 - mae: 0.0724 - root_mean_squared_error: 0.2426 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0859 - root_mean_squared_error: 0.2429\n",
      "Epoch 34: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0713 - mae: 0.0859 - root_mean_squared_error: 0.2429 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0721 - root_mean_squared_error: 0.2427\n",
      "Epoch 35: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0713 - mae: 0.0721 - root_mean_squared_error: 0.2427 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0705 - root_mean_squared_error: 0.2425\n",
      "Epoch 36: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0711 - mae: 0.0705 - root_mean_squared_error: 0.2425 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0702 - mae: 0.0790 - root_mean_squared_error: 0.2406\n",
      "Epoch 37: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0702 - mae: 0.0790 - root_mean_squared_error: 0.2406 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0707 - mae: 0.0832 - root_mean_squared_error: 0.2416\n",
      "Epoch 38: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 0.0707 - mae: 0.0832 - root_mean_squared_error: 0.2416 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0765 - root_mean_squared_error: 0.2425\n",
      "Epoch 39: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0711 - mae: 0.0765 - root_mean_squared_error: 0.2425 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0765 - root_mean_squared_error: 0.2414\n",
      "Epoch 40: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0706 - mae: 0.0765 - root_mean_squared_error: 0.2414 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0743 - root_mean_squared_error: 0.2425\n",
      "Epoch 41: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0712 - mae: 0.0743 - root_mean_squared_error: 0.2425 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0708 - root_mean_squared_error: 0.2440\n",
      "Epoch 42: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0719 - mae: 0.0708 - root_mean_squared_error: 0.2440 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0804 - root_mean_squared_error: 0.2420\n",
      "Epoch 43: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0709 - mae: 0.0804 - root_mean_squared_error: 0.2420 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0662 - root_mean_squared_error: 0.2433\n",
      "Epoch 44: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0716 - mae: 0.0662 - root_mean_squared_error: 0.2433 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0707 - mae: 0.0718 - root_mean_squared_error: 0.2415\n",
      "Epoch 45: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0707 - mae: 0.0718 - root_mean_squared_error: 0.2415 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0730 - root_mean_squared_error: 0.2422\n",
      "Epoch 46: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0710 - mae: 0.0730 - root_mean_squared_error: 0.2422 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0764 - root_mean_squared_error: 0.2424\n",
      "Epoch 47: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 0.0711 - mae: 0.0764 - root_mean_squared_error: 0.2424 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0604 - root_mean_squared_error: 0.2431\n",
      "Epoch 48: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0714 - mae: 0.0604 - root_mean_squared_error: 0.2431 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0788 - root_mean_squared_error: 0.2426\n",
      "Epoch 49: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0712 - mae: 0.0788 - root_mean_squared_error: 0.2426 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0678 - root_mean_squared_error: 0.2441\n",
      "Epoch 50: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0719 - mae: 0.0678 - root_mean_squared_error: 0.2441 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0688 - root_mean_squared_error: 0.2427\n",
      "Epoch 51: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0712 - mae: 0.0688 - root_mean_squared_error: 0.2427 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0787 - root_mean_squared_error: 0.2434\n",
      "Epoch 52: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0716 - mae: 0.0787 - root_mean_squared_error: 0.2434 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0803 - root_mean_squared_error: 0.2424\n",
      "Epoch 53: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0711 - mae: 0.0803 - root_mean_squared_error: 0.2424 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0707 - mae: 0.0734 - root_mean_squared_error: 0.2415\n",
      "Epoch 54: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0707 - mae: 0.0734 - root_mean_squared_error: 0.2415 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0742 - mae: 0.1063 - root_mean_squared_error: 0.2487\n",
      "Epoch 55: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0742 - mae: 0.1063 - root_mean_squared_error: 0.2487 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0722 - mae: 0.0628 - root_mean_squared_error: 0.2446\n",
      "Epoch 56: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0722 - mae: 0.0628 - root_mean_squared_error: 0.2446 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0684 - root_mean_squared_error: 0.2438\n",
      "Epoch 57: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0718 - mae: 0.0684 - root_mean_squared_error: 0.2438 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0799 - root_mean_squared_error: 0.2434\n",
      "Epoch 58: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0716 - mae: 0.0799 - root_mean_squared_error: 0.2434 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0884 - root_mean_squared_error: 0.2427\n",
      "Epoch 59: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0712 - mae: 0.0884 - root_mean_squared_error: 0.2427 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0648 - root_mean_squared_error: 0.2426\n",
      "Epoch 60: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0712 - mae: 0.0648 - root_mean_squared_error: 0.2426 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0834 - root_mean_squared_error: 0.2427\n",
      "Epoch 61: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0713 - mae: 0.0834 - root_mean_squared_error: 0.2427 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0722 - mae: 0.0945 - root_mean_squared_error: 0.2447\n",
      "Epoch 62: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0722 - mae: 0.0945 - root_mean_squared_error: 0.2447 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0684 - root_mean_squared_error: 0.2429\n",
      "Epoch 63: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0713 - mae: 0.0684 - root_mean_squared_error: 0.2429 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0682 - root_mean_squared_error: 0.2437\n",
      "Epoch 64: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0717 - mae: 0.0682 - root_mean_squared_error: 0.2437 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0708 - mae: 0.0751 - root_mean_squared_error: 0.2418\n",
      "Epoch 65: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0708 - mae: 0.0751 - root_mean_squared_error: 0.2418 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0841 - root_mean_squared_error: 0.2436\n",
      "Epoch 66: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0717 - mae: 0.0841 - root_mean_squared_error: 0.2436 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0596 - root_mean_squared_error: 0.2442\n",
      "Epoch 67: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0720 - mae: 0.0596 - root_mean_squared_error: 0.2442 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0811 - root_mean_squared_error: 0.2420\n",
      "Epoch 68: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0709 - mae: 0.0811 - root_mean_squared_error: 0.2420 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0708 - mae: 0.0688 - root_mean_squared_error: 0.2418\n",
      "Epoch 69: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0708 - mae: 0.0688 - root_mean_squared_error: 0.2418 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0707 - mae: 0.0880 - root_mean_squared_error: 0.2417\n",
      "Epoch 70: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0707 - mae: 0.0880 - root_mean_squared_error: 0.2417 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0708 - root_mean_squared_error: 0.2428\n",
      "Epoch 71: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0713 - mae: 0.0708 - root_mean_squared_error: 0.2428 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0781 - root_mean_squared_error: 0.2434\n",
      "Epoch 72: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0716 - mae: 0.0781 - root_mean_squared_error: 0.2434 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0879 - root_mean_squared_error: 0.2428\n",
      "Epoch 73: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 0.0713 - mae: 0.0879 - root_mean_squared_error: 0.2428 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0723 - mae: 0.0828 - root_mean_squared_error: 0.2450\n",
      "Epoch 74: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0723 - mae: 0.0828 - root_mean_squared_error: 0.2450 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0889 - root_mean_squared_error: 0.2429\n",
      "Epoch 75: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0714 - mae: 0.0889 - root_mean_squared_error: 0.2429 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0637 - root_mean_squared_error: 0.2436\n",
      "Epoch 76: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0717 - mae: 0.0637 - root_mean_squared_error: 0.2436 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0705 - mae: 0.0747 - root_mean_squared_error: 0.2412\n",
      "Epoch 77: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0705 - mae: 0.0747 - root_mean_squared_error: 0.2412 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0740 - root_mean_squared_error: 0.2423\n",
      "Epoch 78: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0710 - mae: 0.0740 - root_mean_squared_error: 0.2423 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0707 - mae: 0.0687 - root_mean_squared_error: 0.2416\n",
      "Epoch 79: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0707 - mae: 0.0687 - root_mean_squared_error: 0.2416 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0779 - root_mean_squared_error: 0.2423\n",
      "Epoch 80: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 0.0710 - mae: 0.0779 - root_mean_squared_error: 0.2423 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0594 - root_mean_squared_error: 0.2442\n",
      "Epoch 81: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0720 - mae: 0.0594 - root_mean_squared_error: 0.2442 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0711 - root_mean_squared_error: 0.2435\n",
      "Epoch 82: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0716 - mae: 0.0711 - root_mean_squared_error: 0.2435 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0590 - root_mean_squared_error: 0.2442\n",
      "Epoch 83: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0719 - mae: 0.0590 - root_mean_squared_error: 0.2442 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0814 - root_mean_squared_error: 0.2424\n",
      "Epoch 84: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0711 - mae: 0.0814 - root_mean_squared_error: 0.2424 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0594 - root_mean_squared_error: 0.2444\n",
      "Epoch 85: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0721 - mae: 0.0594 - root_mean_squared_error: 0.2444 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0707 - mae: 0.0714 - root_mean_squared_error: 0.2415\n",
      "Epoch 86: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0707 - mae: 0.0714 - root_mean_squared_error: 0.2415 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0707 - mae: 0.0736 - root_mean_squared_error: 0.2415\n",
      "Epoch 87: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0707 - mae: 0.0736 - root_mean_squared_error: 0.2415 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0748 - root_mean_squared_error: 0.2437\n",
      "Epoch 88: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.0717 - mae: 0.0748 - root_mean_squared_error: 0.2437 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0734 - mae: 0.1006 - root_mean_squared_error: 0.2470\n",
      "Epoch 89: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0734 - mae: 0.1006 - root_mean_squared_error: 0.2470 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0761 - root_mean_squared_error: 0.2426\n",
      "Epoch 90: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0712 - mae: 0.0761 - root_mean_squared_error: 0.2426 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0771 - root_mean_squared_error: 0.2433\n",
      "Epoch 91: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0715 - mae: 0.0771 - root_mean_squared_error: 0.2433 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0633 - root_mean_squared_error: 0.2433\n",
      "Epoch 92: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0715 - mae: 0.0633 - root_mean_squared_error: 0.2433 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0707 - mae: 0.0805 - root_mean_squared_error: 0.2415\n",
      "Epoch 93: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0707 - mae: 0.0805 - root_mean_squared_error: 0.2415 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0751 - root_mean_squared_error: 0.2439\n",
      "Epoch 94: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0718 - mae: 0.0751 - root_mean_squared_error: 0.2439 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0761 - mae: 0.1227 - root_mean_squared_error: 0.2526\n",
      "Epoch 95: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0761 - mae: 0.1227 - root_mean_squared_error: 0.2526 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0802 - root_mean_squared_error: 0.2435\n",
      "Epoch 96: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0716 - mae: 0.0802 - root_mean_squared_error: 0.2435 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0747 - root_mean_squared_error: 0.2426\n",
      "Epoch 97: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0712 - mae: 0.0747 - root_mean_squared_error: 0.2426 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0696 - root_mean_squared_error: 0.2436\n",
      "Epoch 98: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 0.0717 - mae: 0.0696 - root_mean_squared_error: 0.2436 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0762 - root_mean_squared_error: 0.2427\n",
      "Epoch 99: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0712 - mae: 0.0762 - root_mean_squared_error: 0.2427 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0759 - root_mean_squared_error: 0.2426\n",
      "Epoch 100: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0712 - mae: 0.0759 - root_mean_squared_error: 0.2426 - val_loss: 0.0155 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "idx_tr: [ 0  1  2  3  4  5  6  7  8  9 10 11 13 14 15]\n",
      "idx_va: [12]\n",
      "train_mask: [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False  True  True  True]\n",
      "val_mask: [False False False False False False False False False False False False\n",
      "  True False False False]\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0679 - root_mean_squared_error: 0.2429\n",
      "Epoch 1: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.0713 - mae: 0.0679 - root_mean_squared_error: 0.2429 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0627 - root_mean_squared_error: 0.2431\n",
      "Epoch 2: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0714 - mae: 0.0627 - root_mean_squared_error: 0.2431 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0793 - root_mean_squared_error: 0.2437\n",
      "Epoch 3: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0717 - mae: 0.0793 - root_mean_squared_error: 0.2437 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0688 - root_mean_squared_error: 0.2433\n",
      "Epoch 4: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0715 - mae: 0.0688 - root_mean_squared_error: 0.2433 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0867 - root_mean_squared_error: 0.2422\n",
      "Epoch 5: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.0710 - mae: 0.0867 - root_mean_squared_error: 0.2422 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0677 - root_mean_squared_error: 0.2428\n",
      "Epoch 6: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0713 - mae: 0.0677 - root_mean_squared_error: 0.2428 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0754 - mae: 0.1094 - root_mean_squared_error: 0.2512\n",
      "Epoch 7: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0754 - mae: 0.1094 - root_mean_squared_error: 0.2512 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0683 - root_mean_squared_error: 0.2431\n",
      "Epoch 8: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.0714 - mae: 0.0683 - root_mean_squared_error: 0.2431 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0704 - mae: 0.0712 - root_mean_squared_error: 0.2409\n",
      "Epoch 9: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0704 - mae: 0.0712 - root_mean_squared_error: 0.2409 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0688 - root_mean_squared_error: 0.2428\n",
      "Epoch 10: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 0.0713 - mae: 0.0688 - root_mean_squared_error: 0.2428 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0619 - root_mean_squared_error: 0.2428\n",
      "Epoch 11: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0713 - mae: 0.0619 - root_mean_squared_error: 0.2428 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0723 - mae: 0.0978 - root_mean_squared_error: 0.2449\n",
      "Epoch 12: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0723 - mae: 0.0978 - root_mean_squared_error: 0.2449 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0897 - root_mean_squared_error: 0.2442\n",
      "Epoch 13: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0720 - mae: 0.0897 - root_mean_squared_error: 0.2442 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0698 - root_mean_squared_error: 0.2429\n",
      "Epoch 14: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0713 - mae: 0.0698 - root_mean_squared_error: 0.2429 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0718 - root_mean_squared_error: 0.2426\n",
      "Epoch 15: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 0.0712 - mae: 0.0718 - root_mean_squared_error: 0.2426 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0705 - mae: 0.0744 - root_mean_squared_error: 0.2413\n",
      "Epoch 16: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0705 - mae: 0.0744 - root_mean_squared_error: 0.2413 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0709 - root_mean_squared_error: 0.2428\n",
      "Epoch 17: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0713 - mae: 0.0709 - root_mean_squared_error: 0.2428 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0766 - root_mean_squared_error: 0.2414\n",
      "Epoch 18: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0706 - mae: 0.0766 - root_mean_squared_error: 0.2414 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0698 - root_mean_squared_error: 0.2427\n",
      "Epoch 19: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0712 - mae: 0.0698 - root_mean_squared_error: 0.2427 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0817 - root_mean_squared_error: 0.2426\n",
      "Epoch 20: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0712 - mae: 0.0817 - root_mean_squared_error: 0.2426 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0643 - root_mean_squared_error: 0.2435\n",
      "Epoch 21: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0716 - mae: 0.0643 - root_mean_squared_error: 0.2435 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0775 - root_mean_squared_error: 0.2426\n",
      "Epoch 22: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0712 - mae: 0.0775 - root_mean_squared_error: 0.2426 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0761 - root_mean_squared_error: 0.2425\n",
      "Epoch 23: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0711 - mae: 0.0761 - root_mean_squared_error: 0.2425 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0736 - root_mean_squared_error: 0.2426\n",
      "Epoch 24: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0712 - mae: 0.0736 - root_mean_squared_error: 0.2426 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0843 - root_mean_squared_error: 0.2427\n",
      "Epoch 25: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 0.0712 - mae: 0.0843 - root_mean_squared_error: 0.2427 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0806 - root_mean_squared_error: 0.2424\n",
      "Epoch 26: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0711 - mae: 0.0806 - root_mean_squared_error: 0.2424 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0690 - root_mean_squared_error: 0.2435\n",
      "Epoch 27: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0716 - mae: 0.0690 - root_mean_squared_error: 0.2435 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0838 - root_mean_squared_error: 0.2444\n",
      "Epoch 28: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0720 - mae: 0.0838 - root_mean_squared_error: 0.2444 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0768 - root_mean_squared_error: 0.2437\n",
      "Epoch 29: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0717 - mae: 0.0768 - root_mean_squared_error: 0.2437 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0720 - root_mean_squared_error: 0.2425\n",
      "Epoch 30: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0711 - mae: 0.0720 - root_mean_squared_error: 0.2425 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0651 - root_mean_squared_error: 0.2444\n",
      "Epoch 31: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0720 - mae: 0.0651 - root_mean_squared_error: 0.2444 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0765 - root_mean_squared_error: 0.2438\n",
      "Epoch 32: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0718 - mae: 0.0765 - root_mean_squared_error: 0.2438 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0870 - root_mean_squared_error: 0.2431\n",
      "Epoch 33: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0714 - mae: 0.0870 - root_mean_squared_error: 0.2431 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0865 - root_mean_squared_error: 0.2427\n",
      "Epoch 34: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0712 - mae: 0.0865 - root_mean_squared_error: 0.2427 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0729 - root_mean_squared_error: 0.2421\n",
      "Epoch 35: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0709 - mae: 0.0729 - root_mean_squared_error: 0.2421 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0705 - mae: 0.0819 - root_mean_squared_error: 0.2412\n",
      "Epoch 36: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0705 - mae: 0.0819 - root_mean_squared_error: 0.2412 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0724 - mae: 0.0555 - root_mean_squared_error: 0.2452\n",
      "Epoch 37: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 0.0724 - mae: 0.0555 - root_mean_squared_error: 0.2452 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0739 - root_mean_squared_error: 0.2424\n",
      "Epoch 38: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0711 - mae: 0.0739 - root_mean_squared_error: 0.2424 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0678 - root_mean_squared_error: 0.2431\n",
      "Epoch 39: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0714 - mae: 0.0678 - root_mean_squared_error: 0.2431 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0657 - root_mean_squared_error: 0.2441\n",
      "Epoch 40: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0719 - mae: 0.0657 - root_mean_squared_error: 0.2441 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0746 - root_mean_squared_error: 0.2433\n",
      "Epoch 41: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0715 - mae: 0.0746 - root_mean_squared_error: 0.2433 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0825 - root_mean_squared_error: 0.2427\n",
      "Epoch 42: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0712 - mae: 0.0825 - root_mean_squared_error: 0.2427 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0777 - root_mean_squared_error: 0.2423\n",
      "Epoch 43: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0710 - mae: 0.0777 - root_mean_squared_error: 0.2423 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0656 - root_mean_squared_error: 0.2429\n",
      "Epoch 44: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0713 - mae: 0.0656 - root_mean_squared_error: 0.2429 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0648 - root_mean_squared_error: 0.2434\n",
      "Epoch 45: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0716 - mae: 0.0648 - root_mean_squared_error: 0.2434 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0640 - root_mean_squared_error: 0.2434\n",
      "Epoch 46: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0715 - mae: 0.0640 - root_mean_squared_error: 0.2434 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0759 - root_mean_squared_error: 0.2414\n",
      "Epoch 47: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0706 - mae: 0.0759 - root_mean_squared_error: 0.2414 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0709 - root_mean_squared_error: 0.2440\n",
      "Epoch 48: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 0.0719 - mae: 0.0709 - root_mean_squared_error: 0.2440 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0723 - mae: 0.0927 - root_mean_squared_error: 0.2450\n",
      "Epoch 49: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0723 - mae: 0.0927 - root_mean_squared_error: 0.2450 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0691 - root_mean_squared_error: 0.2431\n",
      "Epoch 50: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0714 - mae: 0.0691 - root_mean_squared_error: 0.2431 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0611 - root_mean_squared_error: 0.2441\n",
      "Epoch 51: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0719 - mae: 0.0611 - root_mean_squared_error: 0.2441 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0680 - root_mean_squared_error: 0.2422\n",
      "Epoch 52: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0710 - mae: 0.0680 - root_mean_squared_error: 0.2422 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0728 - mae: 0.0525 - root_mean_squared_error: 0.2460\n",
      "Epoch 53: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0728 - mae: 0.0525 - root_mean_squared_error: 0.2460 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0811 - root_mean_squared_error: 0.2420\n",
      "Epoch 54: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0709 - mae: 0.0811 - root_mean_squared_error: 0.2420 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0728 - root_mean_squared_error: 0.2437\n",
      "Epoch 55: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0717 - mae: 0.0728 - root_mean_squared_error: 0.2437 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2426 - lr: 1.0000e-05\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0690 - root_mean_squared_error: 0.2429\n",
      "Epoch 56: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.0713 - mae: 0.0690 - root_mean_squared_error: 0.2429 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0633 - root_mean_squared_error: 0.2437\n",
      "Epoch 57: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0717 - mae: 0.0633 - root_mean_squared_error: 0.2437 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0681 - root_mean_squared_error: 0.2430\n",
      "Epoch 58: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0714 - mae: 0.0681 - root_mean_squared_error: 0.2430 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0672 - root_mean_squared_error: 0.2430\n",
      "Epoch 59: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0714 - mae: 0.0672 - root_mean_squared_error: 0.2430 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0659 - root_mean_squared_error: 0.2441\n",
      "Epoch 60: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0719 - mae: 0.0659 - root_mean_squared_error: 0.2441 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0632 - root_mean_squared_error: 0.2426\n",
      "Epoch 61: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0712 - mae: 0.0632 - root_mean_squared_error: 0.2426 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0725 - mae: 0.0796 - root_mean_squared_error: 0.2453\n",
      "Epoch 62: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0725 - mae: 0.0796 - root_mean_squared_error: 0.2453 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0664 - root_mean_squared_error: 0.2432\n",
      "Epoch 63: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0714 - mae: 0.0664 - root_mean_squared_error: 0.2432 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0707 - mae: 0.0695 - root_mean_squared_error: 0.2417\n",
      "Epoch 64: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0707 - mae: 0.0695 - root_mean_squared_error: 0.2417 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0678 - root_mean_squared_error: 0.2423\n",
      "Epoch 65: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0710 - mae: 0.0678 - root_mean_squared_error: 0.2423 - val_loss: 0.0139 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0593 - root_mean_squared_error: 0.2444\n",
      "Epoch 66: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0720 - mae: 0.0593 - root_mean_squared_error: 0.2444 - val_loss: 0.0139 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0729 - root_mean_squared_error: 0.2427\n",
      "Epoch 67: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 0.0712 - mae: 0.0729 - root_mean_squared_error: 0.2427 - val_loss: 0.0139 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0667 - root_mean_squared_error: 0.2423\n",
      "Epoch 68: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.0710 - mae: 0.0667 - root_mean_squared_error: 0.2423 - val_loss: 0.0139 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0804 - root_mean_squared_error: 0.2413\n",
      "Epoch 69: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0706 - mae: 0.0804 - root_mean_squared_error: 0.2413 - val_loss: 0.0139 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0725 - mae: 0.0857 - root_mean_squared_error: 0.2453\n",
      "Epoch 70: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0725 - mae: 0.0857 - root_mean_squared_error: 0.2453 - val_loss: 0.0139 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0691 - root_mean_squared_error: 0.2421\n",
      "Epoch 71: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0709 - mae: 0.0691 - root_mean_squared_error: 0.2421 - val_loss: 0.0139 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0866 - root_mean_squared_error: 0.2441\n",
      "Epoch 72: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0719 - mae: 0.0866 - root_mean_squared_error: 0.2441 - val_loss: 0.0139 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0736 - mae: 0.1066 - root_mean_squared_error: 0.2475\n",
      "Epoch 73: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0736 - mae: 0.1066 - root_mean_squared_error: 0.2475 - val_loss: 0.0139 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0687 - root_mean_squared_error: 0.2433\n",
      "Epoch 74: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0715 - mae: 0.0687 - root_mean_squared_error: 0.2433 - val_loss: 0.0139 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0836 - root_mean_squared_error: 0.2428\n",
      "Epoch 75: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0713 - mae: 0.0836 - root_mean_squared_error: 0.2428 - val_loss: 0.0139 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0825 - root_mean_squared_error: 0.2423\n",
      "Epoch 76: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0710 - mae: 0.0825 - root_mean_squared_error: 0.2423 - val_loss: 0.0139 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0773 - root_mean_squared_error: 0.2423\n",
      "Epoch 77: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0710 - mae: 0.0773 - root_mean_squared_error: 0.2423 - val_loss: 0.0139 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0708 - mae: 0.0647 - root_mean_squared_error: 0.2419\n",
      "Epoch 78: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0708 - mae: 0.0647 - root_mean_squared_error: 0.2419 - val_loss: 0.0139 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0707 - mae: 0.0746 - root_mean_squared_error: 0.2417\n",
      "Epoch 79: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0707 - mae: 0.0746 - root_mean_squared_error: 0.2417 - val_loss: 0.0139 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0601 - root_mean_squared_error: 0.2440\n",
      "Epoch 80: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0718 - mae: 0.0601 - root_mean_squared_error: 0.2440 - val_loss: 0.0139 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0748 - root_mean_squared_error: 0.2433\n",
      "Epoch 81: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0715 - mae: 0.0748 - root_mean_squared_error: 0.2433 - val_loss: 0.0139 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0705 - root_mean_squared_error: 0.2436\n",
      "Epoch 82: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 0.0716 - mae: 0.0705 - root_mean_squared_error: 0.2436 - val_loss: 0.0139 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0706 - root_mean_squared_error: 0.2427\n",
      "Epoch 83: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.0712 - mae: 0.0706 - root_mean_squared_error: 0.2427 - val_loss: 0.0139 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0678 - root_mean_squared_error: 0.2430\n",
      "Epoch 84: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0714 - mae: 0.0678 - root_mean_squared_error: 0.2430 - val_loss: 0.0139 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0713 - root_mean_squared_error: 0.2422\n",
      "Epoch 85: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0710 - mae: 0.0713 - root_mean_squared_error: 0.2422 - val_loss: 0.0139 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0754 - root_mean_squared_error: 0.2425\n",
      "Epoch 86: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0711 - mae: 0.0754 - root_mean_squared_error: 0.2425 - val_loss: 0.0139 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0737 - root_mean_squared_error: 0.2426\n",
      "Epoch 87: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0712 - mae: 0.0737 - root_mean_squared_error: 0.2426 - val_loss: 0.0139 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0707 - mae: 0.0650 - root_mean_squared_error: 0.2416\n",
      "Epoch 88: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0707 - mae: 0.0650 - root_mean_squared_error: 0.2416 - val_loss: 0.0139 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0729 - mae: 0.0728 - root_mean_squared_error: 0.2462\n",
      "Epoch 89: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.0729 - mae: 0.0728 - root_mean_squared_error: 0.2462 - val_loss: 0.0139 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0619 - root_mean_squared_error: 0.2422\n",
      "Epoch 90: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0710 - mae: 0.0619 - root_mean_squared_error: 0.2422 - val_loss: 0.0139 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0701 - root_mean_squared_error: 0.2440\n",
      "Epoch 91: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0718 - mae: 0.0701 - root_mean_squared_error: 0.2440 - val_loss: 0.0139 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0837 - root_mean_squared_error: 0.2425\n",
      "Epoch 92: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0711 - mae: 0.0837 - root_mean_squared_error: 0.2425 - val_loss: 0.0139 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0737 - mae: 0.1089 - root_mean_squared_error: 0.2478\n",
      "Epoch 93: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0737 - mae: 0.1089 - root_mean_squared_error: 0.2478 - val_loss: 0.0139 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0748 - mae: 0.0944 - root_mean_squared_error: 0.2500\n",
      "Epoch 94: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0748 - mae: 0.0944 - root_mean_squared_error: 0.2500 - val_loss: 0.0139 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0896 - root_mean_squared_error: 0.2431\n",
      "Epoch 95: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0714 - mae: 0.0896 - root_mean_squared_error: 0.2431 - val_loss: 0.0139 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0734 - mae: 0.0937 - root_mean_squared_error: 0.2472\n",
      "Epoch 96: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0734 - mae: 0.0937 - root_mean_squared_error: 0.2472 - val_loss: 0.0139 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0683 - root_mean_squared_error: 0.2428\n",
      "Epoch 97: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0713 - mae: 0.0683 - root_mean_squared_error: 0.2428 - val_loss: 0.0139 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0847 - mae: 0.1451 - root_mean_squared_error: 0.2690\n",
      "Epoch 98: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.0847 - mae: 0.1451 - root_mean_squared_error: 0.2690 - val_loss: 0.0139 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0754 - root_mean_squared_error: 0.2425\n",
      "Epoch 99: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 0.0711 - mae: 0.0754 - root_mean_squared_error: 0.2425 - val_loss: 0.0139 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0738 - mae: 0.0909 - root_mean_squared_error: 0.2480\n",
      "Epoch 100: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0738 - mae: 0.0909 - root_mean_squared_error: 0.2480 - val_loss: 0.0139 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "idx_tr: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 14 15]\n",
      "idx_va: [13]\n",
      "train_mask: [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True False  True  True]\n",
      "val_mask: [False False False False False False False False False False False False\n",
      " False  True False False]\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0745 - root_mean_squared_error: 0.2437\n",
      "Epoch 1: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.0717 - mae: 0.0745 - root_mean_squared_error: 0.2437 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0722 - root_mean_squared_error: 0.2426\n",
      "Epoch 2: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0711 - mae: 0.0722 - root_mean_squared_error: 0.2426 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0724 - mae: 0.0561 - root_mean_squared_error: 0.2451\n",
      "Epoch 3: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0724 - mae: 0.0561 - root_mean_squared_error: 0.2451 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0805 - root_mean_squared_error: 0.2422\n",
      "Epoch 4: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0709 - mae: 0.0805 - root_mean_squared_error: 0.2422 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0872 - root_mean_squared_error: 0.2429\n",
      "Epoch 5: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0713 - mae: 0.0872 - root_mean_squared_error: 0.2429 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0676 - root_mean_squared_error: 0.2431\n",
      "Epoch 6: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.0714 - mae: 0.0676 - root_mean_squared_error: 0.2431 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0762 - root_mean_squared_error: 0.2435\n",
      "Epoch 7: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0716 - mae: 0.0762 - root_mean_squared_error: 0.2435 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0930 - root_mean_squared_error: 0.2441\n",
      "Epoch 8: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0719 - mae: 0.0930 - root_mean_squared_error: 0.2441 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0626 - root_mean_squared_error: 0.2437\n",
      "Epoch 9: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 0.0717 - mae: 0.0626 - root_mean_squared_error: 0.2437 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0704 - root_mean_squared_error: 0.2437\n",
      "Epoch 10: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0717 - mae: 0.0704 - root_mean_squared_error: 0.2437 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0665 - root_mean_squared_error: 0.2421\n",
      "Epoch 11: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0709 - mae: 0.0665 - root_mean_squared_error: 0.2421 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0759 - root_mean_squared_error: 0.2427\n",
      "Epoch 12: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0712 - mae: 0.0759 - root_mean_squared_error: 0.2427 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0626 - root_mean_squared_error: 0.2446\n",
      "Epoch 13: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0721 - mae: 0.0626 - root_mean_squared_error: 0.2446 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0794 - root_mean_squared_error: 0.2415\n",
      "Epoch 14: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0706 - mae: 0.0794 - root_mean_squared_error: 0.2415 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0708 - mae: 0.0831 - root_mean_squared_error: 0.2418\n",
      "Epoch 15: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0708 - mae: 0.0831 - root_mean_squared_error: 0.2418 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0603 - root_mean_squared_error: 0.2431\n",
      "Epoch 16: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 0.0714 - mae: 0.0603 - root_mean_squared_error: 0.2431 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0723 - mae: 0.0849 - root_mean_squared_error: 0.2449\n",
      "Epoch 17: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0723 - mae: 0.0849 - root_mean_squared_error: 0.2449 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0742 - root_mean_squared_error: 0.2423\n",
      "Epoch 18: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0710 - mae: 0.0742 - root_mean_squared_error: 0.2423 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0818 - root_mean_squared_error: 0.2425\n",
      "Epoch 19: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0711 - mae: 0.0818 - root_mean_squared_error: 0.2425 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0726 - mae: 0.0591 - root_mean_squared_error: 0.2456\n",
      "Epoch 20: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0726 - mae: 0.0591 - root_mean_squared_error: 0.2456 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0735 - root_mean_squared_error: 0.2423\n",
      "Epoch 21: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0710 - mae: 0.0735 - root_mean_squared_error: 0.2423 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0693 - root_mean_squared_error: 0.2428\n",
      "Epoch 22: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0713 - mae: 0.0693 - root_mean_squared_error: 0.2428 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0840 - root_mean_squared_error: 0.2438\n",
      "Epoch 23: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0717 - mae: 0.0840 - root_mean_squared_error: 0.2438 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0680 - root_mean_squared_error: 0.2429\n",
      "Epoch 24: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0713 - mae: 0.0680 - root_mean_squared_error: 0.2429 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0724 - root_mean_squared_error: 0.2433\n",
      "Epoch 25: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0715 - mae: 0.0724 - root_mean_squared_error: 0.2433 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0727 - root_mean_squared_error: 0.2426\n",
      "Epoch 26: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0711 - mae: 0.0727 - root_mean_squared_error: 0.2426 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0767 - root_mean_squared_error: 0.2414\n",
      "Epoch 27: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0706 - mae: 0.0767 - root_mean_squared_error: 0.2414 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0832 - root_mean_squared_error: 0.2424\n",
      "Epoch 28: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0710 - mae: 0.0832 - root_mean_squared_error: 0.2424 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0652 - root_mean_squared_error: 0.2433\n",
      "Epoch 29: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0715 - mae: 0.0652 - root_mean_squared_error: 0.2433 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0724 - mae: 0.0834 - root_mean_squared_error: 0.2451\n",
      "Epoch 30: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0724 - mae: 0.0834 - root_mean_squared_error: 0.2451 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0705 - mae: 0.0797 - root_mean_squared_error: 0.2413\n",
      "Epoch 31: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0705 - mae: 0.0797 - root_mean_squared_error: 0.2413 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0797 - root_mean_squared_error: 0.2437\n",
      "Epoch 32: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0717 - mae: 0.0797 - root_mean_squared_error: 0.2437 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0658 - root_mean_squared_error: 0.2433\n",
      "Epoch 33: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.0715 - mae: 0.0658 - root_mean_squared_error: 0.2433 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0772 - root_mean_squared_error: 0.2426\n",
      "Epoch 34: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0711 - mae: 0.0772 - root_mean_squared_error: 0.2426 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0662 - root_mean_squared_error: 0.2443\n",
      "Epoch 35: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0720 - mae: 0.0662 - root_mean_squared_error: 0.2443 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0722 - mae: 0.0586 - root_mean_squared_error: 0.2448\n",
      "Epoch 36: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0722 - mae: 0.0586 - root_mean_squared_error: 0.2448 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0810 - root_mean_squared_error: 0.2415\n",
      "Epoch 37: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0706 - mae: 0.0810 - root_mean_squared_error: 0.2415 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0833 - root_mean_squared_error: 0.2427\n",
      "Epoch 38: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0712 - mae: 0.0833 - root_mean_squared_error: 0.2427 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0709 - root_mean_squared_error: 0.2427\n",
      "Epoch 39: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0712 - mae: 0.0709 - root_mean_squared_error: 0.2427 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0801 - root_mean_squared_error: 0.2422\n",
      "Epoch 40: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0709 - mae: 0.0801 - root_mean_squared_error: 0.2422 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0731 - mae: 0.0913 - root_mean_squared_error: 0.2467\n",
      "Epoch 41: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0731 - mae: 0.0913 - root_mean_squared_error: 0.2467 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0892 - root_mean_squared_error: 0.2434\n",
      "Epoch 42: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0715 - mae: 0.0892 - root_mean_squared_error: 0.2434 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0885 - root_mean_squared_error: 0.2442\n",
      "Epoch 43: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.0719 - mae: 0.0885 - root_mean_squared_error: 0.2442 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0802 - root_mean_squared_error: 0.2428\n",
      "Epoch 44: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0712 - mae: 0.0802 - root_mean_squared_error: 0.2428 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0675 - root_mean_squared_error: 0.2438\n",
      "Epoch 45: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0717 - mae: 0.0675 - root_mean_squared_error: 0.2438 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0595 - root_mean_squared_error: 0.2432\n",
      "Epoch 46: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0714 - mae: 0.0595 - root_mean_squared_error: 0.2432 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0888 - root_mean_squared_error: 0.2446\n",
      "Epoch 47: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0721 - mae: 0.0888 - root_mean_squared_error: 0.2446 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0777 - root_mean_squared_error: 0.2415\n",
      "Epoch 48: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0706 - mae: 0.0777 - root_mean_squared_error: 0.2415 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0593 - root_mean_squared_error: 0.2434\n",
      "Epoch 49: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0715 - mae: 0.0593 - root_mean_squared_error: 0.2434 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0768 - root_mean_squared_error: 0.2429\n",
      "Epoch 50: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0713 - mae: 0.0768 - root_mean_squared_error: 0.2429 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0825 - mae: 0.1401 - root_mean_squared_error: 0.2650\n",
      "Epoch 51: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0825 - mae: 0.1401 - root_mean_squared_error: 0.2650 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0705 - mae: 0.0695 - root_mean_squared_error: 0.2413\n",
      "Epoch 52: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0705 - mae: 0.0695 - root_mean_squared_error: 0.2413 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0703 - root_mean_squared_error: 0.2421\n",
      "Epoch 53: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0709 - mae: 0.0703 - root_mean_squared_error: 0.2421 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0831 - root_mean_squared_error: 0.2439\n",
      "Epoch 54: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0717 - mae: 0.0831 - root_mean_squared_error: 0.2439 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0720 - root_mean_squared_error: 0.2416\n",
      "Epoch 55: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0706 - mae: 0.0720 - root_mean_squared_error: 0.2416 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0784 - root_mean_squared_error: 0.2427\n",
      "Epoch 56: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0712 - mae: 0.0784 - root_mean_squared_error: 0.2427 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0707 - mae: 0.0787 - root_mean_squared_error: 0.2417\n",
      "Epoch 57: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0707 - mae: 0.0787 - root_mean_squared_error: 0.2417 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0724 - mae: 0.1000 - root_mean_squared_error: 0.2451\n",
      "Epoch 58: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0724 - mae: 0.1000 - root_mean_squared_error: 0.2451 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0749 - root_mean_squared_error: 0.2425\n",
      "Epoch 59: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0711 - mae: 0.0749 - root_mean_squared_error: 0.2425 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0624 - root_mean_squared_error: 0.2446\n",
      "Epoch 60: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0721 - mae: 0.0624 - root_mean_squared_error: 0.2446 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0737 - mae: 0.1015 - root_mean_squared_error: 0.2479\n",
      "Epoch 61: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0737 - mae: 0.1015 - root_mean_squared_error: 0.2479 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0838 - root_mean_squared_error: 0.2439\n",
      "Epoch 62: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.0718 - mae: 0.0838 - root_mean_squared_error: 0.2439 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0705 - mae: 0.0759 - root_mean_squared_error: 0.2412\n",
      "Epoch 63: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0705 - mae: 0.0759 - root_mean_squared_error: 0.2412 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0773 - root_mean_squared_error: 0.2423\n",
      "Epoch 64: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0710 - mae: 0.0773 - root_mean_squared_error: 0.2423 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0708 - mae: 0.0715 - root_mean_squared_error: 0.2418\n",
      "Epoch 65: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0708 - mae: 0.0715 - root_mean_squared_error: 0.2418 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0853 - root_mean_squared_error: 0.2440\n",
      "Epoch 66: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0718 - mae: 0.0853 - root_mean_squared_error: 0.2440 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0753 - root_mean_squared_error: 0.2425\n",
      "Epoch 67: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0711 - mae: 0.0753 - root_mean_squared_error: 0.2425 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0729 - mae: 0.0549 - root_mean_squared_error: 0.2463\n",
      "Epoch 68: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0729 - mae: 0.0549 - root_mean_squared_error: 0.2463 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0788 - root_mean_squared_error: 0.2415\n",
      "Epoch 69: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0706 - mae: 0.0788 - root_mean_squared_error: 0.2415 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0705 - mae: 0.0840 - root_mean_squared_error: 0.2414\n",
      "Epoch 70: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0705 - mae: 0.0840 - root_mean_squared_error: 0.2414 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0694 - root_mean_squared_error: 0.2428\n",
      "Epoch 71: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0712 - mae: 0.0694 - root_mean_squared_error: 0.2428 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0815 - root_mean_squared_error: 0.2435\n",
      "Epoch 72: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0716 - mae: 0.0815 - root_mean_squared_error: 0.2435 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0695 - root_mean_squared_error: 0.2426\n",
      "Epoch 73: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0711 - mae: 0.0695 - root_mean_squared_error: 0.2426 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0654 - root_mean_squared_error: 0.2426\n",
      "Epoch 74: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0711 - mae: 0.0654 - root_mean_squared_error: 0.2426 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0766 - root_mean_squared_error: 0.2425\n",
      "Epoch 75: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0711 - mae: 0.0766 - root_mean_squared_error: 0.2425 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0805 - root_mean_squared_error: 0.2438\n",
      "Epoch 76: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0717 - mae: 0.0805 - root_mean_squared_error: 0.2438 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0785 - root_mean_squared_error: 0.2423\n",
      "Epoch 77: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0710 - mae: 0.0785 - root_mean_squared_error: 0.2423 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0715 - root_mean_squared_error: 0.2444\n",
      "Epoch 78: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0720 - mae: 0.0715 - root_mean_squared_error: 0.2444 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0714 - root_mean_squared_error: 0.2416\n",
      "Epoch 79: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0706 - mae: 0.0714 - root_mean_squared_error: 0.2416 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0896 - root_mean_squared_error: 0.2422\n",
      "Epoch 80: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0710 - mae: 0.0896 - root_mean_squared_error: 0.2422 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0728 - mae: 0.0523 - root_mean_squared_error: 0.2460\n",
      "Epoch 81: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.0728 - mae: 0.0523 - root_mean_squared_error: 0.2460 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0705 - mae: 0.0742 - root_mean_squared_error: 0.2412\n",
      "Epoch 82: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0705 - mae: 0.0742 - root_mean_squared_error: 0.2412 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0865 - root_mean_squared_error: 0.2434\n",
      "Epoch 83: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0715 - mae: 0.0865 - root_mean_squared_error: 0.2434 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0676 - root_mean_squared_error: 0.2426\n",
      "Epoch 84: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0711 - mae: 0.0676 - root_mean_squared_error: 0.2426 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0697 - root_mean_squared_error: 0.2440\n",
      "Epoch 85: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0718 - mae: 0.0697 - root_mean_squared_error: 0.2440 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0643 - root_mean_squared_error: 0.2432\n",
      "Epoch 86: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0714 - mae: 0.0643 - root_mean_squared_error: 0.2432 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0854 - root_mean_squared_error: 0.2432\n",
      "Epoch 87: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0714 - mae: 0.0854 - root_mean_squared_error: 0.2432 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0761 - root_mean_squared_error: 0.2434\n",
      "Epoch 88: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0715 - mae: 0.0761 - root_mean_squared_error: 0.2434 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0681 - root_mean_squared_error: 0.2430\n",
      "Epoch 89: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0713 - mae: 0.0681 - root_mean_squared_error: 0.2430 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0762 - root_mean_squared_error: 0.2436\n",
      "Epoch 90: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0716 - mae: 0.0762 - root_mean_squared_error: 0.2436 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0717 - root_mean_squared_error: 0.2438\n",
      "Epoch 91: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0717 - mae: 0.0717 - root_mean_squared_error: 0.2438 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0695 - root_mean_squared_error: 0.2416\n",
      "Epoch 92: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 0.0706 - mae: 0.0695 - root_mean_squared_error: 0.2416 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0805 - root_mean_squared_error: 0.2424\n",
      "Epoch 93: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0710 - mae: 0.0805 - root_mean_squared_error: 0.2424 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0627 - root_mean_squared_error: 0.2436\n",
      "Epoch 94: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0716 - mae: 0.0627 - root_mean_squared_error: 0.2436 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0839 - root_mean_squared_error: 0.2415\n",
      "Epoch 95: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0706 - mae: 0.0839 - root_mean_squared_error: 0.2415 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0767 - root_mean_squared_error: 0.2426\n",
      "Epoch 96: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 0.0711 - mae: 0.0767 - root_mean_squared_error: 0.2426 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0661 - root_mean_squared_error: 0.2433\n",
      "Epoch 97: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0715 - mae: 0.0661 - root_mean_squared_error: 0.2433 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0810 - root_mean_squared_error: 0.2433\n",
      "Epoch 98: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0715 - mae: 0.0810 - root_mean_squared_error: 0.2433 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0588 - root_mean_squared_error: 0.2444\n",
      "Epoch 99: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0720 - mae: 0.0588 - root_mean_squared_error: 0.2444 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0649 - root_mean_squared_error: 0.2432\n",
      "Epoch 100: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 0.0714 - mae: 0.0649 - root_mean_squared_error: 0.2432 - val_loss: 0.0128 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "idx_tr: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 15]\n",
      "idx_va: [14]\n",
      "train_mask: [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True False  True]\n",
      "val_mask: [False False False False False False False False False False False False\n",
      " False False  True False]\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0728 - mae: 0.0996 - root_mean_squared_error: 0.2461\n",
      "Epoch 1: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.0728 - mae: 0.0996 - root_mean_squared_error: 0.2461 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0757 - root_mean_squared_error: 0.2415\n",
      "Epoch 2: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0706 - mae: 0.0757 - root_mean_squared_error: 0.2415 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0776 - root_mean_squared_error: 0.2415\n",
      "Epoch 3: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0706 - mae: 0.0776 - root_mean_squared_error: 0.2415 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0817 - root_mean_squared_error: 0.2426\n",
      "Epoch 4: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0711 - mae: 0.0817 - root_mean_squared_error: 0.2426 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0681 - root_mean_squared_error: 0.2437\n",
      "Epoch 5: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0717 - mae: 0.0681 - root_mean_squared_error: 0.2437 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0757 - mae: 0.1015 - root_mean_squared_error: 0.2519\n",
      "Epoch 6: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0757 - mae: 0.1015 - root_mean_squared_error: 0.2519 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0722 - mae: 0.0917 - root_mean_squared_error: 0.2448\n",
      "Epoch 7: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0722 - mae: 0.0917 - root_mean_squared_error: 0.2448 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0756 - root_mean_squared_error: 0.2426\n",
      "Epoch 8: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0711 - mae: 0.0756 - root_mean_squared_error: 0.2426 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0740 - root_mean_squared_error: 0.2425\n",
      "Epoch 9: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0711 - mae: 0.0740 - root_mean_squared_error: 0.2425 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0705 - mae: 0.0775 - root_mean_squared_error: 0.2414\n",
      "Epoch 10: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0705 - mae: 0.0775 - root_mean_squared_error: 0.2414 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0666 - root_mean_squared_error: 0.2423\n",
      "Epoch 11: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.0710 - mae: 0.0666 - root_mean_squared_error: 0.2423 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0714 - root_mean_squared_error: 0.2424\n",
      "Epoch 12: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0710 - mae: 0.0714 - root_mean_squared_error: 0.2424 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0850 - root_mean_squared_error: 0.2427\n",
      "Epoch 13: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0712 - mae: 0.0850 - root_mean_squared_error: 0.2427 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0665 - root_mean_squared_error: 0.2430\n",
      "Epoch 14: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0713 - mae: 0.0665 - root_mean_squared_error: 0.2430 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0570 - root_mean_squared_error: 0.2440\n",
      "Epoch 15: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0718 - mae: 0.0570 - root_mean_squared_error: 0.2440 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0754 - mae: 0.1155 - root_mean_squared_error: 0.2513\n",
      "Epoch 16: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0754 - mae: 0.1155 - root_mean_squared_error: 0.2513 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0802 - root_mean_squared_error: 0.2427\n",
      "Epoch 17: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0712 - mae: 0.0802 - root_mean_squared_error: 0.2427 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0758 - root_mean_squared_error: 0.2434\n",
      "Epoch 18: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 0.0715 - mae: 0.0758 - root_mean_squared_error: 0.2434 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0729 - mae: 0.0964 - root_mean_squared_error: 0.2463\n",
      "Epoch 19: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0729 - mae: 0.0964 - root_mean_squared_error: 0.2463 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0763 - root_mean_squared_error: 0.2434\n",
      "Epoch 20: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0715 - mae: 0.0763 - root_mean_squared_error: 0.2434 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0780 - root_mean_squared_error: 0.2435\n",
      "Epoch 21: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0716 - mae: 0.0780 - root_mean_squared_error: 0.2435 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0810 - root_mean_squared_error: 0.2416\n",
      "Epoch 22: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0706 - mae: 0.0810 - root_mean_squared_error: 0.2416 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0641 - root_mean_squared_error: 0.2434\n",
      "Epoch 23: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0715 - mae: 0.0641 - root_mean_squared_error: 0.2434 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0881 - root_mean_squared_error: 0.2439\n",
      "Epoch 24: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0718 - mae: 0.0881 - root_mean_squared_error: 0.2439 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0624 - root_mean_squared_error: 0.2439\n",
      "Epoch 25: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0717 - mae: 0.0624 - root_mean_squared_error: 0.2439 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0755 - root_mean_squared_error: 0.2425\n",
      "Epoch 26: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.0710 - mae: 0.0755 - root_mean_squared_error: 0.2425 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0707 - mae: 0.0859 - root_mean_squared_error: 0.2417\n",
      "Epoch 27: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0707 - mae: 0.0859 - root_mean_squared_error: 0.2417 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0766 - root_mean_squared_error: 0.2425\n",
      "Epoch 28: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0711 - mae: 0.0766 - root_mean_squared_error: 0.2425 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0738 - root_mean_squared_error: 0.2436\n",
      "Epoch 29: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0716 - mae: 0.0738 - root_mean_squared_error: 0.2436 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0657 - root_mean_squared_error: 0.2434\n",
      "Epoch 30: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0715 - mae: 0.0657 - root_mean_squared_error: 0.2434 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0705 - mae: 0.0781 - root_mean_squared_error: 0.2414\n",
      "Epoch 31: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0705 - mae: 0.0781 - root_mean_squared_error: 0.2414 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0599 - root_mean_squared_error: 0.2440\n",
      "Epoch 32: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0718 - mae: 0.0599 - root_mean_squared_error: 0.2440 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0688 - root_mean_squared_error: 0.2416\n",
      "Epoch 33: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0706 - mae: 0.0688 - root_mean_squared_error: 0.2416 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0727 - root_mean_squared_error: 0.2422\n",
      "Epoch 34: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.0709 - mae: 0.0727 - root_mean_squared_error: 0.2422 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0617 - root_mean_squared_error: 0.2437\n",
      "Epoch 35: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0716 - mae: 0.0617 - root_mean_squared_error: 0.2437 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0822 - root_mean_squared_error: 0.2425\n",
      "Epoch 36: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0711 - mae: 0.0822 - root_mean_squared_error: 0.2425 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0665 - root_mean_squared_error: 0.2426\n",
      "Epoch 37: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0711 - mae: 0.0665 - root_mean_squared_error: 0.2426 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0756 - root_mean_squared_error: 0.2443\n",
      "Epoch 38: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0719 - mae: 0.0756 - root_mean_squared_error: 0.2443 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0716 - root_mean_squared_error: 0.2436\n",
      "Epoch 39: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0716 - mae: 0.0716 - root_mean_squared_error: 0.2436 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0813 - root_mean_squared_error: 0.2415\n",
      "Epoch 40: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0706 - mae: 0.0813 - root_mean_squared_error: 0.2415 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0667 - root_mean_squared_error: 0.2443\n",
      "Epoch 41: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0720 - mae: 0.0667 - root_mean_squared_error: 0.2443 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0705 - mae: 0.0756 - root_mean_squared_error: 0.2413\n",
      "Epoch 42: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0705 - mae: 0.0756 - root_mean_squared_error: 0.2413 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0642 - root_mean_squared_error: 0.2434\n",
      "Epoch 43: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0715 - mae: 0.0642 - root_mean_squared_error: 0.2434 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0659 - root_mean_squared_error: 0.2421\n",
      "Epoch 44: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0709 - mae: 0.0659 - root_mean_squared_error: 0.2421 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0726 - mae: 0.0961 - root_mean_squared_error: 0.2457\n",
      "Epoch 45: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0726 - mae: 0.0961 - root_mean_squared_error: 0.2457 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0630 - root_mean_squared_error: 0.2438\n",
      "Epoch 46: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0717 - mae: 0.0630 - root_mean_squared_error: 0.2438 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0708 - mae: 0.0898 - root_mean_squared_error: 0.2420\n",
      "Epoch 47: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0708 - mae: 0.0898 - root_mean_squared_error: 0.2420 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0787 - mae: 0.1208 - root_mean_squared_error: 0.2578\n",
      "Epoch 48: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0787 - mae: 0.1208 - root_mean_squared_error: 0.2578 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0733 - root_mean_squared_error: 0.2438\n",
      "Epoch 49: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0717 - mae: 0.0733 - root_mean_squared_error: 0.2438 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0819 - root_mean_squared_error: 0.2442\n",
      "Epoch 50: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0719 - mae: 0.0819 - root_mean_squared_error: 0.2442 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0716 - root_mean_squared_error: 0.2427\n",
      "Epoch 51: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0711 - mae: 0.0716 - root_mean_squared_error: 0.2427 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0776 - mae: 0.1256 - root_mean_squared_error: 0.2556\n",
      "Epoch 52: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 0.0776 - mae: 0.1256 - root_mean_squared_error: 0.2556 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0806 - root_mean_squared_error: 0.2415\n",
      "Epoch 53: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0706 - mae: 0.0806 - root_mean_squared_error: 0.2415 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0705 - mae: 0.0746 - root_mean_squared_error: 0.2414\n",
      "Epoch 54: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0705 - mae: 0.0746 - root_mean_squared_error: 0.2414 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0786 - root_mean_squared_error: 0.2434\n",
      "Epoch 55: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0715 - mae: 0.0786 - root_mean_squared_error: 0.2434 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0757 - root_mean_squared_error: 0.2425\n",
      "Epoch 56: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0711 - mae: 0.0757 - root_mean_squared_error: 0.2425 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0668 - root_mean_squared_error: 0.2429\n",
      "Epoch 57: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0712 - mae: 0.0668 - root_mean_squared_error: 0.2429 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0668 - root_mean_squared_error: 0.2434\n",
      "Epoch 58: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0715 - mae: 0.0668 - root_mean_squared_error: 0.2434 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0731 - root_mean_squared_error: 0.2431\n",
      "Epoch 59: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0714 - mae: 0.0731 - root_mean_squared_error: 0.2431 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0669 - root_mean_squared_error: 0.2428\n",
      "Epoch 60: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0712 - mae: 0.0669 - root_mean_squared_error: 0.2428 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0731 - mae: 0.0555 - root_mean_squared_error: 0.2467\n",
      "Epoch 61: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0731 - mae: 0.0555 - root_mean_squared_error: 0.2467 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0707 - mae: 0.0744 - root_mean_squared_error: 0.2419\n",
      "Epoch 62: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0707 - mae: 0.0744 - root_mean_squared_error: 0.2419 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0684 - root_mean_squared_error: 0.2435\n",
      "Epoch 63: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0715 - mae: 0.0684 - root_mean_squared_error: 0.2435 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0779 - root_mean_squared_error: 0.2433\n",
      "Epoch 64: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0715 - mae: 0.0779 - root_mean_squared_error: 0.2433 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0794 - root_mean_squared_error: 0.2415\n",
      "Epoch 65: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 0.0706 - mae: 0.0794 - root_mean_squared_error: 0.2415 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0674 - root_mean_squared_error: 0.2421\n",
      "Epoch 66: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0709 - mae: 0.0674 - root_mean_squared_error: 0.2421 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0844 - root_mean_squared_error: 0.2415\n",
      "Epoch 67: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0706 - mae: 0.0844 - root_mean_squared_error: 0.2415 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0653 - root_mean_squared_error: 0.2432\n",
      "Epoch 68: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0714 - mae: 0.0653 - root_mean_squared_error: 0.2432 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0730 - root_mean_squared_error: 0.2422\n",
      "Epoch 69: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0709 - mae: 0.0730 - root_mean_squared_error: 0.2422 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0704 - root_mean_squared_error: 0.2416\n",
      "Epoch 70: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0706 - mae: 0.0704 - root_mean_squared_error: 0.2416 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0793 - root_mean_squared_error: 0.2434\n",
      "Epoch 71: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0715 - mae: 0.0793 - root_mean_squared_error: 0.2434 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0858 - root_mean_squared_error: 0.2430\n",
      "Epoch 72: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0713 - mae: 0.0858 - root_mean_squared_error: 0.2430 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0704 - mae: 0.0862 - root_mean_squared_error: 0.2411\n",
      "Epoch 73: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0704 - mae: 0.0862 - root_mean_squared_error: 0.2411 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0708 - mae: 0.0805 - root_mean_squared_error: 0.2420\n",
      "Epoch 74: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0708 - mae: 0.0805 - root_mean_squared_error: 0.2420 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0808 - root_mean_squared_error: 0.2424\n",
      "Epoch 75: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0710 - mae: 0.0808 - root_mean_squared_error: 0.2424 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0754 - root_mean_squared_error: 0.2425\n",
      "Epoch 76: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 0.0711 - mae: 0.0754 - root_mean_squared_error: 0.2425 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0726 - mae: 0.0989 - root_mean_squared_error: 0.2457\n",
      "Epoch 77: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0726 - mae: 0.0989 - root_mean_squared_error: 0.2457 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0670 - root_mean_squared_error: 0.2415\n",
      "Epoch 78: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0706 - mae: 0.0670 - root_mean_squared_error: 0.2415 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0714 - root_mean_squared_error: 0.2436\n",
      "Epoch 79: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0716 - mae: 0.0714 - root_mean_squared_error: 0.2436 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0602 - root_mean_squared_error: 0.2430\n",
      "Epoch 80: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0713 - mae: 0.0602 - root_mean_squared_error: 0.2430 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0844 - root_mean_squared_error: 0.2427\n",
      "Epoch 81: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0711 - mae: 0.0844 - root_mean_squared_error: 0.2427 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0789 - root_mean_squared_error: 0.2435\n",
      "Epoch 82: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0715 - mae: 0.0789 - root_mean_squared_error: 0.2435 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0631 - root_mean_squared_error: 0.2446\n",
      "Epoch 83: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0721 - mae: 0.0631 - root_mean_squared_error: 0.2446 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0724 - mae: 0.0594 - root_mean_squared_error: 0.2452\n",
      "Epoch 84: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0724 - mae: 0.0594 - root_mean_squared_error: 0.2452 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0754 - root_mean_squared_error: 0.2425\n",
      "Epoch 85: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 0.0710 - mae: 0.0754 - root_mean_squared_error: 0.2425 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0855 - root_mean_squared_error: 0.2415\n",
      "Epoch 86: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0706 - mae: 0.0855 - root_mean_squared_error: 0.2415 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0825 - root_mean_squared_error: 0.2435\n",
      "Epoch 87: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0715 - mae: 0.0825 - root_mean_squared_error: 0.2435 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0674 - root_mean_squared_error: 0.2421\n",
      "Epoch 88: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0709 - mae: 0.0674 - root_mean_squared_error: 0.2421 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0793 - root_mean_squared_error: 0.2423\n",
      "Epoch 89: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0709 - mae: 0.0793 - root_mean_squared_error: 0.2423 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0708 - mae: 0.0832 - root_mean_squared_error: 0.2421\n",
      "Epoch 90: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0708 - mae: 0.0832 - root_mean_squared_error: 0.2421 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0705 - mae: 0.0746 - root_mean_squared_error: 0.2414\n",
      "Epoch 91: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.0705 - mae: 0.0746 - root_mean_squared_error: 0.2414 - val_loss: 0.0153 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0808 - root_mean_squared_error: 0.2417\n",
      "Epoch 92: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0706 - mae: 0.0808 - root_mean_squared_error: 0.2417 - val_loss: 0.0153 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0590 - root_mean_squared_error: 0.2447\n",
      "Epoch 93: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0721 - mae: 0.0590 - root_mean_squared_error: 0.2447 - val_loss: 0.0153 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0769 - root_mean_squared_error: 0.2424\n",
      "Epoch 94: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 0.0710 - mae: 0.0769 - root_mean_squared_error: 0.2424 - val_loss: 0.0153 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0782 - root_mean_squared_error: 0.2416\n",
      "Epoch 95: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0706 - mae: 0.0782 - root_mean_squared_error: 0.2416 - val_loss: 0.0153 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0751 - mae: 0.1060 - root_mean_squared_error: 0.2507\n",
      "Epoch 96: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0751 - mae: 0.1060 - root_mean_squared_error: 0.2507 - val_loss: 0.0153 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0902 - root_mean_squared_error: 0.2422\n",
      "Epoch 97: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0709 - mae: 0.0902 - root_mean_squared_error: 0.2422 - val_loss: 0.0153 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0731 - root_mean_squared_error: 0.2434\n",
      "Epoch 98: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0715 - mae: 0.0731 - root_mean_squared_error: 0.2434 - val_loss: 0.0153 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0930 - root_mean_squared_error: 0.2446\n",
      "Epoch 99: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0721 - mae: 0.0930 - root_mean_squared_error: 0.2446 - val_loss: 0.0153 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0848 - root_mean_squared_error: 0.2443\n",
      "Epoch 100: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0719 - mae: 0.0848 - root_mean_squared_error: 0.2443 - val_loss: 0.0153 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "idx_tr: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "idx_va: [15]\n",
      "train_mask: [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True False]\n",
      "val_mask: [False False False False False False False False False False False False\n",
      " False False False  True]\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0793 - root_mean_squared_error: 0.2438\n",
      "Epoch 1: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.0717 - mae: 0.0793 - root_mean_squared_error: 0.2438 - val_loss: 0.0497 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0663 - root_mean_squared_error: 0.2426\n",
      "Epoch 2: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0711 - mae: 0.0663 - root_mean_squared_error: 0.2426 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0703 - mae: 0.0688 - root_mean_squared_error: 0.2410\n",
      "Epoch 3: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0703 - mae: 0.0688 - root_mean_squared_error: 0.2410 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0722 - mae: 0.0910 - root_mean_squared_error: 0.2449\n",
      "Epoch 4: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.0722 - mae: 0.0910 - root_mean_squared_error: 0.2449 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0749 - root_mean_squared_error: 0.2425\n",
      "Epoch 5: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0710 - mae: 0.0749 - root_mean_squared_error: 0.2425 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0704 - mae: 0.0796 - root_mean_squared_error: 0.2411\n",
      "Epoch 6: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0704 - mae: 0.0796 - root_mean_squared_error: 0.2411 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0748 - root_mean_squared_error: 0.2423\n",
      "Epoch 7: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0710 - mae: 0.0748 - root_mean_squared_error: 0.2423 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0668 - root_mean_squared_error: 0.2430\n",
      "Epoch 8: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0713 - mae: 0.0668 - root_mean_squared_error: 0.2430 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0890 - root_mean_squared_error: 0.2443\n",
      "Epoch 9: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.0719 - mae: 0.0890 - root_mean_squared_error: 0.2443 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0814 - root_mean_squared_error: 0.2436\n",
      "Epoch 10: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0716 - mae: 0.0814 - root_mean_squared_error: 0.2436 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0629 - root_mean_squared_error: 0.2440\n",
      "Epoch 11: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0717 - mae: 0.0629 - root_mean_squared_error: 0.2440 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0800 - root_mean_squared_error: 0.2426\n",
      "Epoch 12: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0711 - mae: 0.0800 - root_mean_squared_error: 0.2426 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0699 - root_mean_squared_error: 0.2437\n",
      "Epoch 13: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0716 - mae: 0.0699 - root_mean_squared_error: 0.2437 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0645 - root_mean_squared_error: 0.2424\n",
      "Epoch 14: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0710 - mae: 0.0645 - root_mean_squared_error: 0.2424 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0624 - root_mean_squared_error: 0.2428\n",
      "Epoch 15: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0712 - mae: 0.0624 - root_mean_squared_error: 0.2428 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0742 - root_mean_squared_error: 0.2435\n",
      "Epoch 16: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0715 - mae: 0.0742 - root_mean_squared_error: 0.2435 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0709 - root_mean_squared_error: 0.2426\n",
      "Epoch 17: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0711 - mae: 0.0709 - root_mean_squared_error: 0.2426 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0682 - root_mean_squared_error: 0.2439\n",
      "Epoch 18: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0717 - mae: 0.0682 - root_mean_squared_error: 0.2439 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0768 - root_mean_squared_error: 0.2435\n",
      "Epoch 19: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0715 - mae: 0.0768 - root_mean_squared_error: 0.2435 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0633 - root_mean_squared_error: 0.2437\n",
      "Epoch 20: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0716 - mae: 0.0633 - root_mean_squared_error: 0.2437 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0722 - root_mean_squared_error: 0.2424\n",
      "Epoch 21: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0710 - mae: 0.0722 - root_mean_squared_error: 0.2424 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0922 - root_mean_squared_error: 0.2437\n",
      "Epoch 22: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0716 - mae: 0.0922 - root_mean_squared_error: 0.2437 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0828 - root_mean_squared_error: 0.2425\n",
      "Epoch 23: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0710 - mae: 0.0828 - root_mean_squared_error: 0.2425 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0705 - mae: 0.0754 - root_mean_squared_error: 0.2413\n",
      "Epoch 24: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0705 - mae: 0.0754 - root_mean_squared_error: 0.2413 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0731 - root_mean_squared_error: 0.2430\n",
      "Epoch 25: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.0713 - mae: 0.0731 - root_mean_squared_error: 0.2430 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0770 - root_mean_squared_error: 0.2423\n",
      "Epoch 26: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0709 - mae: 0.0770 - root_mean_squared_error: 0.2423 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - mae: 0.0796 - root_mean_squared_error: 0.2423\n",
      "Epoch 27: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0709 - mae: 0.0796 - root_mean_squared_error: 0.2423 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0899 - root_mean_squared_error: 0.2432\n",
      "Epoch 28: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.0714 - mae: 0.0899 - root_mean_squared_error: 0.2432 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0648 - root_mean_squared_error: 0.2435\n",
      "Epoch 29: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0715 - mae: 0.0648 - root_mean_squared_error: 0.2435 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0726 - mae: 0.0574 - root_mean_squared_error: 0.2456\n",
      "Epoch 30: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0726 - mae: 0.0574 - root_mean_squared_error: 0.2456 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0708 - mae: 0.0740 - root_mean_squared_error: 0.2420\n",
      "Epoch 31: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0708 - mae: 0.0740 - root_mean_squared_error: 0.2420 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0606 - root_mean_squared_error: 0.2441\n",
      "Epoch 32: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0718 - mae: 0.0606 - root_mean_squared_error: 0.2441 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0755 - root_mean_squared_error: 0.2437\n",
      "Epoch 33: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0716 - mae: 0.0755 - root_mean_squared_error: 0.2437 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0686 - root_mean_squared_error: 0.2429\n",
      "Epoch 34: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0712 - mae: 0.0686 - root_mean_squared_error: 0.2429 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0632 - root_mean_squared_error: 0.2436\n",
      "Epoch 35: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 0.0716 - mae: 0.0632 - root_mean_squared_error: 0.2436 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0758 - mae: 0.1122 - root_mean_squared_error: 0.2522\n",
      "Epoch 36: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0758 - mae: 0.1122 - root_mean_squared_error: 0.2522 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0705 - root_mean_squared_error: 0.2416\n",
      "Epoch 37: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0706 - mae: 0.0705 - root_mean_squared_error: 0.2416 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0676 - root_mean_squared_error: 0.2428\n",
      "Epoch 38: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0712 - mae: 0.0676 - root_mean_squared_error: 0.2428 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0751 - root_mean_squared_error: 0.2425\n",
      "Epoch 39: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0710 - mae: 0.0751 - root_mean_squared_error: 0.2425 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0893 - root_mean_squared_error: 0.2442\n",
      "Epoch 40: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0719 - mae: 0.0893 - root_mean_squared_error: 0.2442 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0821 - root_mean_squared_error: 0.2425\n",
      "Epoch 41: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0710 - mae: 0.0821 - root_mean_squared_error: 0.2425 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0756 - root_mean_squared_error: 0.2416\n",
      "Epoch 42: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0706 - mae: 0.0756 - root_mean_squared_error: 0.2416 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0727 - mae: 0.0531 - root_mean_squared_error: 0.2459\n",
      "Epoch 43: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0727 - mae: 0.0531 - root_mean_squared_error: 0.2459 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0749 - mae: 0.0925 - root_mean_squared_error: 0.2504\n",
      "Epoch 44: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0749 - mae: 0.0925 - root_mean_squared_error: 0.2504 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0614 - root_mean_squared_error: 0.2428\n",
      "Epoch 45: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0712 - mae: 0.0614 - root_mean_squared_error: 0.2428 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0834 - root_mean_squared_error: 0.2436\n",
      "Epoch 46: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.0715 - mae: 0.0834 - root_mean_squared_error: 0.2436 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0877 - root_mean_squared_error: 0.2447\n",
      "Epoch 47: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0721 - mae: 0.0877 - root_mean_squared_error: 0.2447 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0638 - root_mean_squared_error: 0.2436\n",
      "Epoch 48: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0716 - mae: 0.0638 - root_mean_squared_error: 0.2436 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0657 - root_mean_squared_error: 0.2440\n",
      "Epoch 49: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0718 - mae: 0.0657 - root_mean_squared_error: 0.2440 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0625 - root_mean_squared_error: 0.2446\n",
      "Epoch 50: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0721 - mae: 0.0625 - root_mean_squared_error: 0.2446 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0629 - root_mean_squared_error: 0.2446\n",
      "Epoch 51: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0721 - mae: 0.0629 - root_mean_squared_error: 0.2446 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0600 - root_mean_squared_error: 0.2444\n",
      "Epoch 52: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0720 - mae: 0.0600 - root_mean_squared_error: 0.2444 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0772 - mae: 0.1134 - root_mean_squared_error: 0.2549\n",
      "Epoch 53: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0772 - mae: 0.1134 - root_mean_squared_error: 0.2549 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0690 - root_mean_squared_error: 0.2436\n",
      "Epoch 54: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0716 - mae: 0.0690 - root_mean_squared_error: 0.2436 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0714 - root_mean_squared_error: 0.2426\n",
      "Epoch 55: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0711 - mae: 0.0714 - root_mean_squared_error: 0.2426 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0786 - root_mean_squared_error: 0.2434\n",
      "Epoch 56: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0715 - mae: 0.0786 - root_mean_squared_error: 0.2434 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - mae: 0.0691 - root_mean_squared_error: 0.2448\n",
      "Epoch 57: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0721 - mae: 0.0691 - root_mean_squared_error: 0.2448 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0804 - root_mean_squared_error: 0.2428\n",
      "Epoch 58: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0712 - mae: 0.0804 - root_mean_squared_error: 0.2428 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0707 - mae: 0.0675 - root_mean_squared_error: 0.2418\n",
      "Epoch 59: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.0707 - mae: 0.0675 - root_mean_squared_error: 0.2418 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0708 - mae: 0.0685 - root_mean_squared_error: 0.2420\n",
      "Epoch 60: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0708 - mae: 0.0685 - root_mean_squared_error: 0.2420 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0762 - mae: 0.1131 - root_mean_squared_error: 0.2530\n",
      "Epoch 61: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0762 - mae: 0.1131 - root_mean_squared_error: 0.2530 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.0944 - root_mean_squared_error: 0.2443\n",
      "Epoch 62: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0719 - mae: 0.0944 - root_mean_squared_error: 0.2443 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0785 - root_mean_squared_error: 0.2434\n",
      "Epoch 63: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0715 - mae: 0.0785 - root_mean_squared_error: 0.2434 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.0618 - root_mean_squared_error: 0.2433\n",
      "Epoch 64: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0714 - mae: 0.0618 - root_mean_squared_error: 0.2433 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0945 - root_mean_squared_error: 0.2444\n",
      "Epoch 65: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0720 - mae: 0.0945 - root_mean_squared_error: 0.2444 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0627 - root_mean_squared_error: 0.2437\n",
      "Epoch 66: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0716 - mae: 0.0627 - root_mean_squared_error: 0.2437 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mae: 0.0844 - root_mean_squared_error: 0.2435\n",
      "Epoch 67: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0715 - mae: 0.0844 - root_mean_squared_error: 0.2435 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0758 - root_mean_squared_error: 0.2415\n",
      "Epoch 68: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0706 - mae: 0.0758 - root_mean_squared_error: 0.2415 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0705 - mae: 0.0700 - root_mean_squared_error: 0.2414\n",
      "Epoch 69: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0705 - mae: 0.0700 - root_mean_squared_error: 0.2414 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0766 - mae: 0.1126 - root_mean_squared_error: 0.2538\n",
      "Epoch 70: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0766 - mae: 0.1126 - root_mean_squared_error: 0.2538 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0749 - mae: 0.0804 - root_mean_squared_error: 0.2503\n",
      "Epoch 71: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0749 - mae: 0.0804 - root_mean_squared_error: 0.2503 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0723 - mae: 0.0648 - root_mean_squared_error: 0.2452\n",
      "Epoch 72: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0723 - mae: 0.0648 - root_mean_squared_error: 0.2452 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0707 - mae: 0.0696 - root_mean_squared_error: 0.2418\n",
      "Epoch 73: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0707 - mae: 0.0696 - root_mean_squared_error: 0.2418 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0624 - root_mean_squared_error: 0.2446\n",
      "Epoch 74: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0720 - mae: 0.0624 - root_mean_squared_error: 0.2446 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.0676 - root_mean_squared_error: 0.2441\n",
      "Epoch 75: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0718 - mae: 0.0676 - root_mean_squared_error: 0.2441 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - mae: 0.0641 - root_mean_squared_error: 0.2445\n",
      "Epoch 76: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0720 - mae: 0.0641 - root_mean_squared_error: 0.2445 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0705 - mae: 0.0709 - root_mean_squared_error: 0.2415\n",
      "Epoch 77: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0705 - mae: 0.0709 - root_mean_squared_error: 0.2415 - val_loss: 0.0496 - val_mae: 0.0721 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0898 - root_mean_squared_error: 0.2437\n",
      "Epoch 78: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0716 - mae: 0.0898 - root_mean_squared_error: 0.2437 - val_loss: 0.0496 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0723 - mae: 0.0652 - root_mean_squared_error: 0.2451\n",
      "Epoch 79: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0723 - mae: 0.0652 - root_mean_squared_error: 0.2451 - val_loss: 0.0496 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0793 - root_mean_squared_error: 0.2425\n",
      "Epoch 80: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 0.0710 - mae: 0.0793 - root_mean_squared_error: 0.2425 - val_loss: 0.0496 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0724 - mae: 0.0863 - root_mean_squared_error: 0.2454\n",
      "Epoch 81: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0724 - mae: 0.0863 - root_mean_squared_error: 0.2454 - val_loss: 0.0496 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0784 - root_mean_squared_error: 0.2438\n",
      "Epoch 82: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0716 - mae: 0.0784 - root_mean_squared_error: 0.2438 - val_loss: 0.0496 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0793 - root_mean_squared_error: 0.2424\n",
      "Epoch 83: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0710 - mae: 0.0793 - root_mean_squared_error: 0.2424 - val_loss: 0.0496 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0649 - root_mean_squared_error: 0.2436\n",
      "Epoch 84: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0716 - mae: 0.0649 - root_mean_squared_error: 0.2436 - val_loss: 0.0496 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - mae: 0.0784 - root_mean_squared_error: 0.2436\n",
      "Epoch 85: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0716 - mae: 0.0784 - root_mean_squared_error: 0.2436 - val_loss: 0.0496 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0655 - root_mean_squared_error: 0.2438\n",
      "Epoch 86: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0717 - mae: 0.0655 - root_mean_squared_error: 0.2438 - val_loss: 0.0496 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0783 - root_mean_squared_error: 0.2417\n",
      "Epoch 87: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0706 - mae: 0.0783 - root_mean_squared_error: 0.2417 - val_loss: 0.0496 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - mae: 0.0841 - root_mean_squared_error: 0.2430\n",
      "Epoch 88: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0713 - mae: 0.0841 - root_mean_squared_error: 0.2430 - val_loss: 0.0496 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0761 - root_mean_squared_error: 0.2424\n",
      "Epoch 89: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0710 - mae: 0.0761 - root_mean_squared_error: 0.2424 - val_loss: 0.0496 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0598 - root_mean_squared_error: 0.2429\n",
      "Epoch 90: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0712 - mae: 0.0598 - root_mean_squared_error: 0.2429 - val_loss: 0.0496 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0808 - root_mean_squared_error: 0.2428\n",
      "Epoch 91: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0711 - mae: 0.0808 - root_mean_squared_error: 0.2428 - val_loss: 0.0496 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0790 - root_mean_squared_error: 0.2415\n",
      "Epoch 92: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0706 - mae: 0.0790 - root_mean_squared_error: 0.2415 - val_loss: 0.0496 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - mae: 0.0865 - root_mean_squared_error: 0.2426\n",
      "Epoch 93: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0710 - mae: 0.0865 - root_mean_squared_error: 0.2426 - val_loss: 0.0496 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0569 - root_mean_squared_error: 0.2438\n",
      "Epoch 94: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0717 - mae: 0.0569 - root_mean_squared_error: 0.2438 - val_loss: 0.0496 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - mae: 0.0689 - root_mean_squared_error: 0.2427\n",
      "Epoch 95: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0711 - mae: 0.0689 - root_mean_squared_error: 0.2427 - val_loss: 0.0496 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - mae: 0.0809 - root_mean_squared_error: 0.2428\n",
      "Epoch 96: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0712 - mae: 0.0809 - root_mean_squared_error: 0.2428 - val_loss: 0.0496 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0844 - root_mean_squared_error: 0.2417\n",
      "Epoch 97: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0706 - mae: 0.0844 - root_mean_squared_error: 0.2417 - val_loss: 0.0496 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0705 - root_mean_squared_error: 0.2416\n",
      "Epoch 98: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 0.0706 - mae: 0.0705 - root_mean_squared_error: 0.2416 - val_loss: 0.0496 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - mae: 0.0697 - root_mean_squared_error: 0.2440\n",
      "Epoch 99: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0717 - mae: 0.0697 - root_mean_squared_error: 0.2440 - val_loss: 0.0496 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0705 - mae: 0.0727 - root_mean_squared_error: 0.2414\n",
      "Epoch 100: val_loss did not improve from 0.01255\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0705 - mae: 0.0727 - root_mean_squared_error: 0.2414 - val_loss: 0.0496 - val_mae: 0.0722 - val_root_mean_squared_error: 0.2425 - lr: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "#LOO funzionante\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "'''x_train, a_train, x_test, a_test = train_test_split([x,a], train_size=0.8, shuffle=True, random_state=1)\n",
    "target_train,target_test=train_test_split(labels_encoded, train_size=0.8, shuffle=True, random_state=1)\n",
    "print('x_train:',x_train)\n",
    "print('a_train:',a_train)\n",
    "print('x_test:',x_test)\n",
    "print('a_test:',a_test)'''\n",
    "loo=LeaveOneOut()\n",
    "loo.get_n_splits(c2)\n",
    "#print('np.random.randint(0,10):',np.random.randint(0,10,size=10))\n",
    "predictions=[]\n",
    "for idx_tr, idx_va in loo.split(c2):\n",
    "    print(\"idx_tr:\", idx_tr)\n",
    "    print(\"idx_va:\", idx_va)\n",
    "    #set the mask\n",
    "    train_mask = np.zeros((n2,),dtype=bool)\n",
    "    train_mask[idx_tr] = True\n",
    "    print('train_mask:',train_mask)\n",
    "    val_mask = np.zeros((n2,),dtype=bool)\n",
    "    val_mask[idx_va] = True\n",
    "    print('val_mask:',val_mask)\n",
    "    # Train model\n",
    "    validation_data = ([x2,a2],yy2,val_mask)\n",
    "    #print('validation_data',validation_data)'''\n",
    "    history=model.fit(x=[x2, a2], y=yy2,\n",
    "                      epochs=epochs,\n",
    "                      batch_size=n2,\n",
    "                      validation_data=validation_data,\n",
    "                      shuffle=False,\n",
    "                      callbacks=[handmade_callbacks]).history \n",
    "    \n",
    "\n",
    "#report = classification_report(np.argmax(labels_encoded,axis=1), np.argmax(predictions,axis=1))\n",
    "    \n",
    "    \n",
    "#print('predictions',predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a381dfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step\n",
      "y_pred = [[-0.00466357 -0.01234648  0.06865349 -0.0194058   0.00345619  0.00394421\n",
      "  -0.01024378 -0.03926101  0.03342415 -0.03576304 -0.00340806 -0.05355725\n",
      "  -0.0452601  -0.01604426 -0.05253958  0.15548562]\n",
      " [-0.00482992 -0.01278687  0.07110229 -0.02009799  0.00357947  0.0040849\n",
      "  -0.01060916 -0.04066142  0.03461635 -0.03703867 -0.00352962 -0.05546759\n",
      "  -0.04687447 -0.01661654 -0.05441361  0.16103162]\n",
      " [-0.00482662 -0.01277815  0.0710538  -0.02008429  0.00357702  0.00408211\n",
      "  -0.01060193 -0.04063369  0.03459275 -0.03701342 -0.00352721 -0.05542976\n",
      "  -0.04684251 -0.01660521 -0.05437651  0.1609218 ]\n",
      " [-0.00482496 -0.01277374  0.07102928 -0.02007735  0.00357579  0.0040807\n",
      "  -0.01059827 -0.04061966  0.0345808  -0.03700063 -0.003526   -0.05541062\n",
      "  -0.04682634 -0.01659948 -0.05435773  0.16086625]\n",
      " [-0.00482321 -0.01276913  0.07100365 -0.0200701   0.0035745   0.00407923\n",
      "  -0.01059444 -0.040605    0.03456832 -0.03698728 -0.00352472 -0.05539062\n",
      "  -0.04680944 -0.01659349 -0.05433811  0.16080819]\n",
      " [-0.00482379 -0.01277065  0.07101208 -0.02007249  0.00357492  0.00407971\n",
      "  -0.0105957  -0.04060983  0.03457243 -0.03699167 -0.00352514 -0.05539721\n",
      "  -0.046815   -0.01659546 -0.05434457  0.1608273 ]\n",
      " [-0.00482186 -0.01276555  0.07098372 -0.02006447  0.0035735   0.00407808\n",
      "  -0.01059147 -0.04059361  0.03455862 -0.0369769  -0.00352373 -0.05537508\n",
      "  -0.0467963  -0.01658883 -0.05432286  0.16076307]\n",
      " [-0.00482195 -0.01276578  0.070985   -0.02006484  0.00357356  0.00407816\n",
      "  -0.01059166 -0.04059434  0.03455925 -0.03697757 -0.0035238  -0.05537609\n",
      "  -0.04679715 -0.01658913 -0.05432385  0.16076599]\n",
      " [-0.00482197 -0.01276582  0.07098526 -0.02006491  0.00357357  0.00407817\n",
      "  -0.0105917  -0.04059449  0.03455938 -0.03697771 -0.00352381 -0.05537629\n",
      "  -0.04679732 -0.01658919 -0.05432405  0.16076656]\n",
      " [-0.00482169 -0.0127651   0.07098125 -0.02006377  0.00357337  0.00407794\n",
      "  -0.0105911  -0.04059219  0.03455742 -0.03697561 -0.00352361 -0.05537316\n",
      "  -0.04679468 -0.01658825 -0.05432097  0.16075747]\n",
      " [-0.00482345 -0.01276974  0.07100706 -0.02007107  0.00357467  0.00407942\n",
      "  -0.01059495 -0.04060696  0.03456999 -0.03698906 -0.00352489 -0.0553933\n",
      "  -0.0468117  -0.01659429 -0.05434073  0.16081594]\n",
      " [-0.00482314 -0.01276893  0.07100253 -0.02006979  0.00357444  0.00407916\n",
      "  -0.01059428 -0.04060437  0.03456779 -0.0369867  -0.00352467 -0.05538977\n",
      "  -0.04680871 -0.01659323 -0.05433726  0.16080569]\n",
      " [-0.00482674 -0.01277846  0.07105556 -0.02008478  0.00357711  0.00408221\n",
      "  -0.01060219 -0.04063469  0.0345936  -0.03701432 -0.0035273  -0.05543113\n",
      "  -0.04684367 -0.01660562 -0.05437784  0.16092576]\n",
      " [-0.00482807 -0.01278198  0.07107511 -0.02009031  0.0035781   0.00408333\n",
      "  -0.01060511 -0.04064588  0.03460313 -0.03702451 -0.00352827 -0.05544639\n",
      "  -0.04685657 -0.01661019 -0.05439281  0.16097008]\n",
      " [-0.00482578 -0.01277591  0.07104138 -0.02008077  0.0035764   0.0040814\n",
      "  -0.01060008 -0.04062658  0.0345867  -0.03700694 -0.0035266  -0.05542007\n",
      "  -0.04683432 -0.01660231 -0.05436699  0.16089368]\n",
      " [-0.00469777 -0.01243702  0.06915692 -0.0195481   0.00348153  0.00397313\n",
      "  -0.01031889 -0.0395489   0.03366924 -0.03602528 -0.00343305 -0.05394998\n",
      "  -0.04559198 -0.01616191 -0.05292485  0.15662578]]\n"
     ]
    }
   ],
   "source": [
    "y_pred2 = model.predict([x2, a2],batch_size=n2)\n",
    "print('y_pred =',y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6985632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "y_pred = [[-0.00337737 -0.01566721  0.08013614 -0.02363553  0.00495691  0.00614138\n",
      "  -0.01122669 -0.04829142  0.04233402 -0.04194328 -0.00812595 -0.06916362\n",
      "  -0.05924807 -0.01707729 -0.06648095  0.18039288]\n",
      " [-0.00336998 -0.01563294  0.07996084 -0.02358383  0.00494606  0.00612795\n",
      "  -0.01120213 -0.04818578  0.04224141 -0.04185152 -0.00810817 -0.06901232\n",
      "  -0.05911846 -0.01703993 -0.06633552  0.17999826]\n",
      " [-0.00336845 -0.01562586  0.07992462 -0.02357315  0.00494382  0.00612517\n",
      "  -0.01119705 -0.04816395  0.04222228 -0.04183256 -0.0081045  -0.06898106\n",
      "  -0.05909168 -0.01703221 -0.06630547  0.17991672]\n",
      " [-0.00336879 -0.01562743  0.07993263 -0.02357551  0.00494432  0.00612579\n",
      "  -0.01119818 -0.04816878  0.04222651 -0.04183676 -0.00810531 -0.06898798\n",
      "  -0.05909761 -0.01703392 -0.06631212  0.17993477]\n",
      " [-0.00336741 -0.01562101  0.0798998  -0.02356582  0.00494229  0.00612327\n",
      "  -0.01119358 -0.04814899  0.04220916 -0.04181958 -0.00810198 -0.06895964\n",
      "  -0.05907333 -0.01702692 -0.06628488  0.17986085]\n",
      " [-0.00336652 -0.01561691  0.07987883 -0.02355964  0.00494099  0.00612166\n",
      "  -0.01119064 -0.04813636  0.04219809 -0.0418086  -0.00809986 -0.06894154\n",
      "  -0.05905783 -0.01702245 -0.06626748  0.17981365]\n",
      " [-0.00324304 -0.0150441   0.07694899 -0.02269551  0.00475976  0.00589713\n",
      "  -0.01078018 -0.04637079  0.04065032 -0.04027512 -0.00780276 -0.06641285\n",
      "  -0.05689167 -0.01639809 -0.06383689  0.17321835]\n",
      " [-0.00336743 -0.01562112  0.07990037 -0.02356599  0.00494232  0.00612331\n",
      "  -0.01119366 -0.04814934  0.04220947 -0.04181987 -0.00810204 -0.06896013\n",
      "  -0.05907375 -0.01702704 -0.06628535  0.17986214]\n",
      " [-0.00336428 -0.01560649  0.07982556 -0.02354393  0.0049377   0.00611758\n",
      "  -0.01118318 -0.04810426  0.04216995 -0.04178072 -0.00809445 -0.06889556\n",
      "  -0.05901844 -0.0170111  -0.06622329  0.17969376]\n",
      " [-0.00336461 -0.01560802  0.07983339 -0.02354624  0.00493818  0.00611818\n",
      "  -0.01118427 -0.04810898  0.04217408 -0.04178481 -0.00809525 -0.06890232\n",
      "  -0.05902423 -0.01701277 -0.06622978  0.17971137]\n",
      " [-0.00336728 -0.01562041  0.07989674 -0.02356492  0.0049421   0.00612304\n",
      "  -0.01119315 -0.04814716  0.04220755 -0.04181797 -0.00810167 -0.068957\n",
      "  -0.05907108 -0.01702627 -0.06628235  0.17985399]\n",
      " [-0.00336937 -0.0156301   0.07994633 -0.02357955  0.00494517  0.00612684\n",
      "  -0.0112001  -0.04817704  0.04223375 -0.04184393 -0.0081067  -0.0689998\n",
      "  -0.05910773 -0.01703683 -0.06632348  0.17996562]\n",
      " [-0.00336882 -0.01562755  0.07993326 -0.02357569  0.00494436  0.00612583\n",
      "  -0.01119826 -0.04816916  0.04222684 -0.04183709 -0.00810537 -0.06898852\n",
      "  -0.05909807 -0.01703405 -0.06631264  0.17993617]\n",
      " [-0.0033722  -0.01564323  0.08001345 -0.02359935  0.00494932  0.00613198\n",
      "  -0.0112095  -0.04821749  0.04226921 -0.04187906 -0.00811351 -0.06905773\n",
      "  -0.05915736 -0.01705114 -0.06637917  0.18011673]\n",
      " [-0.00329416 -0.01528122  0.07816182 -0.02305323  0.00483479  0.00599008\n",
      "  -0.0109501  -0.04710167  0.04129104 -0.04090992 -0.00792575 -0.06745962\n",
      "  -0.05778837 -0.01665655 -0.06484307  0.17594855]\n",
      " [-0.00337564 -0.01565923  0.08009529 -0.02362349  0.00495438  0.00613825\n",
      "  -0.01122096 -0.0482668   0.04231244 -0.04192189 -0.00812181 -0.06912836\n",
      "  -0.05921787 -0.01706858 -0.06644706  0.18030092]]\n"
     ]
    }
   ],
   "source": [
    "y_pred4 = model.predict([x4, a4],batch_size=n4)\n",
    "print('y_pred =',y_pred4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1b70d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_lab=np.argmax(y_pred, axis=1)\n",
    "#print(pred_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7f233b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoded_labels=encoder.inverse_transform(pred_lab)\n",
    "#print(decoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2697597d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from sklearn.metrics import classification_report\\nreport = classification_report(np.argmax(labels_encoded,axis=1), np.argmax(y_pred,axis=1),target_names=nodes,zero_division=1)\\nprint('GCN Classification Report: \\n {}'.format(report))\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from sklearn.metrics import classification_report\n",
    "report = classification_report(np.argmax(labels_encoded,axis=1), np.argmax(y_pred,axis=1),target_names=nodes,zero_division=1)\n",
    "print('GCN Classification Report: \\n {}'.format(report))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4426272b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1224x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1224x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1224x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training\n",
    "best_epoch = np.argmin(history['val_loss'])\n",
    "plt.figure(figsize=(17,4))\n",
    "plt.plot(history['loss'], label='Training loss', alpha=.8, color='#ff7f0e')\n",
    "plt.plot(history['val_loss'], label='Validation loss', alpha=.9, color='#5a9aa5')\n",
    "plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n",
    "plt.title('Mean Squared Error (Loss)')\n",
    "plt.legend()\n",
    "plt.grid(alpha=.3)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(17,4))\n",
    "plt.plot(history['mae'], label='Training accuracy', alpha=.8, color='#ff7f0e')\n",
    "plt.plot(history['val_mae'], label='Validation accuracy', alpha=.9, color='#5a9aa5')\n",
    "plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n",
    "plt.title('Mean Absolute Error')\n",
    "plt.legend()\n",
    "plt.grid(alpha=.3)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(17,4))\n",
    "plt.plot(history['root_mean_squared_error'], label='Training accuracy', alpha=.8, color='#ff7f0e')\n",
    "plt.plot(history['val_root_mean_squared_error'], label='Validation accuracy', alpha=.9, color='#5a9aa5')\n",
    "plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n",
    "plt.title('root_mean_squared_error')\n",
    "plt.legend()\n",
    "plt.grid(alpha=.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9fbe8f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n",
      "y_pred1 [[-0.00272818 -0.01265571  0.0647326  -0.01909238  0.00400411  0.0049609\n",
      "  -0.00906873 -0.03900898  0.0341967  -0.03388106 -0.006564   -0.05586918\n",
      "  -0.04785958 -0.01379474 -0.05370217  0.14571828]\n",
      " [-0.00282527 -0.01310608  0.06703622 -0.01977181  0.0041466   0.00513745\n",
      "  -0.00939145 -0.04039718  0.03541364 -0.03508678 -0.00679759 -0.05785738\n",
      "  -0.04956273 -0.01428565 -0.05561325  0.15090391]\n",
      " [-0.00282334 -0.01309713  0.06699043 -0.01975831  0.00414377  0.00513394\n",
      "  -0.00938504 -0.04036959  0.03538945 -0.03506281 -0.00679295 -0.05781786\n",
      "  -0.04952888 -0.01427589 -0.05557526  0.15080082]\n",
      " [-0.00282237 -0.01309263  0.06696744 -0.01975153  0.00414234  0.00513218\n",
      "  -0.00938182 -0.04035573  0.03537731 -0.03505078 -0.00679062 -0.05779802\n",
      "  -0.04951188 -0.01427099 -0.05555619  0.15074909]\n",
      " [-0.00282127 -0.01308754  0.06694137 -0.01974384  0.00414073  0.00513018\n",
      "  -0.00937816 -0.04034002  0.03536353 -0.03503713 -0.00678797 -0.05777552\n",
      "  -0.04949261 -0.01426543 -0.05553456  0.1506904 ]\n",
      " [-0.00282174 -0.01308972  0.06695253 -0.01974713  0.00414142  0.00513103\n",
      "  -0.00937973 -0.04034675  0.03536943 -0.03504297 -0.00678911 -0.05778515\n",
      "  -0.04950086 -0.01426781 -0.05554382  0.15071553]\n",
      " [-0.00282046 -0.01308378  0.06692217 -0.01973817  0.00413954  0.00512871\n",
      "  -0.00937547 -0.04032845  0.03535339 -0.03502708 -0.00678603 -0.05775895\n",
      "  -0.04947841 -0.01426134 -0.05551863  0.1506472 ]\n",
      " [-0.00282056 -0.01308426  0.06692464 -0.0197389   0.0041397   0.00512889\n",
      "  -0.00937582 -0.04032994  0.0353547  -0.03502837 -0.00678628 -0.05776108\n",
      "  -0.04948024 -0.01426187 -0.05552068  0.15065274]\n",
      " [-0.00282068 -0.01308481  0.0669274  -0.01973972  0.00413987  0.00512911\n",
      "  -0.00937621 -0.04033161  0.03535616 -0.03502982 -0.00678656 -0.05776347\n",
      "  -0.04948228 -0.01426246 -0.05552298  0.15065897]\n",
      " [-0.00282034 -0.01308324  0.06691942 -0.01973736  0.00413937  0.0051285\n",
      "  -0.00937509 -0.0403268   0.03535194 -0.03502565 -0.00678575 -0.05775658\n",
      "  -0.04947638 -0.01426076 -0.05551636  0.150641  ]\n",
      " [-0.00282151 -0.01308866  0.06694712 -0.01974553  0.00414109  0.00513062\n",
      "  -0.00937897 -0.04034349  0.03536657 -0.03504014 -0.00678856 -0.05778048\n",
      "  -0.04949686 -0.01426666 -0.05553933  0.15070334]\n",
      " [-0.00282116 -0.01308703  0.0669388  -0.01974308  0.00414057  0.00512998\n",
      "  -0.0093778  -0.04033848  0.03536218 -0.03503579 -0.00678771 -0.0577733\n",
      "  -0.04949071 -0.01426489 -0.05553244  0.15068462]\n",
      " [-0.00282345 -0.01309765  0.06699312 -0.0197591   0.00414393  0.00513414\n",
      "  -0.00938541 -0.04037121  0.03539087 -0.03506422 -0.00679322 -0.05782018\n",
      "  -0.04953086 -0.01427646 -0.05557749  0.15080687]\n",
      " [-0.00282405 -0.01310045  0.0670074  -0.01976332  0.00414482  0.00513524\n",
      "  -0.00938741 -0.04037982  0.03539842 -0.0350717  -0.00679467 -0.05783251\n",
      "  -0.04954143 -0.01427951 -0.05558935  0.15083906]\n",
      " [-0.00282268 -0.01309407  0.06697478 -0.01975369  0.0041428   0.00513274\n",
      "  -0.00938284 -0.04036016  0.03538119 -0.03505462 -0.00679136 -0.05780436\n",
      "  -0.04951731 -0.01427256 -0.05556229  0.15076563]\n",
      " [-0.00274767 -0.01274611  0.06519503 -0.01922877  0.00403271  0.00499634\n",
      "  -0.00913351 -0.03928765  0.03444099 -0.0341231  -0.00661089 -0.05626829\n",
      "  -0.04820146 -0.01389328 -0.0540858   0.14675926]]\n"
     ]
    }
   ],
   "source": [
    "y_pred1= model.predict([x2, a2],batch_size=n2)\n",
    "print('y_pred1',y_pred1)\n",
    "#print('y_pred1[15]',y_pred1[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ed7f1d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'targets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/mariaussano/Desktop/5 sem/tesi/step1_socio_demo_Aggregation/thesis/thesis/GCNN_cluster_size16.ipynb Cella 32\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mariaussano/Desktop/5%20sem/tesi/step1_socio_demo_Aggregation/thesis/thesis/GCNN_cluster_size16.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m y\u001b[39m=\u001b[39mtargets\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mariaussano/Desktop/5%20sem/tesi/step1_socio_demo_Aggregation/thesis/thesis/GCNN_cluster_size16.ipynb#X43sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df\u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39mDataFrame(y,columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39my_true\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mariaussano/Desktop/5%20sem/tesi/step1_socio_demo_Aggregation/thesis/thesis/GCNN_cluster_size16.ipynb#X43sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m pred\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39marray(y_pred1[\u001b[39m15\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'targets' is not defined"
     ]
    }
   ],
   "source": [
    "y=targets\n",
    "df=pd.DataFrame(y,columns=['y_true'])\n",
    "pred=np.array(y_pred1[15])\n",
    "df['y_pred']=pred\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085bfd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          y_true    y_pred\n",
      "0   9.467379e+07  0.101800\n",
      "1   3.839332e+08  0.000265\n",
      "2   2.699809e+08  0.000083\n",
      "3   8.157967e+07  0.022130\n",
      "4   5.449503e+08  0.000037\n",
      "5   1.274099e+09  0.000151\n",
      "6   2.058071e+09  0.001526\n",
      "7   2.101724e+08  0.052930\n",
      "8   1.436575e+09  0.000022\n",
      "9   2.493902e+09  0.408812\n",
      "10  1.325679e+09  0.000682\n",
      "11  1.739703e+09  0.000120\n",
      "12  1.541005e+09  0.390255\n",
      "13  4.256834e+09  0.006363\n",
      "14  1.792077e+09  0.002751\n",
      "15  3.944498e+09  0.012073\n"
     ]
    }
   ],
   "source": [
    "#print(y_pred[15],y)\n",
    "y1=targets1\n",
    "df1=pd.DataFrame(y1,columns=['y_true'])\n",
    "pred1=np.array(y_pred1[15])\n",
    "df1['y_pred']=pred1\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea3f25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0982 - mae: 0.1120 - root_mean_squared_error: 0.3133\n",
      "Test metrics handmade model {'loss': 0.09820312261581421, 'mae': 0.11197242885828018, 'root_mean_squared_error': 0.3132556676864624}\n"
     ]
    }
   ],
   "source": [
    "model_metrics = model.evaluate([x1,a1],labels_encoded1, return_dict=True)\n",
    "print(\"Test metrics handmade model\",model_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db877b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02142636, 0.01534876, 0.01142783, 0.02541816, 0.02090915,\n",
       "       0.02594018, 0.02259602, 0.02497141, 0.018046  , 0.01770126,\n",
       "       0.01409446, 0.01248658, 0.01608839, 0.00888556, 0.00992036,\n",
       "       0.01512774], dtype=float32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[15]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "3a3f7d48386d6cbc8fbbce55b45b43f5d6f0e3927b99bf219dd1615c5238811d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
